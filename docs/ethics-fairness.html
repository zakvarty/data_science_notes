<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 17 Fairness | Effective Data Science</title>
<meta name="author" content="Zak Varty">
<meta name="description" content="Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading. If you would like to contribute to the development of EDS, you may do so at...">
<meta name="generator" content="bookdown 0.30 with bs4_book()">
<meta property="og:title" content="Chapter 17 Fairness | Effective Data Science">
<meta property="og:type" content="book">
<meta property="og:url" content="https://eds-book.zakvarty.com/ethics-fairness.html">
<meta property="og:image" content="https://eds-book.zakvarty.com/images/EDS-logo.jpg">
<meta property="og:description" content="Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading. If you would like to contribute to the development of EDS, you may do so at...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 17 Fairness | Effective Data Science">
<meta name="twitter:description" content="Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading. If you would like to contribute to the development of EDS, you may do so at...">
<meta name="twitter:image" content="https://eds-book.zakvarty.com/images/EDS-logo.jpg">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Effective Data Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">About this Course</a></li>
<li class="book-part">Effective Data Science Workflows</li>
<li><a class="" href="workflows-introduction.html">Introduction</a></li>
<li><a class="" href="workflows-organising-your-work.html"><span class="header-section-number">2</span> Organising your work</a></li>
<li><a class="" href="workflows-naming.html"><span class="header-section-number">3</span> Naming Files</a></li>
<li><a class="" href="workflows-code.html"><span class="header-section-number">4</span> Code</a></li>
<li><a class="" href="workflows-checklist.html">Checklist</a></li>
<li class="book-part">Acquiring and Sharing Data</li>
<li><a class="" href="data-introduction.html">Introduction</a></li>
<li><a class="" href="data-tabular.html"><span class="header-section-number">5</span> Tabular Data</a></li>
<li><a class="" href="data-webscraping.html"><span class="header-section-number">6</span> Web Scraping</a></li>
<li><a class="" href="data-apis.html"><span class="header-section-number">7</span> APIs</a></li>
<li><a class="" href="data-checklist.html">Checklist</a></li>
<li class="book-part">Data Exploration and Visualisation</li>
<li><a class="" href="edav-introduction.html">Introduction</a></li>
<li><a class="" href="edav-wrangling.html"><span class="header-section-number">9</span> Data Wrangling</a></li>
<li><a class="" href="edav-analysis.html"><span class="header-section-number">10</span> Exploratory Data Analysis</a></li>
<li><a class="" href="edav-visualisation.html"><span class="header-section-number">11</span> Data Visualisation</a></li>
<li><a class="" href="edav-checklist.html">Checklist</a></li>
<li class="book-part">Preparing for Production</li>
<li><a class="" href="production-introduction.html">Introduction</a></li>
<li><a class="" href="production-reproducibility.html"><span class="header-section-number">12</span> Reproducibility</a></li>
<li><a class="" href="production-explainability.html"><span class="header-section-number">13</span> Explainability</a></li>
<li><a class="" href="production-scalability.html"><span class="header-section-number">14</span> Scalability</a></li>
<li><a class="" href="production-checklist.html">Checklist</a></li>
<li class="book-part">Data Science Ethics</li>
<li><a class="" href="ethics-introduction.html">Introduction</a></li>
<li><a class="" href="ethics-privacy.html"><span class="header-section-number">16</span> Privacy</a></li>
<li><a class="active" href="ethics-fairness.html"><span class="header-section-number">17</span> Fairness</a></li>
<li><a class="" href="ethics-conduct.html"><span class="header-section-number">18</span> Codes of Conduct</a></li>
<li><a class="" href="ethics-checklist.html">Checklist</a></li>
<li><a class="" href="reading-list.html"><span class="header-section-number">19</span> Reading List</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/zakvarty/data_science_notes/">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ethics-fairness" class="section level1" number="17">
<h1>
<span class="header-section-number">17</span> Fairness<a class="anchor" aria-label="anchor" href="#ethics-fairness"><i class="fas fa-link"></i></a>
</h1>
<div class="rmdnote">
<p>Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.</p>
<p>If you would like to contribute to the development of EDS, you may do so at <a href="https://github.com/zakvarty/data_science_notes" class="uri">https://github.com/zakvarty/data_science_notes</a>.</p>
</div>
<div id="fairness-and-the-data-revolution" class="section level2" number="17.1">
<h2>
<span class="header-section-number">17.1</span> Fairness and the Data Revolution<a class="anchor" aria-label="anchor" href="#fairness-and-the-data-revolution"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="images/502-ethics-fairness/90s-windows-wallpaper.jpg" alt="Windows 95 logo on a background of white fluffy clouds. Text below reads: We're living in the 90s. Image is stylised to look like it is on a low resolution screen from the 90s." width="100%" style="display: block; margin: auto;"></div>
<p>Before the 1990s, large datasets were typically only collected to understand huge, complex systems such as:</p>
<ul>
<li>the weather</li>
<li>infrastructure
<ul>
<li>hospitals</li>
<li>roads</li>
<li>train networks</li>
</ul>
</li>
<li>stock markets</li>
<li>populations.</li>
</ul>
<p>Collecting high quality data on these systems was immensely expensive but paid dividends by allowing us to describe the expected behaviour of these systems at an aggregate level. Using this sort of information, we can’t make journeys or healthcare better at an individual level but we <em>can</em> make changes to try and make these experiences better on average.</p>
<p>Things changed with the widespread adoption of the internet in the mid-1990s and the subsequent surge in data collection, sharing and processing. Suddenly, we as individuals shifted from being just one part of these huge processes to being a complex process worth of modelling all on our own.</p>
<div class="inline-figure"><img src="502-ethics-fairness_files/figure-html/unnamed-chunk-3-1.png" alt="Barplot showing an exponential-like increase in the volume of data created, captured, copied, and consumed worldwide in each year from 2010 to 2022." width="672" style="display: block; margin: auto;"></div>
<p>It was at this point that focus shifted toward making individual, personalised predictions for specific people, based on the vast amounts of data that we generate as we go about our daily lives.</p>
<p>This shift from aggregate to individual behaviour creates the opportunity not only for these predictions to systematically harm groups of people, as they always could, but also to acutely harm individuals.</p>
</div>
<div id="you-are-your-data" class="section level2" number="17.2">
<h2>
<span class="header-section-number">17.2</span> You are Your Data<a class="anchor" aria-label="anchor" href="#you-are-your-data"><i class="fas fa-link"></i></a>
</h2>
<p>The blunt truth is that, as far as a data science model is concerned, you are nothing more than a point in a high-dimensional predictor space.</p>
<div class="inline-figure"><img src="502-ethics-fairness_files/figure-html/you-are-just-a-point-plot-1.png" alt="scatter plot of two predictors, x_1 and x_2. An arrow points to one observation, with the caption 'you are just a point in predictor space.'" width="100%" style="display: block; margin: auto;"></div>
<p>The model might use your location in that space to group you with other points that are in some sense “nearby”. Alternatively, the model might estimate some information about you that it currently doesn’t have, based on what it knows about those surrounding points. These other points also represent unique humans with rich and fascinating lives - but the model doesn’t care about that, it is just there to group some points or predict some values.</p>
<div class="inline-figure"><img src="502-ethics-fairness_files/figure-html/classification-plot-1.png" alt="Two scatter plots illustrating classification and regression with two predictors. The outcome values are shown by colour for all but one point. This point is grey and represents the predictor values at a prediction point representing the reader." width="100%" style="display: block; margin: auto;"></div>
<p>The idea of fairness comes into data science when we begin to ask ourselves which predictors we should provide the model with when carrying out these tasks. We aren’t asking this from a model selection stand-point. We are asking what are morally permissible predictors, not what leads to a significant improvement in model fit.</p>
</div>
<div id="forbidden-predictors" class="section level2" number="17.3">
<h2>
<span class="header-section-number">17.3</span> Forbidden Predictors<a class="anchor" aria-label="anchor" href="#forbidden-predictors"><i class="fas fa-link"></i></a>
</h2>
<p>The argument about what features of a human being can be used to make decisions about them started well before the 1990s. The most contentious of these arguments centre around the inclusion of characteristics that are either immutable or not easily changed. Some of these characteristics including race, gender, age or religion receive legal protections. These protected attributes are often forbidden to be used in important decisions, such as whether a bank loan is accepted.</p>
<p>This natural lead us to ask what classifies as an important decision?</p>
<hr>
<p><a href="https://www.gov.uk/discrimination-your-rights">Protected Characteristics</a> under the <a href="https://www.legislation.gov.uk/ukpga/2010/15/contents">Equality Act (2010)</a></p>
<!-- 
TODO: Format as two columns. Would be prettier but needs separate latex / html implementation
-->
<ul>
<li><p>age</p></li>
<li><p>gender reassignment</p></li>
<li><p>marriage / civil partnership</p></li>
<li><p>pregnancy / parental leave</p></li>
<li><p>disability</p></li>
<li><p>race including colour, nationality, ethnic or national origin</p></li>
<li><p>religion or belief</p></li>
<li><p>sex</p></li>
<li><p>sexual orientation</p></li>
</ul>
<hr>
<p>We also need to be careful if these protected attributes actually have strong predictive power and would improve our predictions (potentially to the benefit of the groups that are being protected by these regulations). Just because a protected attribute isn’t used directly within a model that doesn’t mean the model will not discriminate according to that attribute.</p>
<p>If we have multiple predictors within a model, then withholding a protected attribute does not make the model ignorant of that attribute. If you have access to someone’s browsing history, where they live and some of their recent purchases you can probably make a fairly accurate profile of that person, including many of these supposedly protected attributes. In the same way, a model can use or combine attributes that are not legally protected to create a new variable that acts as an accurate proxy for the protected characteristic.</p>
<p>And why wouldn’t our model do this? When using a standard loss function we have literally asked it to get the best possible predictive performance. If a protected attribute has predictive power then the model is likely to approximate it using the predictors that are available to it.</p>
<p>Before we see how to handle this concern, let’s step back and consider how we can quantify and measure fairness in a model.</p>
</div>
<div id="measuring-fairness" class="section level2" number="17.4">
<h2>
<span class="header-section-number">17.4</span> Measuring Fairness<a class="anchor" aria-label="anchor" href="#measuring-fairness"><i class="fas fa-link"></i></a>
</h2>
<p>Converting the concept of fairness into a mathematical statement is a very difficult task. This is partly because moving from natural language to precise formalism is hard, but it’s also because the term fairness means different things to different people in different contexts. Because of this, there are many complementary definitions of fairness that all try to capture some intuitive notion of a fair model. However, these measures all capture different facets of this complicated concept. Despite this, these measures vary to such an extent they can’t all be satisfied simultaneously.</p>
<p>I’ll introduce four such measures shortly, focusing in on the case of binary outcomes where a “positive” response of 1 corresponds to an event that would be considered favourably when taken in context. For example this might be a loan that will be successfully repaid or that a person released on bail will not re-offend.</p>
<ul>
<li>Binary outcome <span class="math inline">\(Y \in \{0,1\}\)</span>.</li>
</ul>
<p>We’ll consider the simple case where a binary prediction is made in each instance, and where we want our predictions to be fair across the <span class="math inline">\(k\)</span> distinct levels of some protected attribute <span class="math inline">\(A\)</span>.</p>
<ul>
<li>Binary Prediction <span class="math inline">\(\hat Y \in \{0,1\}\)</span>.</li>
<li>Protected attribute <span class="math inline">\(A\)</span> takes values in <span class="math inline">\(\mathcal{A} = \{a_1, \ldots, a_k\}\)</span>.</li>
</ul>
<div id="demographic-parity" class="section level3" number="17.4.1">
<h3>
<span class="header-section-number">17.4.1</span> Demographic Parity<a class="anchor" aria-label="anchor" href="#demographic-parity"><i class="fas fa-link"></i></a>
</h3>
<p>The first, and potentially most obvious fairness definition is that of demographic parity. Here a model is deemed fair if, across all subgroups of the protected attribute, the probability of predicting a successful outcome is equal.</p>
<p><br></p>
<blockquote>
<p>The probability of predicting a ‘positive’ outcome is the same for all groups.</p>
</blockquote>
<p><br></p>
<p><span class="math display">\[\mathbb{P}(\hat Y = 1 | A = a_i) = \mathbb{P}( \hat Y = 1 | A = a_j), \  \text{ for all }\  i,j \in \mathcal{A}.\]</span></p>
<p>An obvious shortcoming demographic parity is that it does not allow us to account for the fact that a positive outcome might not be equally likely in each of these subgroups. In this way demographic parity is analogous to treating people equally, rather than equitably.</p>
</div>
<div id="equal-opportunity" class="section level3" number="17.4.2">
<h3>
<span class="header-section-number">17.4.2</span> Equal Opportunity<a class="anchor" aria-label="anchor" href="#equal-opportunity"><i class="fas fa-link"></i></a>
</h3>
<p>Equality of opportunity addresses this shortcoming by conditioning on a truly positive outcome. Equality of opportunity states that of those who are “worthy” of a loan (in some sense), all subgroups of the protected characteristic should be treated equally.</p>
<p><br></p>
<blockquote>
<p>Among those who have a true ‘positive’ outcome, the probability of predicting a ‘positive’ outcome is the same for all groups.</p>
</blockquote>
<p><br></p>
<p><span class="math display">\[\mathbb{P}(\hat Y = 1 | A = a_i, Y =1) = \mathbb{P}( \hat Y = 1 | A = a_j, Y=1), \  \text{ for all }\  i,j \in \mathcal{A}.\]</span></p>
</div>
<div id="equal-odds" class="section level3" number="17.4.3">
<h3>
<span class="header-section-number">17.4.3</span> Equal Odds<a class="anchor" aria-label="anchor" href="#equal-odds"><i class="fas fa-link"></i></a>
</h3>
<p>Of course, you have encountered two-way tables, type-I and type-II errors. Equally important as granting loans to people who will repay them is to deny loans to those who cannot afford them.</p>
<p>A model satisfying the equal odds condition can identify true positives and false negatives equally well across all sub-groups of the protected characteristic.</p>
<blockquote>
<p>Among those who have a true ‘positive’ outcome, the probability of predicting a ‘positive’ outcome is the same for all groups.</p>
<p><em>AND</em></p>
<p>Among those who have a true ‘negative’ outcome, the probability of predicting a ‘negative’ outcome is the same for all groups.</p>
</blockquote>
<p><br></p>
<p><span class="math display">\[\mathbb{P}(\hat Y = y | A = a_i, Y =y) = \mathbb{P}( \hat Y = y | A = a_j, Y=y), \ \text{ for all } \ y \in \{0,1\} \ \text{ and } \  i,j \in \mathcal{A}.\]</span></p>
</div>
<div id="predictive-parity" class="section level3" number="17.4.4">
<h3>
<span class="header-section-number">17.4.4</span> Predictive Parity<a class="anchor" aria-label="anchor" href="#predictive-parity"><i class="fas fa-link"></i></a>
</h3>
<p>All of the measures we have considered so far consider the probability of a prediction given the true credit-worthiness of an applicant. Predictive Parity reverses the order of conditioning (as compared to equal opportunity).</p>
<p>It ensures that among those predicted to have a successful outcome, the probability of a truly successful outcome should be the same for all subgroups of the protected characteristic. This ensures that, in our financial example, the bank is spreading its risk exposure equally across all subgroups; each subgroup should have an approximately equal proportion of approved loans being successfully repaid.</p>
<blockquote>
<p>The probability of a true ‘positive’ outcome for people who were predicted a ‘positive’ outcome is equal across groups.</p>
</blockquote>
<p><br></p>
<p><span class="math display">\[\mathbb{P}(Y = 1 | \hat Y = 1, A = a_i) = \mathbb{P}(Y_1 = 1 | \hat Y = 1, A = a_j) \ \text{ for all } \  i,j \in \mathcal{A}.\]</span></p>
<p>We can play devil’s advocate here and say that this might not be appropriate if there is a genuine difference in the probability of successful repayment between groups.</p>
</div>
</div>
<div id="metric-madness" class="section level2" number="17.5">
<h2>
<span class="header-section-number">17.5</span> Metric Madness<a class="anchor" aria-label="anchor" href="#metric-madness"><i class="fas fa-link"></i></a>
</h2>
<p>Even with this very simple binary classification problem that there are many ways we can interpret the term fairness. Which, if any, of these will be appropriate is going to be highly context dependent.</p>
<p>An issue with many of these metrics, including some of those introduced, is that they require knowledge of the true outcome. This means that these metrics can only be evaluated retrospectively: if we knew this information to begin with then we wouldn’t need a model to decide who get a loan. On top of this, it means that we only ever get information about the loans that are granted - we don’t have access to the counter factual outcome of whether a loan that was not granted would have been repaid.</p>
<p>An additional problem is that evaluating these fairness metrics requires us to know which protected sub-group each individual belongs to. This is clearly a problem: to evaluate the fairness of our loan applications we need to know sensitive information about the applicants, who would - very reasonably - be unwilling to provide that information because it legally cannot be used to inform the decision making process. For this reason, an independent third-party is often required to assess fairness by collating data from the applicants and the bank.</p>
<p>A third complication here is that these definitions deal in strict equalities. In any given sample, these are almost surely not going to be satisfied even if the metric is truly satisfied. A formal statistical test should be used to assess whether these differences are consistent with a truly fair model, however the more common approach is for regulators to set some acceptable tolerance on the discrepancy in metric values between sub-groups.</p>
<p>Finally, it is worth noting that all of these problems arise for a simple binary classifier but most models are far more complicated than this. Even working with these conditional probability statements requires careful attention, but things get much trickier when the response or sensitive attribute are continuous valued or when other, non-sensitive predictors are also included in the model.</p>
</div>
<div id="modelling-fairly" class="section level2" number="17.6">
<h2>
<span class="header-section-number">17.6</span> Modelling Fairly<a class="anchor" aria-label="anchor" href="#modelling-fairly"><i class="fas fa-link"></i></a>
</h2>
<div id="fairness-aware-loss-functions" class="section level3" number="17.6.1">
<h3>
<span class="header-section-number">17.6.1</span> Fairness Aware Loss Functions<a class="anchor" aria-label="anchor" href="#fairness-aware-loss-functions"><i class="fas fa-link"></i></a>
</h3>
<p>Now we have some methods to detect and quantify the fairness of our models, how do we incorporate that into the model fitting process?</p>
<div class="medium_right">
<div class="inline-figure"><img src="images/502-ethics-fairness/fairness-error-pareto-front.png" alt="Plot of unfairness against error for many potential models. The Pareto frontier is shown by the portion of the convex hull of the points that is closest to the origin."></div>
</div>
<p>We now have multiple objectives: to predict our outcome as accurately as possible while also treating people as fairly as possible (by which ever fairness metric or combination of metrics we care to consider). Unfortunately, these things are generally in competition with each other. There is no one best model but rather a family of best models, from which we have to pick a single model to implement.</p>
<p>Can resolve this issue by linearisation, create our loss function as a linear weighted sum of the two component objectives. This simplifies the problem mathematically, but actually just shift the problem rather than resolving it. Up to scaling constant, each combination of weights corresponds to a unique point on the Pareto frontier, so we have just translated our problem from picking a point on the frontier to picking a pair of weights.</p>
<p>To do actually resolve this issue we need to define our relative preference between fairness and predictive power. When I say “our preference”, what I actually mean that of the company or organisation for whom we are doing an analysis - not our own personal view. Eliciting this preference and communicating the idea of an optimal frontier can be tricky. One solution is to present a small set of possible models, which represent a range of preferences between the two competing objectives, and ask the stakeholder to choose between these.</p>
</div>
<div id="other-approaches" class="section level3" number="17.6.2">
<h3>
<span class="header-section-number">17.6.2</span> Other Approaches<a class="anchor" aria-label="anchor" href="#other-approaches"><i class="fas fa-link"></i></a>
</h3>
<hr>
<ul>
<li>Re-weighting or resampling to better represent minority groups.</li>
<li>Forgetting factor to reduce impact of bias in historical data.</li>
<li>Meta-modelling to intervene in harmful feedback loops.</li>
</ul>
<hr>
<p>Whenever we treating all predictions equally and our loss function optimises purely for predictive performance, good predictions for minority groups will never be prioritised. One strategy to correct for this is to either re-weighting or re-sample each observation so that minority groups are given greater importance within the loss function.</p>
<p>A lot of the problems of fairness that we see are because our models are replicating what happens or used to happen in reality. In some cases, this is being better addressed now and a model can be made more fair by down-weighting training data as it ages. This allows our model to more quickly adapt to changes in the system it is trying to represent.</p>
<p>In other cases the use of historically biased data to train models that are put into production has lead to a feedback loop that makes more recent data even more unjust.
One example of this, we can consider racial disparity in the interest rates offered on mortgages. Suppose that one racial group of applicants was in the past slightly more likely to default on loans, perhaps due to historical pay inequity. This means that models would likely suggest higher interest loans to this group, in an attempt to offset the bank’s exposure to the risk of non-repayment.</p>
<p>This not only reduces the number of loans that will be granted to that racial group but it <em>also</em> makes the loans that are granted more difficult to repay and more likely to be defaulted on. This in turn leads to another increase in the offered interest rate, driving down the number of loans approved and pushing up the chance of non-repayment even further.</p>
<p>The decisions made using this model are impacting its future training data and creating a harmful and self-reinforcing feedback loop. Historical down weighting will do nothing to address this sort of issue, which requires active intervention.</p>
<p>A meta-modelling approach is possible type of intervention. Here post-hoc methods used to estimate the biases within a fitted model and these estimates are used to explicitly correct for historical biases, <em>before</em> the model is used to make predictions or decisions.</p>
</div>
</div>
<div id="wrapping-up-9" class="section level2" number="17.7">
<h2>
<span class="header-section-number">17.7</span> Wrapping Up<a class="anchor" aria-label="anchor" href="#wrapping-up-9"><i class="fas fa-link"></i></a>
</h2>
<p>That’s a good point for us to wrap up this introduction to fairness in data science.</p>
<p>We have seen that optimising for predictive accuracy alone can lead to unjust models. We also raised concerns about protected characteristics being included in models, whether that is directly as a predictor or via a collection of other predictors that well approximate them.</p>
<p>We have seen that there are a multitude of measures to assess the fairness of our models. We can combine these with standard metrics for goodness-of-fit to create custom loss functions which represent our preference between fairness and predictive performance.</p>
<p>As with privacy, there are no universal answers when it comes to measuring and implementing fair data science methodology. This is still a relatively new and rapidly evolving field of data science.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="ethics-privacy.html"><span class="header-section-number">16</span> Privacy</a></div>
<div class="next"><a href="ethics-conduct.html"><span class="header-section-number">18</span> Codes of Conduct</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ethics-fairness"><span class="header-section-number">17</span> Fairness</a></li>
<li><a class="nav-link" href="#fairness-and-the-data-revolution"><span class="header-section-number">17.1</span> Fairness and the Data Revolution</a></li>
<li><a class="nav-link" href="#you-are-your-data"><span class="header-section-number">17.2</span> You are Your Data</a></li>
<li><a class="nav-link" href="#forbidden-predictors"><span class="header-section-number">17.3</span> Forbidden Predictors</a></li>
<li>
<a class="nav-link" href="#measuring-fairness"><span class="header-section-number">17.4</span> Measuring Fairness</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#demographic-parity"><span class="header-section-number">17.4.1</span> Demographic Parity</a></li>
<li><a class="nav-link" href="#equal-opportunity"><span class="header-section-number">17.4.2</span> Equal Opportunity</a></li>
<li><a class="nav-link" href="#equal-odds"><span class="header-section-number">17.4.3</span> Equal Odds</a></li>
<li><a class="nav-link" href="#predictive-parity"><span class="header-section-number">17.4.4</span> Predictive Parity</a></li>
</ul>
</li>
<li><a class="nav-link" href="#metric-madness"><span class="header-section-number">17.5</span> Metric Madness</a></li>
<li>
<a class="nav-link" href="#modelling-fairly"><span class="header-section-number">17.6</span> Modelling Fairly</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#fairness-aware-loss-functions"><span class="header-section-number">17.6.1</span> Fairness Aware Loss Functions</a></li>
<li><a class="nav-link" href="#other-approaches"><span class="header-section-number">17.6.2</span> Other Approaches</a></li>
</ul>
</li>
<li><a class="nav-link" href="#wrapping-up-9"><span class="header-section-number">17.7</span> Wrapping Up</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/zakvarty/data_science_notes//blob/master/502-ethics-fairness.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/zakvarty/data_science_notes//edit/master/502-ethics-fairness.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Effective Data Science</strong>" was written by Zak Varty. It was last built on 2023-04-24.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
