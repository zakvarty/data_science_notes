% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage[inner=30mm,outer=25mm,top=25mm,bottom=25mm]{geometry}
%\usepackage[parfill]{parskip}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Effective Data Science},
  pdfauthor={Zak Varty},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Effective Data Science}
\author{Zak Varty}
\date{2023-02-06}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\setstretch{1.5}
\hypertarget{about-this-course}{%
\chapter*{About this Course}\label{about-this-course}}
\addcontentsline{toc}{chapter}{About this Course}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{course-description}{%
\section*{Course Description}\label{course-description}}
\addcontentsline{toc}{section}{Course Description}

Model building and evaluation are are necessary but not sufficient skills for the effective practice of data science. In this module you will develop the technical and personal skills that are required to work successfully as a data scientist within an organisation.

During this module you will critically explore how to:

\begin{itemize}
\tightlist
\item
  effectively scope and manage a data science project;
\item
  work openly and reproducibly;
\item
  efficiently acquire, manipulate, and present data;
\item
  interpret and explain your work for a variety of stakeholders;
\item
  ensure that your work can be put into production;
\item
  assess the ethical implications of your work as a data scientist.
\end{itemize}

This interdisciplinary course will draw from fields including statistics, computing, management science and data ethics. Each topic will be investigated through a selection of lecture videos, conference presentations and academic papers, hands-on lab exercises, and readings on industry best-practices from recognised professional bodies.

\hypertarget{schedule}{%
\section*{Schedule}\label{schedule}}
\addcontentsline{toc}{section}{Schedule}

These notes are intended for students on the course \textbf{MATH70076: Data Science} in the academic year 2022/23.

As the course is scheduled to take place over five weeks, the suggested schedule is:

\begin{itemize}
\tightlist
\item
  1st week: effective data science workflows;
\item
  2nd week: aquiring and sharing data;
\item
  3rd week: exploratory data analysis and visualisation;
\item
  4th week: preparing for production;
\item
  5th week: ethics and context of data science.
\end{itemize}

A pdf version of these notes may be downloaded \href{./data_science_notes.pdf}{here}. Please be aware that these are very rough and will be updated less frequently than the course webpage.

\hypertarget{learning-outcomes}{%
\section*{Learning outcomes}\label{learning-outcomes}}
\addcontentsline{toc}{section}{Learning outcomes}

On successful completion of this module students should be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independently scope and manage a data science project;
\item
  Source data from the internet through web scraping and APIs;
\item
  Clean, explore and visualise data, justifying and documenting the decisions made;
\item
  Evaluate the need for (and implement) approaches that are explainable, reproducible and scalable;
\item
  Appraise the ethical implications of a data science projects, particularly the risks of compromising privacy or fairness and the potential to cause harm.
\end{enumerate}

\hypertarget{allocation-of-study-hours}{%
\section*{Allocation of Study Hours}\label{allocation-of-study-hours}}
\addcontentsline{toc}{section}{Allocation of Study Hours}

\textbf{Lectures:} 10 Hours (2 hours per week)

\textbf{Group Teaching:} 5 Hours (1 hour per week)

\textbf{Lab / Practical:} 5 hours (1 hour per week)

\textbf{Independent Study:} 105 hours (15 hours per week + 30 hours coursework)

\hypertarget{assessment-structure}{%
\section*{Assessment Structure}\label{assessment-structure}}
\addcontentsline{toc}{section}{Assessment Structure}

The course will be assessed entirely by coursework, reflecting the practical and pragmatic nature of the course material.

\textbf{Coursework 1 (30\%):} To be completed during the fourth week of the course.

\textbf{Coursework 2 (70\%):} To be released in the last week of the course and submitted following the examination period in Summer term.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

These notes were created by Dr Zak Varty. They were inspired by a previous lecture series by Dr Purvasha Chakravarti at Imperial College London and draw from many resource that were made available by the R community.

\hypertarget{part-effective-data-science-workflows}{%
\chapter{(PART) Effective Data Science Workflows}\label{part-effective-data-science-workflows}}

\hypertarget{workflows-introduction}{%
\chapter*{Introduction}\label{workflows-introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Effective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

As a data scientist you will never work alone.

Within a single project a data scientist is likely that you will interact with a range of other people, including but not limited to: one or more project managers, stakeholders and subject matter experts. These experts might come from a single specialism or form a multidisciplinary team, depending on the type of work that you are doing.

To get your project put into use and working at scale you will likely have to collaborate with data engineers. You will also work closely with other data scientists, to review one another's work or to collaborate on larger projects.

Familiarity with the skills, processes and practices that make for collaboration is instrumental to being a successful as a data scientist. The aim for this part of the course is to provide you with a structure on how you organise and perform your work, so that you can be a good collaborator to current colleges and your future self.

This is going to require a bit more effort upfront, but the benefits will compound over time. You will get more done by wasting less time staring quizzically at messy folders of indecipherable code. You will also gain a reputation of someone who is good to work with. This promotes better professional relationships and greater levels of trust, which can in turn lead to working on more exciting and impactful projects.

\hypertarget{workflows-organising-your-work}{%
\chapter{Organising your work}\label{workflows-organising-your-work}}

Effective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

Welcome to this course on effective data science. This week we'll be considering effective data science workflows. These workflows are ways of progressing a project that will help you to produce high quality work and help to make you a good collaborator.

In this Chapter, we'll kick things off by looking at how you can structure data
science projects and organize your work. Familiarity with these skills, processes
and practices for collaborative working are going to be instrumental as you
become a successful data scientist.

\hypertarget{what-are-we-trying-to-do}{%
\section{What are we trying to do?}\label{what-are-we-trying-to-do}}

First, let's consider why we want to provide our data science projects with some sense of structure and organization.

As a data scientist you'll never work alone. Within a single project you'll interact with a whole range of other people. This might be a project manager, one or more business stakeholders or a variety of subject matter experts.These experts might be trained as sociologists, chemists, or civil servants depending on the exact type of data science work that you're doing.

To then get your project put into use and working at scale you'll have to
collaborate with data engineers. You'll also likely work closely with other data scientists. For smaller projects this might be to act as reviewers for one another's work. For larger projects working collaboratively will allow you to tackle larger challenges. These are the sorts of project that wouldn't be feasible alone, because of the inherent limitations on the time and skill of any one individual person.

Even if you work in a small organization, where you're the only data scientist, then adopting a way of working that's focused on collaborating will pay dividends over time. This is because when you inevitably return to the project that you're working on in several weeks or months or years into the future you'll have forgotten almost everything of what you did the first time around. You'll also have forgotten why you made the decisions that you did and what other potential options there were that you didn't take.

This is exactly like working with a current colleague who has shoddy or poor working practices. Nobody wants to be that colleague to somebody else, let alone to their future self. Even when working alone, treating your future self as a current collaborator (and one that you want to get along well with) makes you a kind colleague and a pleasure to work with.

The aim of this week is to provide you with a guiding structure on how you organize and perform your work. None of this is going to be particularly difficult or onerous. However it will require a bit more effort up front and daily discipline. Like with flossing, the daily effort required is not large but the benefits will compound over time.

You'll get more done by wasting less time staring quizzically at a mess of
folders and indecipherable code. You'll also get a reputation as someone who's well organized and good to work with. This promotes better professional relationships and greater levels of trust within your team. These can then, in turn, tead to you working on more exciting and more impactful projects in the future.

\hypertarget{an-r-focused-approach}{%
\section{An R Focused Approach}\label{an-r-focused-approach}}

The structures and workflows that are recommend here and throughout the rest of this module are focused strongly on a workflow that predominantly uses R, markdown and LaTeX.

Similar techniques, code and software can achieve the same results that I show you here when coding in Python or C, or when writing up projects in Quarto or some other markup language. Similarly, different organizations have their own variations on these best practices that we'll go through together. Often organisations will have extensive guidance on these topics.

The important thing is that once you understand what good habits are and have build them in one programming language or business, then transferring these skills to a new setting is largely a matter of learning some new vocabulary or slightly different syntax.

With that said, let's get going!

\hypertarget{one-project-one-directory}{%
\section{One Project = One Directory}\label{one-project-one-directory}}

If there's one thing you should take away from this chapter, it's this one
Golden Rule:

\begin{quote}
Every individual project you work on as a data scientist should be in a single, self-contained directory or folder.
\end{quote}

This is worth repeating. Every single project that you work on should be self-contained and live in a single directory. An analogy here might be having a separate ring-binder folder for each of your modules on a degree program.

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-1}

This one golden rule is deceptively simple.

The first issue here is that it requires a predetermined scope of what is and
what isn't going to be covered by this particular project. This seems straightforward but at the outset of the project you often do not know exactly where your project will go, or how it will link to other pieces of work within your organization.

The second issue is the second law of Thermodynamics, which applies equally well to project management as it does to the heatdeath of the universe. It takes continual external effort to prevent the contents of this one folder from becoming chaotic and disordered over time.

That being said, having a single directory has several benefits which more than justify this additional work.

\hypertarget{properties-of-a-well-orgainsed-project}{%
\section{Properties of a Well-Orgainsed Project}\label{properties-of-a-well-orgainsed-project}}

What are the properties that we would like this single, well-organized project to have? Ideally, we'd like to organize our projects so that I have the following
properties:

\begin{itemize}
\tightlist
\item
  Portable
\item
  Version Control Friendly
\item
  Reproducible
\item
  IDE friendly.
\end{itemize}

Don't worry if you haven't heard of some of these terms already. We're going to look at each of them in a little bit of detail.

\hypertarget{portability}{%
\subsection{Portability}\label{portability}}

A project is said to be portable if it can be easily moved without breaking.

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-2}

This might be a small move, like relocating the directory to a different location on your own computer. It might also mean a moderate move, say to another machine if yours dies just before a big deadline. Alternatively, it might be a large shift - to be uses by another person who is using a different operating system.

From this thought experiment you can see that there's a full spectrum of how portable a project may or may not need to be.

\hypertarget{version-control-friendly}{%
\subsection{Version Control Friendly}\label{version-control-friendly}}

A project under Version Control has all changes tracked either manually or automatically. This means that snapshots of the project are taken regularly as it gradually develops and evolves over time. Having these snapshots as many, incremental changes are made to the project allow it to be rolled back to a specific previous state if something goes wrong.

A version controlled pattern of working helps to avoid the horrendous state that we have all found ourselves in - renaming \texttt{final\_version.doc} to \texttt{final\_final\_version.doc} and so on.

By organising your workflow around incremental changes helps you to acknowledge that no work is ever finally complete. There will always be small changes that need to be done in the future.

\hypertarget{reproducibility}{%
\subsection{Reproducibility}\label{reproducibility}}

\begin{quote}
A study is reproducible if you can take the original data and the computer code used to analyze the data and recreate all of the numerical findings from the study.

Broman et al (2017). ``Recommendations to Funding Agencies for Supporting Reproducible Research''
\end{quote}

In their paper, Broman et al define reproducibility as a project where you can take the original data and code used to perform the analysis and using these we create all of the numerical findings of the study.

This definition leads naturally to several follow-up questions.

Who exactly is \emph{you} in this definition? Does it specifically mean yourself in the future or should someone else with access to all that data and code be able to recreate your findings too? Also, should this reproducibility be limited to just the numerical results? Or should they also be able to create the associated figures, reports and press releases?

Another important question is \emph{when} this project needs to be reproduced. Will it be in a few weeks time or in 10 years time? Do you need to protect your project from changes in dependencies, like new versions of packages or modules? How about different versions of R or Python? Taking this time scale out even further, what about different operating systems and hardware?

It's unlikely you'd consider someone handing you a floppy disk of code that only runs on Windows XP to be acceptably reproducible. Sure, you could probably find a way to get it to work, but that would be an awful lot of effort on your end.

That's perhaps a bit of an extreme example, but it emphasizes the importance of clearly defining the level of reproducibility that you're aiming for within every project you work on. This example also highlights the amount of work that can be required to reproduce an analysis, especially after quite some time. It is important to explicitly think about how we dividing that effort between ourselves as the original developer and the person trying to reproduce the analysis in the future.

\hypertarget{ide-friendly}{%
\subsection{IDE Friendly}\label{ide-friendly}}

Our final desirable property is that we'd like our projects to play nicely
with integrated development environments.

When you're coding document and writing your data science projects it'd be possible for you to work entirely in either a plain text editor or typing code directly at the command line. While these approaches to a data science workflow have the benefit of simplicity, they also expect a great deal from you as a data scientist.

These workflows expect that you should type everything perfectly accurately every time, that you recall the names and argument orders of every function you use, and that you are constantly aware of the current state of all objects within your working environment.

Integrated Development Environments (IDEs) are applications that help to reduce this burden, helping make you a more effective programmer and data scientist. IDEs offer tools like code completion and highlighting to make your code easier to read and to write. They offer tools for debugging, to fix where things are going wrong, and they also offer environment panes so that you don't have to hold everything in your head all at once. Many IDEs also often have templating facilities. These let you save and reuse snippets of code so that you can avoid typing out repetitive, boilerplate code and introducing errors in the process.

Even if you haven't heard of IDEs before, you've likely already used one. Some common examples might be RStudio for R-users, PyCharm for python users, or Visual Studio as a more language agnostic coding environment.

Whichever of these we use, we'd like our project to play nicely with them. This lets us reap their benefits while keeping our project portable, version controlled, and reproducible for someone working with a different set-up.

\hypertarget{project-structure}{%
\section{Project Structure}\label{project-structure}}

I've given a pretty exhaustive argument for why having a single directory for each project is a good idea. Let's now take a look \emph{inside} that directory and define a common starting layout for the content of all of your projects.

Having this sort of project directory template will mean that you'll always know where to find what you're looking for and other members of your team will too. Again, before we start I'll reiterate that we're taking an opinionated approach here and providing a sensible starting point for organizing many projects.

Every project is going to be slightly different and some might require slight alterations to what I suggest here. Indeed, even if you start as I suggest then you might have to adapt your project structure as it develops and grows. I think it's helpful to consider yourself as a tailor when making these changes. I'm providing you with a one size fits all design, that's great for lots of projects but perfect for none of them. It's your job to alter and refine this design for each individual case.

One final caveat before we get started: companies and businesses will many times have a house style how to write and organize your code or projects. If that's the case, then follow the style guide that your business or company uses. The most important thing here is to be consistent at both an individual level and across the entire data science team. It's this consistency that reaps the benefits.

Okay, so imagine now that you've been assigned a shiny new project and have created a single directory in which to house that project. Here we've, quite imaginatively, called that directory \texttt{exciting-new-project}. What do we populate this folder with?

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-3}

In the rest of this video, I'll define the house-style for organizing the root directory of your data science projects in this module.

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-4}

Within the project directory there will be some subdirectories, which you can tell a folders in this file structure because they have a forward slash following their names. There will also be some files directly in the root directory. One of these is called \texttt{readme.md} and the another called either \texttt{makefile} or \texttt{make.r}. We're going to explore each of these files and directories in turn.

\hypertarget{readme.md}{%
\subsection{README.md}\label{readme.md}}

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-5}

Let's begin with the readme file. This gives a brief introduction to your project and gives information on what the project aims to do. The readme file should describe how to get started using the project and how to contribute to its development.

The readme is written either in a plain text format so \texttt{readme.txt} or in markdown
format \texttt{readme.md}. The benefit of using markdown is that it allows some light
formatting such as sections headers and lists using plain text characters.
Here you can see me doing that by using hashes to mark out first and second level headers and using bullet points for a unnumbered list. Whichever format you use, the readme file for your project is always stored in the root directory and is typically named in all uppercase letters.

The readme file should be the first thing that someone who's new to your project reads. By placing the readme in the root directory and capitalising the file name you are increase the visibility of this file and increae the chances of this actually happening.

An additional benefit to keeping the readme in the root directory of your project is that code hosting services like GitHub, GitLab or BitBucket will display the contents of that readme file next to the contents of your project. Those services will also nicely format any markdown that you use for you in your readme file.

When writing the readme, it can be useful to imaginge that you are writing this for a new, junior team member. The readme file should let them get started with the project and make some simple contributions after reading only that file. It might also link out to more detailed project documentation that will help the new team member toward a more advanced understanding or complex contribution.

\hypertarget{inside-the-readme}{%
\subsection{Inside the README}\label{inside-the-readme}}

let's take a quick aside to see in more detail what should be covered within a readme file.

A readme we should include the name of the project, which should be self-explanatory (so nothing like my generic choice of \texttt{exciting-new-project}). The readme should also give the project status, which is just a couple of sentences to say whether your project is still under development, the version oft the current release or, on the other end of the project life-cycle, if the project is being deprecated or
closed.

Following this, we should also include a description of your project. This will state the purpose of your work and to provide, or link to, any additional context or references that visitors aren't assumed to be familiar with.

If your project involves code or depends on other packages then you should give some instruction on how to install those dependencies and run your code. This might just be text but it could also include things like screenshots, code snippets, gifs or a video of the whole process.

It's also a good practice to include some simple examples of how to use the code within your project an the expected results, so that new users can confirm that everything is working on their local instance. Keep the examples as simple and minimal as you can so that new users

For longer or more complicated examples that aren't necessary in this short introductory document you can add links to those in the readme and explain them
in detail elsewhere.

There should ideally be a short description of how people can report issues with the project and also how people can get started in resolving those issues or extend the
project in some way.

That leads me on to one point that I've forgotten to list here. There there should be a section listing the authors of the work and the license in which under which it's
distributed. This is to give credit to all the people who've contributed to
your project and the license file then says how other people may use your work. The license declates how other may use your project and whether they have to give direct
attribution to your work in any modifications that they use.

\hypertarget{data}{%
\subsection{data}\label{data}}

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-7}

Moving back to our project structure, next we have the data directory.

The data directory will have two subdirectories one called \texttt{raw} and one called \texttt{derived}. All data that is not generate as part of your project is stored in the \texttt{raw} subdirectory. To ensure that a project is reproducible, data in the Raw folder should never be edited or modified.

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-8}

In this example we've got two different data types: an Excel spreadsheet the XLS file and a JSON file. These files are exacty as we received them from our project stakeholder.

The text file \texttt{metadata.txt} is a plain text file explaining the contents and interpretation of each of the raw data sets. This metadata should include descriptions of all the measured variables, the units that are recorded in, the date the file was created or acquired, and the source from which it was obtained.

The raw data likely isn't going to be in a form that's amenable to analyzing straight away. To get the data into a more pleasant form to work, it will require some data manipulation and cleaning. Any manipulation or cleaning that is applied should be well documented and the resulting cleaned files saved within the \texttt{derived} data directory.

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-12}

In our exciting new project, we can see the clean versions of the previous data sets which are ready for modelling. There's also a third file in this folder. This is data that we've acquired for ourselves through web scraping, using a script within the project.

\hypertarget{src}{%
\subsection{src}\label{src}}

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-16}

The \texttt{src} or source directory contains all the source code for your project. This will typically be the functions that you've written to make the analysis or modelling code more accessible.

Here we've saved each function in its own R script and, in this project, we've used subdirectories to organise these by their use case. We've got two functions used in data cleaning: the first replaces \texttt{NA} values with a given value, the second replaces these by the mean of all non-missing values.

We also have three helper functions: the first two calculate rolling mean and the
geometric mean of a given vector, the third is a function that scrapes the
web data we saw in the derived data subdirectory.

\hypertarget{tests}{%
\subsection{tests}\label{tests}}

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-12}

Moving on then to the tests directory. The structure of this directory mirrors that of the source directory. Each function file has its own counterpart file of tests.

These test files provide example sets of inputs and the expected outputs for each
function. The test files are used to check edge cases of a function or to assure
yourself that you haven't broken anything while fixing some small bug or adding new capabilities to that function.

\hypertarget{analyses}{%
\subsection{analyses}\label{analyses}}

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-23}

The analyses directory contains what you probably think of as the bulk of your
data science work. It's going to have one subdirectory for each major analysis that's performed within your project and within each of these there might be a series of steps that we collect into separate scripts.

The activity performed at each step is made clear by the name of each script, as is the order in which we're going to perform these steps. Here we can see the scripts used for the 2021 annual report. First is a script used to take the raw monthly receipts and produce the \emph{cleaned} version of the same data set that we saw earlier.
This is followed by a trend analysis of this cleaned data set.

Similarly for the spending review we have a data cleaning step, followed by some forecast modelling and finally the production of some diagnostic plots to compare these
forecasts.

\hypertarget{outputs}{%
\subsection{outputs}\label{outputs}}

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-26}

The outputs directory has again one subdirectory for each meta-analysis within the project. These are then further organized by the output type whether that be some data, a figure, or a table.

Depending on the nature of your project, you might want to use a modified subdirectory structure here. For example, if you're doing several numerical experiments then you might want to arrange your outputs by experiment, rather than by output type.

\hypertarget{reports}{%
\subsection{reports}\label{reports}}

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-29}

The reports directory is then where everything comes together. This is where the written documents that form the final deliverables of your project are created. If these final documents are written in LaTeX or markdown, both the source and
the compiled documents can be found within this directory.

When including content in this report, for example figures, I'd recommend against
making copies of those figure files within the reports directory. If you do that, then you'll have to manually update the files every time you modify them. Instead you can use relative file paths to include these figures. Relative file paths specify how to get to the image, starting from your TeX document and moving up and down through the levels of your project directory.

If you're not using markdown or LaTeX to write your reports, but instead use an online platform like overleaf as a latex editor or Google docs to write collaboratively then links to them in the reports directory using additional readme files. Make sure you set the read and write permissions for those links appropriately, too.

When using these online writing systems, you'll have to manually upload and update your plots whenever you modify any of your earlier analysis. That's one of the drawbacks of these online tools that has to be traded off against their ease of use.

In our exciting new project, here we can see that the annual report is written in a markdown format, which is compiled to both HTML and PDF. The spendiing review is written in LaTeX and we only have the source for it, we don't have the compiled pdf version of the document.

\hypertarget{make-file}{%
\subsection{make file}\label{make-file}}

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-32}

The final element of our template project structure is a make file. We aren't going to cover how to read or write make files in this course. Instead, I'll give you a brief description of what they are and what it is supposed to do.

At a high level, the make file is just a text file. What makes it special is what it contains. Similar to a shell or a bash script, make file contains code that could be run at the command line. This code will create or update each element of your project.

The make file defines shorthand commands for the full lines of code that create each element of your project. The make file also records the order in which these operations have to happen, and which of these steps are dependent on one another. This means that if one step part of your project is updated then any changes will be propagated through your entire project. This is done in quite a clever way so the only part of your projects that are re-run are those that need to be updated.

We're omitting make files from this course not because they're fiendishly difficult to write or read, but rather because they require a reasonable foundation in working at the command line to be understood. What I suggest you do instead throughout this course is to create your own R or markdown file called make. This file will define the intended running order and dependencies of your project and if it is an R file, it might also automate some parts of your analysis.

\hypertarget{wrapping-up}{%
\section{Wrapping up}\label{wrapping-up}}

Wrapping up then, that's everything for this chapter.

I've introduced a project structure that will serve you well as a baseline for the vast majority of projects in data science.

\includegraphics[width=0.8\linewidth]{images/101-workflows-organising-your-work/directory-structure-drawings/directory-structure-drawing-33}

In your own work, remember that the key here is standardisation. Working consistently across projects, a company or a group is more important than sticking rigidly to the particular structure that I have defined here.

There are two notable exceptions where you probably don't want to use this project structure. That's when you're building an app or you're building a package. These require specific organisation of the files within your project directory. We'll
explore the project structure used for package development during the live
session this week.

\hypertarget{workflows-naming}{%
\chapter{Naming Files}\label{workflows-naming}}

Effective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{quote}
``There are only two hard things in Computer Science: cache invalidation and naming things.''

\textbf{Phil Karlton, Netscape Developer}
\end{quote}

When working on a data science project we can in principle name directories, files, functions and other objects whatever we like. In reality though, using an ad-hoc system of naming is likely to cause confusion, headaches and mistakes. We obviously want to avoid all of those things, in the spirit of being kind to our current colleges and also to our future selves.

Coming up with good names is an art form. Like most art, naming things is an activity that you get better at with practice. Another similarity is that the best naming systems don't come from giving data scientists free reign over their naming system. Like all art, the best approaches to naming things give you strong guidelines and boundaries within which to express your creativity and skill.

In this lecture we'll explore what these boundaries and what we want them to achieve for us. The content of this lecture is based largely around a \href{https://speakerdeck.com/jennybc/how-to-name-files}{talk of the same name} given by Jennifer Bryan and the \href{https://style.tidyverse.org/}{tidyverse style guide}, which forms the basis of Google's style guide for R programming.

\hypertarget{naming-files}{%
\section{Naming Files}\label{naming-files}}

We'll be begin by focusing in on what we call our files. That is, we'll first focus on the part of the file name that comes before the dot. In the second part of this video, we'll then cycle back around to discuss file extensions.

\hypertarget{what-do-we-want-from-our-file-names}{%
\subsection{What do we want from our file names?}\label{what-do-we-want-from-our-file-names}}

Before we dive into naming files, we should first consider what we want from the file names that we choose. There are three key properties that that we would like to satisfy.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Machine Readable
\item
  Human Readable
\item
  Order Friendly
\end{enumerate}

Thee first desirable property is for file names to be easily readable by computers, the second is for the file names to be easily readable by humans and finally the file names should take advantage of the default ordering imposed on our files.

This set of current file names is sorely lacking across all of these properties:

\begin{verbatim}
abstract.docx
Effective Data Science's module guide 2022.docx 
fig 12.png
Rplot7.png
1711.05189.pdf
HR Protocols 2015 FINAL (Nov 2015).pdf
\end{verbatim}

We want to provide naming conventions to move us toward the better file names listed below.

\begin{verbatim}
2015-10-22_human-resources-protocols.pdf
2022_effective-data-science-module-guide.docx
2022_RSS-conference-abstract.docx 
fig12_earthquake-timeseries.png 
fig07_earthquake-location-map.png
ogata_1984_spacetime-clustering.pdf
\end{verbatim}

Let's take a few minutes to examine what exactly we mean by each of these properties.

\hypertarget{machine-readable}{%
\subsection{Machine Readable}\label{machine-readable}}

What do we mean by machine readable file names?

\begin{itemize}
\tightlist
\item
  Easy to compute on by \emph{deliberate use of delimiters}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{underscores\_separate\_metadata}, \texttt{hyphens-separate-words}.
  \end{itemize}
\item
  Play nicely with \emph{regular expressions} and \emph{globbing}:

  \begin{itemize}
  \tightlist
  \item
    avoid spaces, punctuation, accents, cases;
  \item
    \texttt{rm\ Rplot*.png}
  \end{itemize}
\end{itemize}

Machine readable names are useful when:

\begin{itemize}
\item
  \emph{managing files}: ordering, finding, moving, deleting:
\item
  \emph{extracting information} directly from file names,
\item
  \emph{working programmatically} with file names and regex.
\end{itemize}

When we are operating on a large number of files it is useful to be able to work with them programmatically.

One example of where this might be useful is when downloading assessments for marking. This might require me to unzip a large number of zip files, copying the pdf report from each unzipped folder into a single directory and all of the R scripts from each unzipped folder into another directory. The marked scripts and code then need to be paired back up in folders named by student, and re-zipped ready to be returned.

This is \emph{monotonously} dull and might work for \textasciitilde50 students but not for \textasciitilde5000. Working programmatically with files is the way to get this job done efficiently. This requires the file names to play nicely with the way that computers interpret file names, which they regard as a string of characters.

It is often helpful to have some metadata included in the file name, for example the student's id number and the assessment title. We will use an underscore to separate elements of metadata within the file name and a hyphen to separate sub-elements of meta-data, for example words within the assessment title.

Regular expressions and globbing are two ideas from string manipulation that you may not have met, but which will inform our naming conventions. Regular expressions allow you to search for strings (in our case file names) that match a particular pattern. Regular expressions can do really complicated searches but become gnarly when you have to worry about special characters like spaces, punctuation, accents and cases, so these should be avoided in file names.

A special type of regular expression is called globbing where a star is used to replace any number of subsequent characters in a file name, so that here we can delete all png images that begin with Rplot using a single line of code. Globbing becomes particular powerful when you use a consistent structure to create your file names.

As in the assessment marking example, having machine readable file names is particularly useful when managing files, such as ordering, finding, moving or deleting them. Another example of this is when your analysis requires you to load a large number of individual data files.

Machine readable file names are also useful for extracting meta-information from files without having to open them in memory. This is particularly useful when the files might be too large to load into memory, or you only want to load data from a certain year.

The final benefit we list here is the scalability, reduction in drudgery and lowered risk for human error when operating on a very large number of files.

\hypertarget{order-friendly}{%
\subsection{Order Friendly}\label{order-friendly}}

The next property we will focus on also links to how computers operate. We'd like our file names to exploit the default orderings used by computers. This means starting file names with character strings or metadata that allow us order our files in some meaningful way.

\hypertarget{running-order}{%
\subsubsection{Running Order}\label{running-order}}

One example of this is where there's some logical order in which your code should be executed, as in the example analysis below.

\begin{verbatim}
diagnositc-plots.R
download.R
runtime-comparison.R
...
model-evaluation.R
wrangle.R
\end{verbatim}

Prepreding numbers to these file names can make the intended ordering immediately obvious.

\begin{verbatim}
00_download.R
01_wrangle.R
02_model.R
...
09_model-evaluation.R
10_model-comparison-plots.R
\end{verbatim}

Starting single digit numbers with a leading 0 is a very good idea here to prevent script 1 being sorted in with the tens, script 2 in with the twenties and so on. If you might have over 100 files, for example when saving the output from many simulations, use two or more zeros to maintain this nice ordering.

\hypertarget{date-order}{%
\subsubsection{Date Order}\label{date-order}}

A second example of orderable file names is when the file has a date associated with it. This might be a version of a report or the date on which some data were recorded, cleaned or updated.

\begin{verbatim}
2015-10-22_human-resources-protocols.pdf
...
2022-effective-data-science-module-guide.docx
\end{verbatim}

When using dates, in file names or elsewhere, you should conform to the ISO standard date format.

\begin{quote}
ISO 8601 sets an international standard format for dates: \texttt{YYYY-MM-DD}.
\end{quote}

This format uses four numbers for the year, followed by two numbers for the month and two numbers of the day of the month. This structure mirrors a nested file structure moving from least to most specific. It also avoids confusion over the ordering of the date elements. Without using the ISO standard a date like 04-05-22 might be interpreted as the fourth of May 2022, the fifth of April 2022, or the 22nd of May 2004.

\hypertarget{human-readable}{%
\subsection{Human Readable}\label{human-readable}}

The final property we would like our file names to have is human readability. This requires the names of our files to be meaningful, informative and easily read by real people.

The first two of these are handled by including appropriate metadata in the file name. The ease with which these are read by real people is determined by the length of the file name and by how that name is formatted.

There are lots of formatting options with fun names like \texttt{camelCase}, \texttt{PascalCase}, and \texttt{snake\_case}.

\begin{verbatim}
   easilyReadByRealPeople (camelCase)
   EasilyReadByRealPeople (PascalCase)
   easily_read_by_real_people (snake_case)
   easily-read-by-real-people (skewer-case)
\end{verbatim}

here's weak evidence that suggests snake and skewer cases are most the readable. We'll use a mixture of these, using snake case \emph{between} metadata items and skewer case \emph{within} them. This has a slight cost to legibility, in a trade-off against making computing on these file names easier.

The final aspect that you have control over is the length of the name. Having short, evocative and useful file names is not easy and is a skill in itself. For some hints and tips you might want to look into tips for writing URL slugs. These are last part of a web address that are intended to improve accessibility by being immediately and intuitively meaningful to any user.

\hypertarget{naming-files---summary}{%
\subsection{Naming Files - Summary}\label{naming-files---summary}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  File names should be meaningful, informative and scripts end in \texttt{.r}
\item
  Stick to letters, numbers underscores (\texttt{\_}) and hyphens (\texttt{-}).
\item
  Pay attention to capitalisation \texttt{file.r} \(\neq\) \texttt{File.r} on all operating systems.
\item
  Show order with left-padded numbers or ISO dates.
\end{enumerate}

\hypertarget{file-extensions-and-where-you-work}{%
\section{File Extensions and Where You Work}\label{file-extensions-and-where-you-work}}

So far we have focused entirely on what comes before the dot, that is the file name.Equally, if not more, important is what comes after the dot, the file extension.

\begin{verbatim}
example-script.r
example-script.py

project-writeup.doc
project-writeup.tex
\end{verbatim}

The file extension describes how information is stored in that file and determines the software that can use, view or run that file.

You likely already use file extensions to distinguish between code scripts, written documents, images, and notebook files. We'll now explore the benefits and drawbacks of various file types with respect to several important features.

\hypertarget{open-source-vs-proprietary-file-types}{%
\subsection{Open Source vs Proprietary File Types}\label{open-source-vs-proprietary-file-types}}

The first feature we'll consider is whether the file type is open source, and can be used by anyone without charge, or if specialist software must be paid for in order to interact with those files.

\includegraphics[width=0.8\linewidth]{images/102-workflows-naming-files/file-types-image}

In the figure above, each column represents a different class of file, moving left to right we have example file types for tabular data, list-like data and text documents. File types closer to the top are open source while those lower down rely on proprietary software, which may or may not require payment.

To make sure that our work is accessible to as many people as possible we should favour the open source options like csv files over Google sheets or excel, JSON files over Matlab data files, and tex or markdown over a word or Google doc.

This usually has a benefit in terms of project longevity and scalability. The open source file types are often somewhat simpler in structure, making them more robust to changes over time less memory intensive.

To see this, let's take a look inside some data files.

\hypertarget{inside-data-files}{%
\subsection{Inside Data Files}\label{inside-data-files}}

\hypertarget{inside-a-csv-file}{%
\subsubsection{Inside a CSV file}\label{inside-a-csv-file}}

CSV or comma separated value files are used to store tabular data.

In tabular data, each row of the data represents one record and each column represents a data value.

A csv encodes this by having each record on a separate line and using commas to separate values with that record. You can see this by opening a csv file in a text editor such as notepad.

The raw data stores line breaks using \texttt{\textbackslash{}n} and indicates new rows by \texttt{\textbackslash{}r}. These backslashed indicae that these are \emph{escape characters} with special meanings, and should not be literally interpreted as the letters n and r.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{read\_file}\NormalTok{(}\AttributeTok{file =} \StringTok{"images/102{-}workflows{-}naming{-}files/example.csv"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Name,Number\textbackslash{}r\textbackslash{}nA,1\textbackslash{}r\textbackslash{}nB,2\textbackslash{}r\textbackslash{}nC,3"}
\end{Highlighting}
\end{Shaded}

When viewed in a text editor, the example file would look something like this.

\begin{verbatim}
Name,Number 
A,1
B,2
C,3
\end{verbatim}

\hypertarget{inside-a-tsv-file}{%
\subsubsection{Inside a TSV file}\label{inside-a-tsv-file}}

TSV or tab separated value files are also used to store tabular data.

Like in a csv each record is given on a new line but in a tsv tabs rather than commas are used to separate values with each record. This can also be seen by opening a tsv file in a text editor such as notepad.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{read\_file}\NormalTok{(}\AttributeTok{file =} \StringTok{"images/102{-}workflows{-}naming{-}files/example.tsv"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Name\textbackslash{}tNumber\textbackslash{}r\textbackslash{}nA\textbackslash{}t1\textbackslash{}r\textbackslash{}nB\textbackslash{}t2\textbackslash{}r\textbackslash{}nC\textbackslash{}t3"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Name    Number }
\NormalTok{A   1}
\NormalTok{B   2}
\NormalTok{C   3}
\end{Highlighting}
\end{Shaded}

One thing to note is that tabs are a separate character and are not just multiple spaces. In plain text these can be impossible to tell apart, so most text editors have an option to display tabs differently from repeated spaces, though this is usually not enabled by default.

\hypertarget{inside-an-excel-file}{%
\subsubsection{Inside an Excel file}\label{inside-an-excel-file}}

When you open an excel file in a text editor, you will immediately see that this is not a human interpretable file format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{504b 0304 1400 0600 0800 0000 2100 62ee}
\NormalTok{9d68 5e01 0000 9004 0000 1300 0802 5b43}
\NormalTok{6f6e 7465 6e74 5f54 7970 6573 5d2e 786d}
\NormalTok{6c20 a204 0228 a000 0200 0000 0000 0000}
\NormalTok{0000 0000 0000 0000 0000 0000 0000 0000}
\NormalTok{.... .... .... .... .... .... .... ....}
\NormalTok{0000 0000 0000 0000 ac92 4d4f c330 0c86}
\NormalTok{ef48 fc87 c8f7 d5dd 9010 424b 7741 48bb}
\NormalTok{2154 7e80 49dc 0fb5 8da3 241b ddbf 271c}
\NormalTok{1054 1a83 0347 7fbd 7efc cadb dd3c 8dea}
\NormalTok{.... .... .... .... .... .... .... ....}
\end{Highlighting}
\end{Shaded}

Each entry here is a four digit hexadecimal number and there are a lot more of them than we have entries in our small table.

This is because excel files can carry a lot of additional information that a csv or tsv are not able to, for example cell formatting or having multiple tables (called sheets by excel) stored within a single file.

This means that excel files take up much more memory because they are carrying a lot more information than is strictly contained within the data itself.

\hypertarget{indise-a-json-file}{%
\subsubsection{Indise a JSON file}\label{indise-a-json-file}}

JSON, or Java Script Object Notation, files are an open source format for list-like data. Each record is represented by a collection of \texttt{key:value} pairs. In our example table each entry has two fields, one corresponding to the \texttt{Name} key and one corresponding to the \texttt{Number} key.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{[}\FunctionTok{\{}
    \DataTypeTok{"Name"}\FunctionTok{:} \StringTok{"A"}\FunctionTok{,}
    \DataTypeTok{"Number"}\FunctionTok{:} \StringTok{"1"}
\FunctionTok{\}}\OtherTok{,} \FunctionTok{\{}
    \DataTypeTok{"Name"}\FunctionTok{:} \StringTok{"B"}\FunctionTok{,}
    \DataTypeTok{"Number"}\FunctionTok{:} \StringTok{"2"}
\FunctionTok{\}}\OtherTok{,} \FunctionTok{\{}
    \DataTypeTok{"Name"}\FunctionTok{:} \StringTok{"C"}\FunctionTok{,}
    \DataTypeTok{"Number"}\FunctionTok{:} \StringTok{"3"}
\FunctionTok{\}}\OtherTok{]}
\end{Highlighting}
\end{Shaded}

This list-like structure allows non-tabular data to be stored by using a property called nesting: the value taken by a key can be a single value, a vector of values or another list-like object.

This ability to create nested data structures has lead to this data format being used widely in a range of applications that require data transfer.

\hypertarget{inside-an-xml-file}{%
\subsubsection{Inside an XML file}\label{inside-an-xml-file}}

XML files are another open source format for list-like data, where each record is represented by a collection of \texttt{key:value} pairs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{\textless{}?xml}\NormalTok{ version="1.0" encoding="UTF{-}8"}\KeywordTok{?\textgreater{}}
\KeywordTok{\textless{}root\textgreater{}}
  \KeywordTok{\textless{}row\textgreater{}}
    \KeywordTok{\textless{}Name\textgreater{}}\NormalTok{A}\KeywordTok{\textless{}/Name\textgreater{}}
    \KeywordTok{\textless{}Number\textgreater{}}\NormalTok{1}\KeywordTok{\textless{}/Number\textgreater{}}
  \KeywordTok{\textless{}/row\textgreater{}}
  \KeywordTok{\textless{}row\textgreater{}}
    \KeywordTok{\textless{}Name\textgreater{}}\NormalTok{B}\KeywordTok{\textless{}/Name\textgreater{}}
    \KeywordTok{\textless{}Number\textgreater{}}\NormalTok{2}\KeywordTok{\textless{}/Number\textgreater{}}
  \KeywordTok{\textless{}/row\textgreater{}}
  \KeywordTok{\textless{}row\textgreater{}}
    \KeywordTok{\textless{}Name\textgreater{}}\NormalTok{C}\KeywordTok{\textless{}/Name\textgreater{}}
    \KeywordTok{\textless{}Number\textgreater{}}\NormalTok{3}\KeywordTok{\textless{}/Number\textgreater{}}
  \KeywordTok{\textless{}/row\textgreater{}}
\KeywordTok{\textless{}/root\textgreater{}}
\end{Highlighting}
\end{Shaded}

The difference from a JSON file is mainly in how those records are formatted within the file. In a JSON file this is designed to look like objects in the Java Script programming language and in XML the formatting is done to look like html, the markup language used to write websites.

\hypertarget{a-note-on-notebooks}{%
\subsection{A Note on Notebooks}\label{a-note-on-notebooks}}

\begin{itemize}
\item
  There are two and a half notebook formats that you are likely to use to: \texttt{.rmd} (alternatively \texttt{.qmd}) and \texttt{.ipynb}.
\item
  R markdown documents \texttt{.rmd} are plain text files, so are very human friendly.
\item
  \textbf{\emph{JuPy}}te\textbf{\emph{R}} notebooks have multi-language support but are not so human friendly (JSON in disguise).
\item
  Quarto documents offer the best of both worlds and more extensive language support. Not yet as established as a format.
\end{itemize}

In addition to the files you read and write, the files that you code in will largely determine your workflow.

There are three main options for the way that you code: first is typing it directly at the command line, second is using a text editor or IDE to write scripts and third is writing a notebook that mixes code. text and output together in a single document.

We'll compare these methods of working on the next slide, but first let's do a quick review of what notebooks are available to you and why you might want to use them.

As a data scientist, there are two and a half notebook formats that you're likely to have met before. The first two are Rmarkdown files for those working predominantly in R and interactive python or jupyter notebooks for those working predominantly in python. The final half format are quarto markdown documents, which are relatively new and extend the functionality of Rmarkdown files.

The main benefit of R markdown documents is that they're plain text files, so they're very human friendly. \textbf{\emph{JuPy}}te\textbf{\emph{R}} notebooks have the benefit of supporting code written in Julia, Python or R, but are not so human friendly - under the hood these documents are JSON files that should not be edited directly because a misplaced bracket will break them.

Quarto documents offer the best of both worlds, with plain text formatting and even more extensive language support than jupyter notebooks. Quarto is a recent extension of Rmarkdown, which is rapidly becoming popular in the data science community.

Each format has its benefits and drawbacks depending on the context in which they are used and all have some shared benefits and limitations by nature of them all being notebook documents.

\hypertarget{file-extensions-and-where-you-code}{%
\subsection{File Extensions and Where You Code}\label{file-extensions-and-where-you-code}}

\begin{longtable}[]{@{}lccc@{}}
\toprule()
Property & Notebook & Script & Command Line \\
\midrule()
\endhead
reproducible & \textasciitilde{} &  & X \\
readable & \textasciitilde{} &  & \textasciitilde{} \\
self-documenting &  & X & X \\
in production & X &  & \textasciitilde{} \\
ordering / automation & \textasciitilde{} &  & \textasciitilde{} \\
\bottomrule()
\end{longtable}

The main benefit of notebook documents is that they are self-documenting, in that they can mix the documentation, code and report all into a single document. Notebooks also they provide a level of interactivity when coding that is not possible when working directly at the command line or using a text editor to write scripts. This second factor is easily overcome by using an integrated development environment when scripting.

Writing code in .r files is not self-documenting but this separation of code, documentation and outputs has many other benefits. Firstly, the resulting scripts provide a reproducible and automatable workflow, unlike one-off lines of code being run at the command line. Secondly, using an IDE to write these provides you with syntax highlighting and code linting features to help you write readable and accurate code. Finally, the separation of code from documentation and output allows your work to be more easily or even directly put into production.

In this course we will advocate for a scripting-first approach to data science, though notebooks and command line work definitely have their place.

Notebooks are great as teaching and rapid development tools but have strong limitations with being put into production. Conversely, coding directly at the command line can leave no trace of your workflow and lead to an analysis that cannot be replicated in the future.

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

Finally, let's wrap things up by summarising what we have learned about naming files.

Before the dot we want to pick file names that machine readable, human friendly and play nicely with the default orderings provided to us.

\begin{quote}
\textbf{Name files so that they are:}

\begin{itemize}
\tightlist
\item
  Machine Readable,
\item
  Human Readable,
\item
  Order Friendly.
\end{itemize}
\end{quote}

After the dot, we want to pick file types that are widely accessible, easily read by humans and allow for our entire analysis to be reproduced.

\begin{quote}
\textbf{Use document types that are:}

\begin{itemize}
\tightlist
\item
  Widely accessible,
\item
  Easy to read and reproduce,
\item
  Appropriate for the task at hand.
\end{itemize}
\end{quote}

Above all we want to name our files and pick our file types to best match with the team we are working in and the task that we is at hand.

\hypertarget{workflows-code}{%
\chapter{Code}\label{workflows-code}}

Effective Data Science is still a work-in-progress. This chapter should be readable but is currently undergoing final polishing.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

We have already described how we might organise an effective data science project at the directory and file level. In this chapter we will delve one step deeper and consider how we can structure our work within those files. In particular, we'll focus on code files here.

We'll start by comparing the two main approaches to structuring our code, namely functional programming and object oriented programming.

We'll then see how we should order code within our scripts and conventions on how to name the functions and objects that we work with in our code.

Rounding up this chapter, I'll summarise the main points from the R style guide that we will be following in this course and highlight some useful packages for writing effective code.

\hypertarget{functional-programming}{%
\section{Functional Programming}\label{functional-programming}}

A functional programming style has two major properties:

\begin{itemize}
\item
  Object immutability,
\item
  Complex programs written using function composition.
\end{itemize}

Firstly, the original data or objects aren't modified or altered by the code. We have met this idea before when making new, cleaner versions of our raw data but leave that original messy data intact. Object immutability is the exact same idea but in a code context rather than data context.

Secondly, In functional programming, complex problems are solved by decomposing them into a series of smaller problems. A separate, self-contained function is then written to solve each sub-problem. Each individual function is, in itself, simple and easy to understand. This makes these small functions easy to test and easy to reuse in many places. Code complexity is then built up by composing these functions in various ways.

It can be difficult to get into this way of thinking, but people with mathematical training often find it quite natural. This is because mathematicians have many years of experience in working with function compositions in the abstract, mathematical sense.

\[y = g(x) = f_3 \circ f_2 \circ f_1(x).\]

\hypertarget{the-pipe-operator}{%
\subsection{The Pipe Operator}\label{the-pipe-operator}}

One issue with functional programming is that lots of nested functions means that there are also lots of nested brackets. These start to get tricky to keep track of when you have upwards of 3 functions being composed. This reading difficulty is only exacerbated if your functions have additional arguments on top of the original inputs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{log}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\FunctionTok{cos}\NormalTok{(}\FunctionTok{sin}\NormalTok{(pi))))}
\CommentTok{\#\textgreater{} [1] 1}
\end{Highlighting}
\end{Shaded}

The pipe operator \texttt{\%\textgreater{}\%} from the \texttt{\{magrittr\}} package helps with this issue. It works exactly like function composition: it takes the whatever is on the left (whether that is an object or the output of a function) and passes it to the following function call as the first argument of that function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{pi }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{sin}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cos}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{exp}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{log}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\CommentTok{\#\textgreater{} 1          5.1         3.5          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 2          4.9         3.0          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 3          4.7         3.2          1.3         0.2  setosa}
\end{Highlighting}
\end{Shaded}

The pipe operator is often referred to as ``syntactic sugar''. This is because it doesn't add anything to your code in itself but rather it makes your code \emph{so} much more palatable.

In R versions 4.1 and greater, there's a built-in version of this pipe operator, which is written using the vertical bar symbol followed by a greater than sign. To type the vertical bar, you can usually find it found above backslash on the keyboard.

(Just to cause confusion, the vertical bar symbol is also called the pipe symbol in general programming contexts. )

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{pi }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sin}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cos}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{exp}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{log}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 1}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}   Sepal.Length Sepal.Width Petal.Length Petal.Width Species}
\CommentTok{\#\textgreater{} 1          5.1         3.5          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 2          4.9         3.0          1.4         0.2  setosa}
\CommentTok{\#\textgreater{} 3          4.7         3.2          1.3         0.2  setosa}
\end{Highlighting}
\end{Shaded}

The base R pipe usually behaves in the same way as the pipe from magrittr, but there are a few cases where they differ. For reasons of back-compatibility and consistency we'll stick to the magrittr pipe in this course.

\hypertarget{when-not-to-pipe}{%
\subsection{When not to pipe}\label{when-not-to-pipe}}

Pipes are designed to put focus on the the actions you are performing rather than the object that you are preforming those operations on. This means that there are two cases where you should almost certainly not use a pipe.

The first of these is when you need to manipulate more than one object at a time. Using secondary objects as reference points (but leaving them unchanged) is of course perfectly fine, but pipes should be used when applying a sequence of steps to create a new, modified version of one primary object.

Secondly, just because you \emph{can} chain together many actions into a single pipeline, that doesn't mean that you necessarily should. Very long sequences of piped operations are easier to read than nested functions, however they still burden the reader with the same cognitive load on their short term memory. Be kind and create meaningful, intermediate objects with informative names. This'll help the reader to more easily understand the logic within your code.

\hypertarget{object-oriented-programming}{%
\section{Object Oriented Programming}\label{object-oriented-programming}}

The main alternative to functional programming is object oriented programming.

\begin{itemize}
\tightlist
\item
  Solve problems by using lots of simple objects
\item
  R has 3 OOP systems: S3, S4 and R6.
\item
  Objects belong to a class, have methods and fields.
\item
  Example: agent based simulation of beehive.
\end{itemize}

\hypertarget{oop-philosophy}{%
\subsection{OOP Philosophy}\label{oop-philosophy}}

In functional programming, we solve complicated problems by using lots of simple functions. In object oriented programming we solve complicated problems using lots of simple objects. Which of these programming approaches is best will depend on the particular type of problem that you are trying to solve.

Functional programming is excellent for most types of data science work. Object oriented comes into its own when your problem has many small components interacting with one another. This makes it great for things like designing agent-based simulations, which I'll come back to in a moment.

In R there are three different systems for doing object oriented programming (called S3, S4, and R6), so things can get a bit complicated. We won't go into detail about them here, but I'll give you an overview of the main ideas.

This approach to programming might be useful for you in the future, for example if you want to extend base R functions to work with new types of input, and to have user-friendly displays. In that case (Advanced R){[}\url{https://adv-r.hadley.nz/}{]} by Hadley Wickham is an excellent reference text.

In OOP, each object belongs to a class and has a set of methods associated with it. The class defines what an object \emph{is} and methods describe what that object can \emph{do}. On top of that, each object has class-specific attributes or data fields. These fields are shared by all objects in a class but the values that they take give information about that specific object.

\hypertarget{oop-example}{%
\subsection{OOP Example}\label{oop-example}}

This is is all sounding very abstract. Let's consider writing some object oriented code to simulate a beehive. Each object will be a bee, and each bee is an instance of one of three bee classes: it might be a queen, a worker or a drone for example. Different bee classes have different methods associated with them, which describe what the bee can do, for example all bees would have 6 methods that let them move up, down, left, right, forward and backward within the hive. An additional ``reproduce'' method might only be defined for queen bees and a pollinate method might only be defined for workers. Each instance of a bee has its own fields, which give data about that specific bee. All bees have x, y and z coordinate fields giving their location within the hive. The queen class might have an additional field for their number of offspring and the workers might have an additional field for how much pollen they are carrying.

As the simulation progresses, methods are applied to each object altering their fields and potentially creating or destroying objects. This is very different from the preservation mindset of functional programming, but hopefully you can see that it is a very natural approach to many types of problem.

\hypertarget{structuring-r-script-headers}{%
\section{Structuring R Script Headers}\label{structuring-r-script-headers}}

\begin{quote}
TL;DR
- Start script with a comment of 1-2 sentences explaining what it \textgreater{} does.
- \texttt{setwd()} and \texttt{rm(ls())} are the devil's work.
- ``Session'' \textgreater{} ``Restart R'' or Keyboard shortcut: crtl/cmd + shift \textgreater{} + 0
- Polite to gather all \texttt{library()} and \texttt{source()} calls.
- Rude to mess with other people's set up using \textgreater{} \texttt{install.packages()}.
- Portable scripts use paths relative to the root directory of the project.
\end{quote}

First things first, let's discuss what should be at the top of your R scripts.

It is almost always a good idea to start your file with a few commented out sentences describing the purpose of the script and, if you work in a large team, perhaps who contact with any questions about this script. (There is more on comments coming up soon, don't worry!)

It is also good practise to move all \texttt{library()} and \texttt{source()} calls to the top of your script. These indicate the packages and helper function that are dependencies of your script; it's useful to know what you need to have installed before trying to run any code.

That segues nicely to the next point, which is never to hard code package installations. It is extremely bad practise and very rude to do so because then your script might alter another person's R installation. If you don't know already, this is precisely the difference between an \texttt{install.packages()} and \texttt{library()} call: \texttt{install.packages()} will download the code for that package to the users computer, while \texttt{library()} takes that downloaded code and makes it available in the current R session. To avoid messing with anyone's R installation, you should always type \texttt{install.package()} commands directly in the console and then place the corresponding \texttt{library()} calls within your scripts.

Next, it is likely that you, or someone close to you, will commit the felony of starting every script by setting the working directory and clearing R's global environment. This is \emph{very} bad practice, it's indicative of a workflow that's not project based and it's problematic for at least two reasons. Firstly, the path you set will likely not work on anyone else's computer. Secondly, clearing the environment like this may \emph{look} like it gets you back to fresh, new R session but all of your previously loaded packages will still be loaded and lurking in the background.

Instead, to achieve your original aim of starting a new R session, go to the menu and select the ``Session'' drop down then select ``Restart R''. Alternatively, you can use keyboard shortcuts to do the same. This is ``crtl + shift + 0'' on Windows and ``cmd + shift + 0'' on a mac. The fact that a keyboard shortcut exists for this should quite strongly hint that, in a reproducible and project oriented workflow, you should be restarting R quite often in an average working day. This is the scripting equivalent of ``clear all output and rerun all'' in a notebook.

Finally, let's circle back to the point I made earlier about setting the working directory. The reason that this will not work is because you are likely giving file paths that are specific to your computer, your operating system and your file organisation system. The chances of someone else having all of these the same are practically zero.

\hypertarget{portable-file-paths-with-here}{%
\section{\texorpdfstring{Portable File paths with \texttt{\{here\}}}{Portable File paths with \{here\}}}\label{portable-file-paths-with-here}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bad {-} breaks if project moved}
\FunctionTok{source}\NormalTok{(}\StringTok{"zaks{-}mbp/Desktop/exciting{-}new{-}project/src/helper\_functions/rolling\_mean.R"}\NormalTok{)}

\CommentTok{\# Better {-} breaks if Windows}
\FunctionTok{source}\NormalTok{(}\StringTok{"../../src/helper\_functions/rolling\_mean.R"}\NormalTok{)}

\CommentTok{\# Best {-} but use here:here() to check root directory correctly identified}
\FunctionTok{source}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"src"}\NormalTok{,}\StringTok{"helper\_functions"}\NormalTok{,}\StringTok{"rolling\_mean.R"}\NormalTok{))}

\CommentTok{\# For more info on the here package:}
\FunctionTok{vignette}\NormalTok{(}\StringTok{"here"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To fix the problem of person- and computer-specific file paths you can have two options.

The first is to use \emph{relative} file paths. In this you assume that each R script is being run in its current location and my moving up and down through the levels of your project directory you point to the file that you need.

This is good in that it solves the problem of paths breaking because you move the project to a different location on your own laptop. However, it does not fully solve the portability problem because you might move your file to a different location \emph{within} the same project. It also does not solve the problem that windows uses MacOS and linux use forward slashes in file paths with widows uses backslashes.

To resolve these final two issues I recommend using the \texttt{here()} function from the \{here\} package. This package looks for a \texttt{.Rproj} or \texttt{.git} file to identify the root directory of your project and creates file paths relative to the root of your project, that are suitable for the operating system the code is being run on.

It really is quite marvellous. For more information on how to use the here package, explore its chapter in \href{https://rstats.wtf/project-oriented-workflow.html}{R - What They Forgot}, \href{https://r4ds.had.co.nz/workflow-projects.html}{R for Data Science} or this \href{https://www.tidyverse.org/blog/2017/12/workflow-vs-script/}{project oriented workflow blog post}.

\hypertarget{code-body}{%
\section{Code Body}\label{code-body}}

Moving on now, we will go from the head to the body of the code. Having well named and organised code will facilitate both reading and understanding. Comments and sectioning do the rest of this work.

This section is designed as an introduction to the \href{https://style.tidyverse.org/}{tidyverse style guide} and not as a replacement to it.
\#\#\# Comments

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This is an example script showing good use of comments and sectioning }

\FunctionTok{library}\NormalTok{(here)}
\FunctionTok{source}\NormalTok{(}\FunctionTok{here}\NormalTok{(}\StringTok{"src"}\NormalTok{,}\StringTok{"helper\_functions"}\NormalTok{,}\StringTok{"rolling\_mean.R"}\NormalTok{))}

\CommentTok{\#===============================================================================  \textless{}{-} 80 characters max for readability}
\CommentTok{\# Major Section on Comments {-}{-}{-}{-}}
\CommentTok{\#===============================================================================}

\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\#  Minor Section on inline comments {-}{-}{-}{-} }
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10} \CommentTok{\# this is an inline comment}

\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\DocumentationTok{\#\#  Minor Section on full line comments {-}{-}{-}{-} }
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\FunctionTok{rolling\_mean}\NormalTok{(x)}
\CommentTok{\# This is an full line comment}
\end{Highlighting}
\end{Shaded}

Comments may be either short in-line comments at the end of a line or full lines dedicated to comments. To create either type of comment in R, simply type hash followed by one space. The rest of that line will not be evaluated and will function as a comment. If multi-line comments are needed simply start multiple lines with a hash and a space.

The purpose of these comments is to explain the \emph{why} of what you are doing, not the what. If you are explaining \emph{what} you are doing in most of your comments then you perhaps need to consider writing more informative function names, something we will return to in the general advice section.

Comments can also be used to add structure to your code, buy using commented lines of hyphens and equal signs to chunk your files into minor and major sections.

Markdown-like section titles can be added to these section and subsection headers. Many IDEs, such as RStudio, will interpret these as a table of contents for you, so that you can more easily navigate your code.

\hypertarget{objects-are-nouns}{%
\subsection{Objects are Nouns}\label{objects-are-nouns}}

\begin{itemize}
\item
  Object names should use only lowercase letters, numbers, and \texttt{\_}.
\item
  Use underscores (\texttt{\_}) to separate words within a name. (\texttt{snake\_case})
\item
  Use nouns, preferring singular over plural names.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good}
\NormalTok{day\_one}
\NormalTok{day\_1}

\CommentTok{\# Bad}
\NormalTok{first\_day\_of\_the\_month}
\NormalTok{DayOne}
\NormalTok{dayone}
\NormalTok{djm1}
\end{Highlighting}
\end{Shaded}

When creating and naming objects a strong guideline in that objects should be named using short but meaningful nouns. Names should not include any special characters and should use underscores to separate words within the object name.

This is similar to our file naming guide, but note that hyphens can't be used in object names because this conflicts with the subtraction operator.

When naming objects, as far as possible use singular nouns. The main reason for this is that the plurisation rules in English are complex and will eventually trip up either you or a user of your code.

\hypertarget{functions-are-verbs}{%
\subsection{Functions are Verbs}\label{functions-are-verbs}}

\begin{itemize}
\item
  Function names should use only lowercase letters, numbers, and \texttt{\_}.
\item
  Use underscores (\texttt{\_}) to separate words within a name. (\texttt{snake\_case})
\item
  Suggest imperative mood, as in a recipe.
\item
  Break long functions over multiple lines. 4 vs 2 spaces.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good}
\FunctionTok{add\_row}\NormalTok{()}
\FunctionTok{permute}\NormalTok{()}

\CommentTok{\# Bad}
\FunctionTok{row\_adder}\NormalTok{()}
\FunctionTok{permutation}\NormalTok{()}

\NormalTok{long\_function\_name }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
    \AttributeTok{a =} \StringTok{"a long argument"}\NormalTok{,}
    \AttributeTok{b =} \StringTok{"another argument"}\NormalTok{,}
    \AttributeTok{c =} \StringTok{"another long argument"}\NormalTok{) \{}
  \CommentTok{\# As usual code is indented by two spaces.}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The guidelines for naming functions are broadly similar, with the advice that functions should be verbs rather than nouns.

Functions should be named in the imperative mood, like in a recipe. This is again for consistency; having function names in a range of moods and tenses leads to coding nightmares.

As with object names you should aim to give your functions and their arguments short, evocative names. For functions with many arguments or a long name, you might not be able to fit the function definition on a single line. In this case you can should place each argument on its own double indented line and the function body on a single indented line.

\hypertarget{casing-consistantly}{%
\subsection{Casing Consistantly}\label{casing-consistantly}}

As we have mentioned already, we have many options for separating words within names:

\begin{itemize}
\tightlist
\item
  \texttt{CamelCase}
\item
  \texttt{pascalCase}
\item
  \texttt{snakecase}
\item
  \texttt{underscore\_separated} 
\item
  \texttt{hyphen-separated}
\item
  \texttt{point.separated} 
\end{itemize}

For people used to working in Python it is tempting to use point separation in function names, in the spirit of methods from object oriented programming. Indeed, some base R functions even use this convention.

However, the reason that we advise against it is because it is already used for methods in some of R's inbuilt OOP functionality. We will use underscore separation in our work.

\hypertarget{style-guide-summary}{%
\subsection{Style Guide Summary}\label{style-guide-summary}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use comments to structure your code
\item
  Objects = Nouns
\item
  Functions = Verbs
\item
  Use snake case and consistant grammar
\end{enumerate}

\hypertarget{further-tips-for-friendly-coding}{%
\section{Further Tips for Friendly Coding}\label{further-tips-for-friendly-coding}}

In addition to naming conventions the style guide gives lots of other guidance on writing code in a way that is kind to future readers of that code.

I'm not going to go repeat all of that guidance here, but the motivation for all of these can be boiled down into the following points.

\begin{itemize}
\item
  Write your code to be easily understood by humans.
\item
  Use informative names, typing is cheap.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bad}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in}\NormalTok{ dmt)\{}
  \FunctionTok{print}\NormalTok{(i)}
\NormalTok{\}}

\CommentTok{\# Good}
\ControlFlowTok{for}\NormalTok{(temperature }\ControlFlowTok{in}\NormalTok{ daily\_max\_temperature)\{}
  \FunctionTok{print}\NormalTok{(temperature)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Divide your work into logical stages, human memory is expensive.
\end{itemize}

When writing your code, keep that future reader in mind. This means using names that are informative and reasonably short, it also means adding white space, comments and formatting to aid comprehension. Adding this sort of structure to your code also helps to reduce the cognitive burden that you are placing on the human reading your code.

Informative names are more important than short names. This is particularly true when using flow controls, which are things like for loops and while loops. Which of these for loops would you like to encounter when approaching a deadline or urgently fixing a bug? Almost surely the second one, where context is immediately clear.

A computer doesn't care if you call a variable by only a single letter, by a random key smash (like \texttt{aksnbioawb}) or by an informative name. A computer also doesn't care if you include no white space your code - the script will still run. However, doing these things are friendly practices that can help yourself when debugging and your co-workers when collaborating.

\hypertarget{reduce-reuse-recycle}{%
\section{Reduce, Reuse, Recycle}\label{reduce-reuse-recycle}}

In this final section, we'll look at how you can make your workflow more efficient by reducing the amount of code you write, as well as reusing and recycling code that you've already written.

\hypertarget{dry-coding}{%
\subsection{DRY Coding}\label{dry-coding}}

This idea of making your workflow more efficient by reducing, reusing and recycling your code is summarised by the DRY acronym: don't repeat yourself.

This can be boiled down to three main points:

\begin{quote}
\begin{itemize}
\tightlist
\item
  if you do something twice in a single script, then write a function to do that thing,
\item
  if you want to use your function elsewhere \emph{within} your project, then save it in a separate script
\item
  If you want to use your function \emph{across} projects, then add it to a package.
\end{itemize}
\end{quote}

Of course, like with scoping projects in the first place, this requires some level of clairvoyance: you have to be able to look into the future and see whether you'll use a function in another script or project. This is difficult, bordering on impossible. So in practice, this is done retrospectively - you find a second script or project that needs a function then pull it out its own separate file or include it in a package.

As a rule of thumb, if you are having to consider whether or not to make the function more widely available then you should do it. It takes much less effort to do this work now, while it's fresh in your mind, than to have to refamiliarise yourself with the code in several years time.

Let's now look at how to implement those sub-bullet points:
``when you write a function, document it'' and ``when you write a function, test it''.

\hypertarget{rememer-how-to-use-your-own-code}{%
\subsection{Rememer how to use your own code}\label{rememer-how-to-use-your-own-code}}

When you come to use a function written by somebody else, you likely have to refer to their documentation to teach or to remind yourself of things like what the expected inputs are and how exactly the method is implemented.

When writing your own functions you should create documentation that fills the same need. Even if the function is just for personal use, over time you'll forget exactly how it works.

\begin{quote}
\emph{When you write a function, document it.}
\end{quote}

But what should that documentation contain?

\begin{itemize}
\tightlist
\item
  Inputs
\item
  Outputs
\item
  Example use cases
\item
  Author (if not obvious or working in a team)
\end{itemize}

Your documentation should describe the inputs and outputs of your function, some simple example uses. If you are working in a large team, the documentation should also indicate who wrote the function and who's responsible for maintaining it over time.

\hypertarget{roxygen2-for-documentation}{%
\subsection{\texorpdfstring{\texttt{\{roxygen2\}} for documentation}{\{roxygen2\} for documentation}}\label{roxygen2-for-documentation}}

In the same way that we used the \texttt{\{here\}} package to simplify our file path problems, we'll use the \texttt{\{roxygen2\}} package to simplify our testing workflow.

The \{roxygen2\} package gives us an easily insert-able temple for documenting our functions. This means we don't have to waste our time and energy typing out and remembering boilerplate code. It also puts our documentation in a format that allows us to get hints and auto-completion for our own functions, just like the functions we use from packages that are written by other people.

To use Roxygen, you only need to install it once - it doesn't need to be loaded with a library call at the top of your script. After you've done this, and with your cursor inside a function definition, you can then insert skeleton code to document that function in one of two ways: you can either use the Rstudio menu or the keyboard short cut for your operating system.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{install.packages("roxygen2")}
\item
  With cursor inside function: Code \textgreater{} Insert Roxygen Skeleton
\item
  Keyboard shortcut: \emph{cmd + option + shift + r} or \emph{crtl + option + shift + r}
\item
  Fill out relevant fields
\end{enumerate}

\hypertarget{an-roxygen2-example}{%
\subsection{\texorpdfstring{An \texttt{\{roxygen2\}} example}{An \{roxygen2\} example}}\label{an-roxygen2-example}}

Below, we've got an example of an Roxygen skeleton to document a function that calculates the geometric mean of a vector. Here, the hash followed by an apostrophe is a special type of comment. It indicates that this is function documentation rather than just a regular comment.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} Title}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @param x }
\CommentTok{\#\textquotesingle{} @param remove\_NA }
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @return}
\CommentTok{\#\textquotesingle{} @export}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @examples}
\NormalTok{geometric\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{)\{}
  \CommentTok{\# Function body goes here}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We'll fill in all of the fields in this skeleton apart from export, which we'll remove. If we put this function in a R package, then the export field makes it available to users of that package. Since this is just a standalone function we won't need the export field, though keeping it wouldn't actually cause us any problems either.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} Calculate the geometric mean of a numeric vector}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @param x numeric vector}
\CommentTok{\#\textquotesingle{} @param remove\_NA logical scalar, indicating whether NA values should be stripped before computation proceeds. }
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @return the geometric mean of the values in \textasciigrave{}x\textasciigrave{}, a numeric scalar value. }
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @examples}
\CommentTok{\#\textquotesingle{} geometric\_mean(x = 1:10)}
\CommentTok{\#\textquotesingle{} geometric\_mean(x = c(1:10, NA), remove\_NA = TRUE)}
\CommentTok{\#\textquotesingle{} }
\NormalTok{geometric\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{)\{}
  \CommentTok{\# Function body goes here}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Once we have filled in the skeleton documentation it might look something like this. We have described what the function does, what the expected inputs are and what the user can expect as an output. We've also given an few simple examples of how the function can be used.

For more on Roxygen, see the \href{https://roxygen2.r-lib.org/}{package documentation} or the chapter of R packages on \href{https://r-pkgs.org/man.html}{function documentation}.

\hypertarget{checking-your-code}{%
\subsection{Checking Your Code}\label{checking-your-code}}

\begin{quote}
\emph{If you write a function, test it.}
\end{quote}

Testing code has two main purposes:

\begin{itemize}
\tightlist
\item
  To warn or prevent user misuse (e.g.~strange inputs),
\item
  To catch edge cases.
\end{itemize}

On top of explaining how our functions \emph{should} work, we really ought to check that they \emph{do} work. This is the job of unit testing.

Whenever you write a function you should test that it works as you intended it to. Additionally, you should test that your function is robust to being misused by the user. Depending on the context, this might be accidental or malicious misuse. Finally, you should check that the function behaves properly for strange, but still valid, inputs. These are known as edge cases.

Testing can be a bit of a brutal process, you've just created a beautiful function and now you're job is to do your best to break it!

\hypertarget{an-informal-testing-workflow}{%
\subsection{An Informal Testing Workflow}\label{an-informal-testing-workflow}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a function
\item
  Experiment with the function in the console, try to break it
\item
  Fix the break and repeat.
\end{enumerate}

\textbf{Problems:} Time consuming and not reproducible.

An informal approach to testing your code might be to first write a function and then play around with it in the console to check that it behaves well when you give it obvious inputs, edge cases and deliberately wrong inputs. Each time you manage to break the function, you edit it to fix the problem and then start the process all over again.

This \emph{is} testing the code, but only informally. There's no record of how you have tried to break your code already. The problem with this approach is that when you return to this code to add a new feature, you'll probably have forgotten at least one of the informal tests you ran the first time around. This goes against our efforts towards reproducibility and automation. It also makes it very easy to break code that used to work just fine.

\hypertarget{a-formal-testing-workflow}{%
\subsection{A Formal Testing Workflow}\label{a-formal-testing-workflow}}

We can formalise this testing workflow by writing our tests in their own R script and saving them for future reference. Remember from the first lecture that these should be saved in the \texttt{tests/} directory, the structure of which should mirror that of the \texttt{src/} directory for your project. All of the tests for one function should live in a single file, which is named after that function.

One way of writing these tests is to use lots of if statements. The \texttt{\{testthat\}} can do some of that syntactic heavy lifting for us. It has lots of helpful functions to test that the output of your function is what you expect.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{geometric\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x , }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{)\{}\FunctionTok{prod}\NormalTok{(x)}\SpecialCharTok{\^{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(x))\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testthat}\SpecialCharTok{::}\FunctionTok{expect\_equal}\NormalTok{(}
  \AttributeTok{object =} \FunctionTok{geometric\_mean}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{), }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{),}
  \AttributeTok{expected =} \ConstantTok{NA}\NormalTok{)}

\CommentTok{\# Error: geometric\_mean(x = c(1, NA), remove\_NA = FALSE) not equal to NA.}
\CommentTok{\# Types not compatible: double is not logical}
\end{Highlighting}
\end{Shaded}

In this example, we have an error because our function returns a logical \texttt{NA} rather than a double \texttt{NA}. Yes, R really does have different types of \texttt{NA} for different types of missing data, it usually just handles these nicely in the background for you.

This subtle difference is probably not something that you would have spotted on your own, until it caused you trouble much further down the line. This rigorous approach is one of the benefits of of using the \texttt{\{testthat\}} functions.

To fix this test we change out expected output to \texttt{NA\_real\_}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testthat}\SpecialCharTok{::}\FunctionTok{expect\_equal}\NormalTok{(}
  \AttributeTok{object =} \FunctionTok{geometric\_mean}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\ConstantTok{NA}\NormalTok{), }\AttributeTok{remove\_NA =} \ConstantTok{FALSE}\NormalTok{),}
  \AttributeTok{expected =} \ConstantTok{NA\_real\_}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We'll revisit the \texttt{\{testthat\}} package in the live session this week, when we will learn how to use it to test functions within our own packages.

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

\begin{itemize}
\tightlist
\item
  Functional and Object Oriented Programming
\item
  Structuring your scripts
\item
  Styling your code
\item
  Reduce, reuse, recycle
\item
  Documenting and testing
\end{itemize}

Let's wrap up by summarising what we have learned in this chapter.

We started out with a discussion on the differences between functional and object oriented programming. While R is capable of both, data science work tends to have more of a functional flavour to it.

We've then described how to structure your scripts and style your code to make it as human-friendly and easy to debug as possible.

Finally, we discussed how to write DRY code that is well documented and tested.

\hypertarget{workflows-checklist}{%
\chapter*{Checklist}\label{workflows-checklist}}
\addcontentsline{toc}{chapter}{Checklist}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{videos-chapters}{%
\section{Videos / Chapters}\label{videos-chapters}}

\begin{itemize}
\tightlist
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=eb93df23-751e-4f79-8397-af72013634d0}{Organising your work} (30 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/01-01-organising-your-work/01-01-organising-your-work.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=0f676fbc-3de6-490a-ac38-af7200ee1396}{Naming Files} (20 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/01-02-naming-files/01-02-naming-files.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=572c25c0-4cac-4260-97fe-af7200ee1358}{Organising your code} (27 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/01-03-organising-your-code/01-03-organising-your-code.pdf}{{[}slides{]}}
\end{itemize}

\hypertarget{reading}{%
\section{Reading}\label{reading}}

Use the \protect\hyperlink{workflows-reading}{workflows section} of the reading list to support and guide your exploration of this week's materials. Note that these texts are divided into core reading, reference materials and materials of interest.

\hypertarget{tasks}{%
\section{Tasks}\label{tasks}}

\emph{Core:}

\begin{itemize}
\item
  Find 3 data science projects on Github and explore how they organise their work. Write a post on the EdStem forum that links to all three, and in a couple of paragraphs describe the content and structure of one project.
\item
  Create your own project directory (or directories) for this course and its assignments.
\item
  Write two of your own R functions. The first should calculate the geometric mean of a numeric vector. The second should calculate the rolling arithmetic mean of a numeric vector.
\end{itemize}

\emph{Bonus:}

\begin{itemize}
\item
  Re-factor an old project to match the project organisation and coding guides for this course. This might be a small research project, class notes or a collection of homework assignments. Use an R-based project if possible. If you only have python projects, then either translate these to R or apply the \href{https://peps.python.org/pep-0008/}{PEP8} style guide. Take care to select a suitably sized project so that this is a meaningful exercise but does not take more than a few hours.
\item
  If you are able to do so, host your re-factored project publicly and share it with the rest of the class on the EdStem Discussion forum.
\end{itemize}

\hypertarget{live-session}{%
\section{Live Session}\label{live-session}}

In the live session we will begin with a discussion of this week's tasks. We will then create a minimal R package to organise and test the functions you have written.

Please come to the live session prepared to discuss the following points:

\begin{itemize}
\item
  Did you make the assignment projects as subdirectories or as their stand alone projects? Why?
\item
  What were some terms that you had not met before during the readings? How did you find their meanings?
\item
  What did you have to consider when writing your rolling mean function?
\end{itemize}

\hypertarget{part-acquiring-and-sharing-data}{%
\part{Acquiring and Sharing Data}\label{part-acquiring-and-sharing-data}}

\hypertarget{data-introduction}{%
\chapter*{Introduction}\label{data-introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\emph{Data can be difficult to acquire and gnarly when you get it.}

The raw material that you work with as a data scientist is, unsurprisingly, data. In this part of the course we will focus on the different ways in which data can be stored, distributed and obtained.

Being able to obtain and read a dataset is often a surprisingly large hurdle in getting a new data science project off the ground. The skill of being able to source and read data from many locations is usually sanitised during a statistics programme: you're given a ready-to-go, cleaned CSV file and all focus is placed on modelling. This week aims to remedy that by equipping you with the skills to acquire and manage your own data.

We will begin this week by explore different file types. This dictates what type of information you can store, who can access that information and how they read that it into R. We will then turn our attention to the case when data are not given to you directly. We will learn how to obtain data from a raw webpage and how to request data that via a service known as an API.

\hypertarget{data-tabular}{%
\chapter{Tabular Data}\label{data-tabular}}

Effective Data Science is still a work-in-progress. This chapter is undergoing heavy restructuring and may be confusing or incomplete.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{loading-tabular-data}{%
\section{Loading Tabular Data}\label{loading-tabular-data}}

Recall that simpler, open source formats improve accessibility and reproducibility. We will begin by reading in three open data formats for tabular data.

\begin{itemize}
\item
  \texttt{random-data.csv}
\item
  \texttt{random-data.tsv}
\item
  \texttt{random-data.txt}
\end{itemize}

Each of these data sets contains 26 observations of 4 variables:

\begin{itemize}
\tightlist
\item
  \texttt{id}, a Roman letter identifier;
\item
  \texttt{gaussian}, standard normal random variates;
\item
  \texttt{gamma}, gamma(1,1) random variates;
\item
  \texttt{uniform}, uniform(0,1) random variates.
\end{itemize}

\hypertarget{base-r}{%
\subsection{Base R}\label{base-r}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random\_df }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file =} \StringTok{\textquotesingle{}random{-}data.csv\textquotesingle{}}\NormalTok{)}
\FunctionTok{print}\NormalTok{(random\_df)}
\CommentTok{\#\textgreater{}    id    gaussian      gamma    uniform}
\CommentTok{\#\textgreater{} 1   a {-}1.20706575 0.98899970 0.22484576}
\CommentTok{\#\textgreater{} 2   b  0.27742924 0.03813386 0.08498474}
\CommentTok{\#\textgreater{} 3   c  1.08444118 1.09462335 0.63729826}
\CommentTok{\#\textgreater{} 4   d {-}2.34569770 1.49301101 0.43101637}
\CommentTok{\#\textgreater{} 5   e  0.42912469 5.40361248 0.07271609}
\CommentTok{\#\textgreater{} 6   f  0.50605589 1.72386539 0.80240202}
\CommentTok{\#\textgreater{} 7   g {-}0.57473996 1.95357133 0.32527830}
\CommentTok{\#\textgreater{} 8   h {-}0.54663186 0.07807803 0.75728904}
\CommentTok{\#\textgreater{} 9   i {-}0.56445200 0.21198194 0.58427152}
\CommentTok{\#\textgreater{} 10  j {-}0.89003783 0.20803673 0.70883941}
\CommentTok{\#\textgreater{} 11  k {-}0.47719270 2.08607862 0.42697577}
\CommentTok{\#\textgreater{} 12  l {-}0.99838644 0.49463708 0.34357270}
\CommentTok{\#\textgreater{} 13  m {-}0.77625389 0.77171305 0.75911999}
\CommentTok{\#\textgreater{} 14  n  0.06445882 0.37216648 0.42403021}
\CommentTok{\#\textgreater{} 15  o  0.95949406 1.88207991 0.56088725}
\CommentTok{\#\textgreater{} 16  p {-}0.11028549 0.76622568 0.11613577}
\CommentTok{\#\textgreater{} 17  q {-}0.51100951 0.50488585 0.30302180}
\CommentTok{\#\textgreater{} 18  r {-}0.91119542 0.22979791 0.47880269}
\CommentTok{\#\textgreater{} 19  s {-}0.83717168 0.75637275 0.34483055}
\CommentTok{\#\textgreater{} 20  t  2.41583518 0.62435969 0.60071414}
\CommentTok{\#\textgreater{} 21  u  0.13408822 0.64638373 0.07608332}
\CommentTok{\#\textgreater{} 22  v {-}0.49068590 0.11247545 0.95599261}
\CommentTok{\#\textgreater{} 23  w {-}0.44054787 0.11924307 0.02220682}
\CommentTok{\#\textgreater{} 24  x  0.45958944 4.91805535 0.84171063}
\CommentTok{\#\textgreater{} 25  y {-}0.69372025 0.60282666 0.63244245}
\CommentTok{\#\textgreater{} 26  z {-}1.44820491 0.64446571 0.31009417}
\end{Highlighting}
\end{Shaded}

Output is a \texttt{data.frame} object. (List of vectors with some nice methods)

\hypertarget{readr}{%
\subsection{\texorpdfstring{\texttt{\{readr\}}}{\{readr\}}}\label{readr}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{random\_tbl }\OtherTok{\textless{}{-}}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{\textquotesingle{}random{-}data.csv\textquotesingle{}}\NormalTok{)}
\CommentTok{\#\textgreater{} Rows: 26 Columns: 4}
\CommentTok{\#\textgreater{} {-}{-} Column specification {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{} Delimiter: ","}
\CommentTok{\#\textgreater{} chr (1): id}
\CommentTok{\#\textgreater{} dbl (3): gaussian, gamma, uniform}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} i Use \textasciigrave{}spec()\textasciigrave{} to retrieve the full column specification for this data.}
\CommentTok{\#\textgreater{} i Specify the column types or set \textasciigrave{}show\_col\_types = FALSE\textasciigrave{} to quiet this message.}
\FunctionTok{print}\NormalTok{(random\_tbl)}
\CommentTok{\#\textgreater{} \# A tibble: 26 x 4}
\CommentTok{\#\textgreater{}   id    gaussian  gamma uniform}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 a       {-}1.21  0.989   0.225 }
\CommentTok{\#\textgreater{} 2 b        0.277 0.0381  0.0850}
\CommentTok{\#\textgreater{} 3 c        1.08  1.09    0.637 }
\CommentTok{\#\textgreater{} 4 d       {-}2.35  1.49    0.431 }
\CommentTok{\#\textgreater{} 5 e        0.429 5.40    0.0727}
\CommentTok{\#\textgreater{} 6 f        0.506 1.72    0.802 }
\CommentTok{\#\textgreater{} \# ... with 20 more rows}
\end{Highlighting}
\end{Shaded}

Output is a \texttt{tibble} object. (List of vectors with some nicer methods)

\hypertarget{benefits-of-readrread_csv}{%
\subsubsection{\texorpdfstring{Benefits of \texttt{readr::read\_csv()}}{Benefits of readr::read\_csv()}}\label{benefits-of-readrread_csv}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Increased speed (approx. 10x) and progress bar.
\item
  Strings are not coerced to factors. No more \texttt{stringsAsFactors\ =\ FALSE}
\item
  No row names and nice column names.
\item
  Reproducibility bonus: does not depend on operating system.
\end{enumerate}

\hypertarget{wtf-tibbles}{%
\subsection{WTF: Tibbles}\label{wtf-tibbles}}

\hypertarget{printing}{%
\subsubsection{Printing}\label{printing}}

\begin{itemize}
\item
  Default to first 10 rows and as many columns as will comfortably fit on your screen.
\item
  Can adjust this behaviour in the print call:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# print first three rows and all columns}
\FunctionTok{print}\NormalTok{(random\_tbl, }\AttributeTok{n =} \DecValTok{3}\NormalTok{, }\AttributeTok{width =} \ConstantTok{Inf}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 26 x 4}
\CommentTok{\#\textgreater{}   id    gaussian  gamma uniform}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}    \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 a       {-}1.21  0.989   0.225 }
\CommentTok{\#\textgreater{} 2 b        0.277 0.0381  0.0850}
\CommentTok{\#\textgreater{} 3 c        1.08  1.09    0.637 }
\CommentTok{\#\textgreater{} \# ... with 23 more rows}
\end{Highlighting}
\end{Shaded}

\textbf{Bonus:} Colour formatting in IDE and each column tells you it's type.

\hypertarget{subsetting}{%
\subsubsection{Subsetting}\label{subsetting}}

Subsetting tibbles will always return another tibble.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Row Subsetting}
\NormalTok{random\_tbl[}\DecValTok{1}\NormalTok{, ] }\CommentTok{\# returns tibble}
\NormalTok{random\_df[}\DecValTok{1}\NormalTok{, ]  }\CommentTok{\# returns data.frame}

\CommentTok{\# Column Subsetting}
\NormalTok{random\_tbl[ , }\DecValTok{1}\NormalTok{]      }\CommentTok{\# returns tibble}
\NormalTok{random\_df[ , }\DecValTok{1}\NormalTok{]       }\CommentTok{\# returns vector}

\CommentTok{\# Combined Subsetting}
\NormalTok{random\_tbl[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]      }\CommentTok{\# returns 1x1 tibble}
\NormalTok{random\_df[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{]       }\CommentTok{\# returns single value}
\end{Highlighting}
\end{Shaded}

This helps to avoids edge cases associated with working on data frames.

\hypertarget{other-readr-functions}{%
\subsection{\texorpdfstring{Other \texttt{\{readr\}} functions}{Other \{readr\} functions}}\label{other-readr-functions}}

See \texttt{\{readr\}} \href{https://readr.tidyverse.org/}{documentation}, there are lots of useful additional arguments that can help you when reading messy data.

Functions for reading and writing other types of tabular data work analogously.

\hypertarget{reading-tabular-data}{%
\subsubsection{Reading Tabular Data}\label{reading-tabular-data}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{read\_tsv}\NormalTok{(}\StringTok{"random{-}data.tsv"}\NormalTok{)}
\FunctionTok{read\_delim}\NormalTok{(}\StringTok{"random{-}data.txt"}\NormalTok{, }\AttributeTok{delim =} \StringTok{" "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{writing-tabular-data}{%
\subsubsection{Writing Tabular Data}\label{writing-tabular-data}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(random\_tbl, }\StringTok{"random{-}data{-}2.csv"}\NormalTok{)}
\FunctionTok{write\_tsv}\NormalTok{(random\_tbl, }\StringTok{"random{-}data{-}2.tsv"}\NormalTok{)}
\FunctionTok{write\_delim}\NormalTok{(random\_tbl, }\StringTok{"random{-}data{-}2.tsv"}\NormalTok{, }\AttributeTok{delim =} \StringTok{" "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{need-for-speed}{%
\subsection{Need for Speed}\label{need-for-speed}}

Some times you have to load \emph{lots of large data sets}, in which case a 10x speed-up might not be sufficient.

If each data set still fits inside RAM, then check out \texttt{data.table::fread()} which is optimised for speed. (Alternatives exist for optimal memory usage and data too large for working memory, but not covered here.)

\textbf{Note:} While it can be much faster, the resulting data.table object lacks the consistancy properties of a tibble so be sure to check for edge cases, where the returned value is not what you might expect.

\hypertarget{tidy-data}{%
\section{Tidy Data}\label{tidy-data}}

\hypertarget{wide-vs.-tall-data}{%
\subsection{Wide vs.~Tall Data}\label{wide-vs.-tall-data}}

\hypertarget{wide-data}{%
\subsubsection{Wide Data}\label{wide-data}}

\begin{itemize}
\item
  First column has unique entries
\item
  Easier for humans to read and compute on
\item
  Harder for machines to compute on
\end{itemize}

\hypertarget{tall-data}{%
\subsubsection{Tall Data}\label{tall-data}}

\begin{itemize}
\item
  First column has repeating entries
\item
  Harder for humans to read and compute on
\item
  Easier for machines to compute on
\end{itemize}

\hypertarget{examples}{%
\subsubsection{Examples}\label{examples}}

\textbf{Example 1 (Wide)}

\begin{longtable}[]{@{}llll@{}}
\toprule()
\textbf{Person } & \textbf{Age } & \textbf{Weight } & \textbf{Height } \\
\midrule()
\endhead
Bob & 32 & 168 & 180 \\
Alice & 24 & 150 & 175 \\
Steve & 64 & 144 & 165 \\
\bottomrule()
\end{longtable}

\textbf{Example 1 (Tall)}

\begin{longtable}[]{@{}ccc@{}}
\toprule()
\textbf{Person } & \textbf{Variable } & \textbf{Value } \\
\midrule()
\endhead
Bob & Age & 32 \\
Bob & Weight & 168 \\
Bob & Height & 180 \\
Alice & Age & 24 \\
Alice & Weight & 150 \\
Alice & Height & 175 \\
Steve & Age & 64 \\
Steve & Weight & 144 \\
Steve & Height & 165 \\
\bottomrule()
\end{longtable}

{[}Source: Wikipedia - Wide and narrow data{]}

\textbf{Example 2 (Wide)}

\begin{longtable}[]{@{}llll@{}}
\toprule()
Team & Points & Assists & Rebounds \\
\midrule()
\endhead
A & 88 & 12 & 22 \\
B & 91 & 17 & 28 \\
C & 99 & 24 & 30 \\
D & 94 & 28 & 31 \\
\bottomrule()
\end{longtable}

\textbf{Example 2 (Tall)}

\begin{longtable}[]{@{}lll@{}}
\toprule()
Team & Variable & Value \\
\midrule()
\endhead
A & Points & 88 \\
A & Assists & 12 \\
A & Rebounds & 22 \\
B & Points & 91 \\
B & Assists & 17 \\
B & Rebounds & 28 \\
C & Points & 99 \\
C & Assists & 24 \\
C & Rebounds & 30 \\
D & Points & 94 \\
D & Assists & 28 \\
D & Rebounds & 31 \\
\bottomrule()
\end{longtable}

{[}Source: Statology - Long vs wide data{]}

\hypertarget{pivoting-wider-and-longer}{%
\subsubsection{Pivoting Wider and Longer}\label{pivoting-wider-and-longer}}

\begin{itemize}
\item
  Error control at input and analysis is format-dependent.
\item
  Switching between long and wide formats useful to control errors.
\item
  Easy with the \texttt{\{tidyr\}} package functions
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_longer}\NormalTok{()}
\NormalTok{tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tidy-what}{%
\subsection{Tidy What?}\label{tidy-what}}

\begin{figure}
\centering
\includegraphics{images/201-tabular-data/tidy-1.png}
\caption{{[}Image: R4DS - Chapter 12{]}}
\end{figure}

\emph{Tidy Data} is an opinionated way to store tabular data.

Image Source: Chapter 12 of R for Data Science.

\begin{itemize}
\tightlist
\item
  Each column corresponds to a exactly one measured variable
\item
  Each row corresponds to exactly one observational unit
\item
  Each cell contains exactly one value.
\end{itemize}

\textbf{Benefits of tidy data}

\begin{itemize}
\item
  \emph{Consistent data format:} Reduces cognitive load and allows specialised tools (functions) to efficiently work with tabular data.
\item
  \emph{Vectorisation}: Keeping variables as columns allows for very efficient data manipulation. (this goes back to data frames and tibbles being lists of vectors)
\end{itemize}

\hypertarget{example---tidy-longer}{%
\subsection{Example - Tidy Longer}\label{example---tidy-longer}}

Consider trying to plot these data as time series. The \texttt{year} variable is trapped in the column names!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{countries}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 3}
\CommentTok{\#\textgreater{}   country     \textasciigrave{}1999\textasciigrave{} \textasciigrave{}2000\textasciigrave{}}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}        \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Afghanistan    745   2666}
\CommentTok{\#\textgreater{} 2 Brazil       37737  80488}
\CommentTok{\#\textgreater{} 3 China       212258 213766}
\end{Highlighting}
\end{Shaded}

To tidy this data, we need to \texttt{pivot\_longer()}. We will turn the column names into a new \texttt{year} variable and retaining cell contents as a new variable called \texttt{cases}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(magrittr)}
\NormalTok{countries }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{1999}\StringTok{\textasciigrave{}}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{2000}\StringTok{\textasciigrave{}}\NormalTok{), }\AttributeTok{names\_to =} \StringTok{"year"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"cases"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 3}
\CommentTok{\#\textgreater{}   country     year   cases}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}       \textless{}chr\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Afghanistan 1999     745}
\CommentTok{\#\textgreater{} 2 Afghanistan 2000    2666}
\CommentTok{\#\textgreater{} 3 Brazil      1999   37737}
\CommentTok{\#\textgreater{} 4 Brazil      2000   80488}
\CommentTok{\#\textgreater{} 5 China       1999  212258}
\CommentTok{\#\textgreater{} 6 China       2000  213766}
\end{Highlighting}
\end{Shaded}

Much better!

\hypertarget{example---tidy-wider}{%
\subsection{Example - Tidy Wider}\label{example---tidy-wider}}

There are other times where we might have to widen our data to tidy it.

This example is not tidy. Why not?

\begin{longtable}[]{@{}lll@{}}
\toprule()
Team & Variable & Value \\
\midrule()
\endhead
A & Points & 88 \\
A & Assists & 12 \\
A & Rebounds & 22 \\
B & Points & 91 \\
B & Assists & 17 \\
B & Rebounds & 28 \\
C & Points & 99 \\
C & Assists & 24 \\
C & Rebounds & 30 \\
D & Points & 94 \\
D & Assists & 28 \\
D & Rebounds & 31 \\
\bottomrule()
\end{longtable}

The observational unit here is a team. However, each variable should be a stored in a separate column, with cells containing their values.

To tidy this data we first generate it as a tibble. We use the \texttt{tribble()} function, which allows us to create a tibble row-wise rather than column-wise.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tournament }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{tribble}\NormalTok{(}
\SpecialCharTok{\textasciitilde{}}\NormalTok{Team  , }\SpecialCharTok{\textasciitilde{}}\NormalTok{Variable , }\SpecialCharTok{\textasciitilde{}}\NormalTok{Value,}
\StringTok{"A"}\NormalTok{    , }\StringTok{"Points"}\NormalTok{  , }\DecValTok{88}\NormalTok{    ,}
\StringTok{"A"}\NormalTok{    , }\StringTok{"Assists"}\NormalTok{ , }\DecValTok{12}\NormalTok{    ,}
\StringTok{"A"}\NormalTok{    , }\StringTok{"Rebounds"}\NormalTok{, }\DecValTok{22}\NormalTok{    ,}
\StringTok{"B"}\NormalTok{    , }\StringTok{"Points"}\NormalTok{  , }\DecValTok{91}\NormalTok{    ,}
\StringTok{"B"}\NormalTok{    , }\StringTok{"Assists"}\NormalTok{ , }\DecValTok{17}\NormalTok{    ,}
\StringTok{"B"}\NormalTok{    , }\StringTok{"Rebounds"}\NormalTok{, }\DecValTok{28}\NormalTok{    ,}
\StringTok{"C"}\NormalTok{    , }\StringTok{"Points"}\NormalTok{  , }\DecValTok{99}\NormalTok{    ,}
\StringTok{"C"}\NormalTok{    , }\StringTok{"Assists"}\NormalTok{ , }\DecValTok{24}\NormalTok{    ,}
\StringTok{"C"}\NormalTok{    , }\StringTok{"Rebounds"}\NormalTok{, }\DecValTok{30}\NormalTok{    ,}
\StringTok{"D"}\NormalTok{    , }\StringTok{"Points"}\NormalTok{  , }\DecValTok{94}\NormalTok{    ,}
\StringTok{"D"}\NormalTok{    , }\StringTok{"Assists"}\NormalTok{ , }\DecValTok{28}\NormalTok{    ,}
\StringTok{"D"}\NormalTok{    , }\StringTok{"Rebounds"}\NormalTok{, }\DecValTok{31}\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

We can then tidy it by creating new columns for each value of the current \texttt{Variable} column and taking the values for these from the current \texttt{Value} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tournament }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(}
    \AttributeTok{id\_cols =} \StringTok{"Team"}\NormalTok{, }
    \AttributeTok{names\_from =} \StringTok{"Variable"}\NormalTok{,}
    \AttributeTok{values\_from =} \StringTok{"Value"}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 4}
\CommentTok{\#\textgreater{}   Team  Points Assists Rebounds}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 A         88      12       22}
\CommentTok{\#\textgreater{} 2 B         91      17       28}
\CommentTok{\#\textgreater{} 3 C         99      24       30}
\CommentTok{\#\textgreater{} 4 D         94      28       31}
\end{Highlighting}
\end{Shaded}

\hypertarget{other-helpful-functions}{%
\subsection{Other helpful functions}\label{other-helpful-functions}}

The \texttt{pivot\_*()} family of functions resolve issues with rows (too many observations per row or rows per observation).

There are similar helper functions to solve column issues:

\begin{itemize}
\item
  Multiple variables per column: \texttt{tidyr::separate()},
\item
  Multiple columns per variable: \texttt{tidyr::unite()}.
\end{itemize}

\hypertarget{missing-data}{%
\subsection{Missing Data}\label{missing-data}}

In tidy data, every cell contains a value. Including cells with missing values.

\begin{itemize}
\item
  Missing values are coded as \texttt{NA} (generic) or a type-specific \texttt{NA}, such as \texttt{NA\_character\_}.
\item
  The \texttt{\{readr\}} family of \texttt{read\_*()} function have good defaults and helpful \texttt{na} argument.
\item
  Explicitly code \texttt{NA} values when collecting data, avoid ambiguity: '' ``, -999 or worst of all 0.
\item
  More on missing values in EDA videos\ldots{}
\end{itemize}

\hypertarget{wrapping-up-1}{%
\section{Wrapping Up}\label{wrapping-up-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Reading in tabular data by a range of methods
\item
  Introduced the \texttt{tibble} and tidy data (+ tidy not always best)
\item
  Tools for tidying messy tabular data
\end{enumerate}

\hypertarget{data-webscraping}{%
\chapter{Web Scraping}\label{data-webscraping}}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{scraping-webpage-data-using-rvest}{%
\section{Scraping webpage data using \{rvest\}}\label{scraping-webpage-data-using-rvest}}

You can't always rely on tidy, tabular data to land on your desk. Sometimes you are going to have to go out and gather data for yourself.

I'm not suggesting you will need to do this manually, but you will likely need to get data from the internet that's been made publicly or privately available to you.

This might be information from a webpage that you gather yourself, or data shared with you by a collaborator using an API.

In this chapter we will cover the basics of scraping webpages, following the \href{https://cran.r-project.org/web/packages/rvest/vignettes/rvest.html\#fnref3}{vignette} for the \{rvest\} package.

\hypertarget{what-is-a-webpage}{%
\section{What is a webpage?}\label{what-is-a-webpage}}

Before we can even hope to get data from a webpage, we first need to understand \emph{what} a webpage is.

Webpages are written in a similar way to LaTeX: the content and styling of webpages are handled separately and are coded using plain text files.

In fact, websites go one step further than LaTeX. The content and styling of websites are written in different files and in different languages. HTML (HyperText Markup Language) is used to write the content and then CSS (Cascading Style Sheets) are used to control the appearance of that content when it's displayed to the user.

\hypertarget{html}{%
\section{HTML}\label{html}}

A basic HTML page with no styling applied might look something like this:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{\textless{}html\textgreater{}}
\KeywordTok{\textless{}head\textgreater{}}
  \KeywordTok{\textless{}title\textgreater{}}\NormalTok{Page title}\KeywordTok{\textless{}/title\textgreater{}}
\KeywordTok{\textless{}/head\textgreater{}}
\KeywordTok{\textless{}body\textgreater{}}
  \KeywordTok{\textless{}h1} \ErrorTok{id}\OtherTok{=}\StringTok{\textquotesingle{}first\textquotesingle{}}\KeywordTok{\textgreater{}}\NormalTok{A level 1 heading}\KeywordTok{\textless{}/h1\textgreater{}}
  \KeywordTok{\textless{}p\textgreater{}}\NormalTok{Hello World!}\KeywordTok{\textless{}/p\textgreater{}}
  \KeywordTok{\textless{}p\textgreater{}}\NormalTok{Here is some plain text }\DecValTok{\&amp;} \KeywordTok{\textless{}b\textgreater{}}\NormalTok{some bold text.}\KeywordTok{\textless{}/b\textgreater{}\textless{}/p\textgreater{}}
  \KeywordTok{\textless{}img} \ErrorTok{src}\OtherTok{=}\StringTok{\textquotesingle{}myimg.png\textquotesingle{}} \ErrorTok{width}\OtherTok{=}\StringTok{\textquotesingle{}100\textquotesingle{}} \ErrorTok{height}\OtherTok{=}\StringTok{\textquotesingle{}100\textquotesingle{}}\KeywordTok{\textgreater{}}
\KeywordTok{\textless{}/body\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{html-elements}{%
\subsection{HTML elements}\label{html-elements}}

Just like XML data files, HTML has a hierarchical structure. This structure is crafted using HTML elements. Each HTML element is made up of of a start tag, optional attributes, an end tag.

We can see each of these in the first level header, where \texttt{\textless{}h1\textgreater{}} is the opening tag, \texttt{id=\textquotesingle{}first\textquotesingle{}} is an additional attribute and \texttt{\textless{}/h1\textgreater{}} is the closing tag. Everything between the opening and closing tag are the contents of that element. There are also some special elements that consist of only a single tag and its optional attributes. An example of this is the \texttt{\textless{}img\textgreater{}} tag.

Since \texttt{\textless{}} and \texttt{\textgreater{}} are used for start and end tags, you can't write them directly in an HTML document. Instead, you have to use escape characters. This sounds fancy, but it's just an alternative way to write characters that serve some special function within a language.

You can write greater than \texttt{\&gt;} and less than as \texttt{\&lt;}. You might notice that those escapes use an ampersand (\&). This means that if you want a literal ampersand on your webpage, you have to escape too using \texttt{\&amp;}.

There are a wide range of possible HTML tags and escapes. We'll cover the most common tags in this lecture and you don't need to worry about escapes too much because \texttt{\{rvest\}} will automatically handle them for you.

\hypertarget{important-html-elements}{%
\subsection{Important HTML Elements}\label{important-html-elements}}

In all, there are in excess of 100 HTML elements. The most important ones for you to know about are:

\begin{itemize}
\item
  The \texttt{\textless{}html\textgreater{}} element, that must enclose every HTML page. The \texttt{\textless{}html\textgreater{}} element must have two child elements within it. The \texttt{\textless{}head\textgreater{}} element contains metadata about the document, like the page title that is shown in the browser tab and the CSS style sheet that should be applied. The \texttt{\textless{}body\textgreater{}} element then contains all of the content that you see in the browser.
\item
  Block elements are used to give structure to the page. These are elements like headings, sub-headings and so on from \texttt{\textless{}h1\textgreater{}} all the way down to \texttt{\textless{}h6\textgreater{}}. This category also contains paragraph elements \texttt{\textless{}p\textgreater{}}, ordered lists \texttt{\textless{}ol\textgreater{}} unordered lists \texttt{\textless{}ul\textgreater{}}.
\item
  Finally, inline tags like \texttt{\textless{}b\textgreater{}} for bold, \texttt{\textless{}i\textgreater{}} for italics, and \texttt{\textless{}a\textgreater{}} for hyperlinks are used to format text inside block elements.
\end{itemize}

When you come across a tag that you've never seen before, you can find out what it does with just a little bit of googling. A good resource here is the \href{https://developer.mozilla.org/en-US/docs/Web/HTML}{MDN Web Docs} which are produced by Mozilla, the company that makes the Firefox web browser. The \href{https://www.w3schools.com/html/default.asp}{W3schools website} is another great resource for web development and coding resources more generally.

\hypertarget{html-attributes}{%
\section{HTML Attributes}\label{html-attributes}}

We've seen one example of a header with an additional attribute. More generally, all tags can have named attributes. These attributes are contained within the opening tag and look something like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{\textless{}tag} \ErrorTok{attribute1}\OtherTok{=}\StringTok{\textquotesingle{}value1\textquotesingle{}} \ErrorTok{attribute2}\OtherTok{=}\StringTok{\textquotesingle{}value2\textquotesingle{}}\KeywordTok{\textgreater{}}\NormalTok{element contents}\KeywordTok{\textless{}/tag\textgreater{}}
\end{Highlighting}
\end{Shaded}

Two of the most important attributes are \texttt{id} and \texttt{class}. These attributes are used in conjunction with the CSS file to control the visual appearance of the page. These are often very useful to identify the elements that you are interested in when scraping data off a page.

\hypertarget{css-selectors}{%
\section{CSS Selectors}\label{css-selectors}}

The Cascading Style Sheet is used to describe how your HTML content will be displayed. To do this, CSS has it's own system for selecting elements of a webpage, called CSS selectors.

CSS selectors define patterns for locating the HTML elements that a particular style should be applied to. A happy side-effect of this is that they can sometimes be very useful for scraping, because they provide a concise way of describing which elements you want to extract.

CSS Selectors can work on the level of an element type, a class, or a tag and these can be used in a nested (or \emph{cascading}) way.

\begin{itemize}
\item
  The \texttt{p} selector will select all paragraph \texttt{\textless{}p\textgreater{}} elements.
\item
  The \texttt{.title} selector will select all elements with class \texttt{title}.
\item
  The \texttt{p.special} selector will select all\texttt{\textless{}p\textgreater{}} elements with class \texttt{special}.
\item
  The \texttt{\#title} selector will select the element with the id attribute \texttt{title}.
\end{itemize}

When you want to select a single element \texttt{id} attributes are particularly useful because that \emph{must} be unique within a html document. Unfortunately, this is only helpful if the developer added an \texttt{id} attribute to the element(s) you want to scrape!

If you want to learn more CSS selectors I recommend starting with the fun \href{https://flukeout.github.io/}{CSS dinner tutorial} to build a base of knowledge and then using the \href{https://www.w3schools.com/css/default.asp}{W3schools resources} as a reference to explore more webpages in the wild.

\hypertarget{which-attributes-and-selectors-do-you-need}{%
\section{Which Attributes and Selectors Do You Need?}\label{which-attributes-and-selectors-do-you-need}}

To scrape data from a webpage, you first have to identify the tag and attribute combinations that you are interested in gathering.

To find your elements of interest, you have three options. These go from hardest to easiest but also from most to least robust.

\begin{itemize}
\tightlist
\item
  right click + ``inspect page source'' (F12)
\item
  right click + ``inspect''
\item
  Rvest \href{https://rvest.tidyverse.org/articles/selectorgadget.html}{Selector Gadget} (very useful but fallible)
\end{itemize}

Inspecting the source of some familiar websites can be a useful way to get your head around these concepts. Beware though that sophisticated webpages can be quite intimidating. A good place to start is with simpler, static websites such as personal websites, rather than the dynamic webpages of online retailers or social media platforms.

\hypertarget{reading-html-with-rvest}{%
\section{\texorpdfstring{Reading HTML with \texttt{\{rvest\}}}{Reading HTML with \{rvest\}}}\label{reading-html-with-rvest}}

With \texttt{\{rvest\}}, reading a html page can be as simple as loading in tabular data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\OtherTok{\textless{}{-}}\NormalTok{ rvest}\SpecialCharTok{::}\FunctionTok{read\_html}\NormalTok{(}\StringTok{"https://www.zakvarty.com/professional/teaching.html"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{class} of the resulting object is an \texttt{xml\_document}. This type of object is from the low-level package \texttt{\{xml2\}}, which allows you to read xml files into R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(html)}
\CommentTok{\#\textgreater{} [1] "xml\_document" "xml\_node"}
\end{Highlighting}
\end{Shaded}

We can see that this object is split into several components: first is some metadata on the type of document we have scraped, followed by the head and then the body of that html document.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html}
\CommentTok{\#\textgreater{} \{html\_document\}}
\CommentTok{\#\textgreater{} \textless{}html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"\textgreater{}}
\CommentTok{\#\textgreater{} [1] \textless{}head\textgreater{}\textbackslash{}n\textless{}meta http{-}equiv="Content{-}Type" content="text/html; charset=UT ...}
\CommentTok{\#\textgreater{} [2] \textless{}body class="nav{-}fixed"\textgreater{}\textbackslash{}n\textbackslash{}n\textless{}div id="quarto{-}search{-}results"\textgreater{}\textless{}/div\textgreater{}\textbackslash{}n   ...}
\end{Highlighting}
\end{Shaded}

We have several possible approaches to extracting information from this document.

\hypertarget{extracting-html-elements}{%
\section{Extracting HTML elements}\label{extracting-html-elements}}

In \texttt{\{rvest\}} you can extract a single element with \texttt{html\_element()}, or all matching elements with \texttt{html\_elements()}. Both functions take a document object and one or more CSS selectors as inputs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}

\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"h1"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (1)\}}
\CommentTok{\#\textgreater{} [1] \textless{}h1\textgreater{}Teaching\textless{}/h1\textgreater{}}
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"h2"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (2)\}}
\CommentTok{\#\textgreater{} [1] \textless{}h2 id="toc{-}title"\textgreater{}On this page\textless{}/h2\textgreater{}}
\CommentTok{\#\textgreater{} [2] \textless{}h2 class="anchored" data{-}anchor{-}id="course{-}history"\textgreater{}Course History\textless{}/h2\textgreater{}}
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (2)\}}
\CommentTok{\#\textgreater{} [1] \textless{}p\textgreater{}I am fortunate to have had the opportunity to teach in a variety of ...}
\CommentTok{\#\textgreater{} [2] \textless{}p\textgreater{}I am an associate fellow of the Higher Education Academy, which you ...}
\end{Highlighting}
\end{Shaded}

You can also combine and nest these selectors. For example you might want to extract all links that are within paragraphs \emph{and} all second level headers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p a,h2"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{xml\_nodeset (3)\}}
\CommentTok{\#\textgreater{} [1] \textless{}h2 id="toc{-}title"\textgreater{}On this page\textless{}/h2\textgreater{}}
\CommentTok{\#\textgreater{} [2] \textless{}a href="https://www.advance{-}he.ac.uk/fellowship/associate{-}fellowship" ...}
\CommentTok{\#\textgreater{} [3] \textless{}h2 class="anchored" data{-}anchor{-}id="course{-}history"\textgreater{}Course History\textless{}/h2\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{extracting-data-from-html-elements}{%
\section{Extracting Data From HTML Elements}\label{extracting-data-from-html-elements}}

Now that we've got the elements we care about extracted from the complete document. But how do we get the data we need out of those elements?

You'll usually get the data from either the contents of the HTML element or else from one of it's attributes. If you're really lucky, the data you need will already be formatted for you as a HTML table or list.

\hypertarget{extracting-text}{%
\subsection{Extracting text}\label{extracting-text}}

The functions \texttt{rvest::html\_text()} and \texttt{rvest::html\_text2()} can be used to extract the plain text contents of an HTML element.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"\#teaching li"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "one{-}to{-}one tuition for high school students;"                                   }
\CommentTok{\#\textgreater{} [2] "running workshops and computer labs for undergraduate and postgraduate modules;"}
\CommentTok{\#\textgreater{} [3] "delivering short courses on scientific communication and LaTeX;"                }
\CommentTok{\#\textgreater{} [4] "supervising an undergraduate research project;"                                 }
\CommentTok{\#\textgreater{} [5] "developing and lecturing postgraduate modules in statistics and data science."}
\end{Highlighting}
\end{Shaded}

The difference between \texttt{html\_text()} and \texttt{html\_text2()} is in how they handle whitespace. In HTML whitespace and line breaks have very little influence over how the code is interpreted by the computer (this is similar to R but very different from Python). \texttt{html\_text()} will extract the text as it is in the raw html, while \texttt{html\_text2()} will do its best to extract the text in a way that gives you something similar to what you'd see in the browser.

\hypertarget{extracting-attributes}{%
\subsection{Extracting Attributes}\label{extracting-attributes}}

Attributes are also used to record information that you might like to collect. For example, the destination of links are stored in the \texttt{href} attribute and the source of images is stored in the \texttt{src} attribute.

As an example of this, consider trying to extract the twitter link from the icon in the page footer. This is quite tricky to locate in the html source, so I used the \href{https://rvest.tidyverse.org/articles/selectorgadget.html}{Selector Gadget} to help find the correct combination of elements.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{".compact:nth{-}child(1) .nav{-}link"}\NormalTok{)}
\CommentTok{\#\textgreater{} \{html\_node\}}
\CommentTok{\#\textgreater{} \textless{}a class="nav{-}link" href="https://www.twitter.com/zakvarty"\textgreater{}}
\CommentTok{\#\textgreater{} [1] \textless{}i class="bi bi{-}twitter" role="img"\textgreater{}\textbackslash{}n\textless{}/i\textgreater{}}
\end{Highlighting}
\end{Shaded}

To extract the \texttt{href} attribute from the scraped element, we use the \texttt{rvest::html\_attr()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{".compact:nth{-}child(1) .nav{-}link"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"href"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "https://www.twitter.com/zakvarty"}
\end{Highlighting}
\end{Shaded}

\textbf{Note:} \texttt{rvest::html\_attr()} will always return a character string (or list of character strings). If you are extracting an attribute that describes a quantity, such as the width of an image, you'll need to convert this from a string to your required data type. For example, of the width is measures in pixels you might use \texttt{as.integer()}.

\hypertarget{extracting-tables}{%
\subsection{Extracting tables}\label{extracting-tables}}

HTML tables are composed in a similar, nested manner to LaTeX tables.

There are four main elements to know about that make up an HTML table:

\begin{itemize}
\tightlist
\item
  \texttt{\textless{}table\textgreater{}},
\item
  \texttt{\textless{}tr\textgreater{}} (table row),
\item
  \texttt{\textless{}th\textgreater{}} (table heading),
\item
  \texttt{\textless{}td\textgreater{}} (table data).
\end{itemize}

Here's our simple example data, formatted as an HTML table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html\_2 }\OtherTok{\textless{}{-}} \FunctionTok{minimal\_html}\NormalTok{(}\StringTok{"}
\StringTok{  \textless{}table\textgreater{}}
\StringTok{    \textless{}tr\textgreater{}}
\StringTok{      \textless{}th\textgreater{}Name\textless{}/th\textgreater{}}
\StringTok{      \textless{}th\textgreater{}Number\textless{}/th\textgreater{}}
\StringTok{    \textless{}/tr\textgreater{}}
\StringTok{    \textless{}tr\textgreater{}}
\StringTok{      \textless{}td\textgreater{}A\textless{}/td\textgreater{}}
\StringTok{      \textless{}td\textgreater{}1\textless{}/td\textgreater{}}
\StringTok{    \textless{}/tr\textgreater{}}
\StringTok{    \textless{}tr\textgreater{}}
\StringTok{      \textless{}td\textgreater{}B\textless{}/td\textgreater{}}
\StringTok{      \textless{}td\textgreater{}2\textless{}/td\textgreater{}}
\StringTok{    \textless{}/tr\textgreater{}}
\StringTok{    \textless{}tr\textgreater{}}
\StringTok{      \textless{}td\textgreater{}C\textless{}/td\textgreater{}}
\StringTok{      \textless{}td\textgreater{}3\textless{}/td\textgreater{}}
\StringTok{    \textless{}/tr\textgreater{}}
\StringTok{  \textless{}/table\textgreater{}}
\StringTok{  "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since tables are a common way to store data, \texttt{\{rvest\}} includes a useful function \texttt{html\_table()} that converts directly from an HTML table into a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html\_2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"table"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_table}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   Name  Number}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 A          1}
\CommentTok{\#\textgreater{} 2 B          2}
\CommentTok{\#\textgreater{} 3 C          3}
\end{Highlighting}
\end{Shaded}

Applying this to our real scraped data we can easily extract the table of taught courses.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"table"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_table}\NormalTok{()}
\CommentTok{\#\textgreater{} \# A tibble: 25 x 3}
\CommentTok{\#\textgreater{}   Year      Course                            Role                       }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}                             \textless{}chr\textgreater{}                      }
\CommentTok{\#\textgreater{} 1 "2021{-}22" Supervised Learning               Lecturer                   }
\CommentTok{\#\textgreater{} 2 ""        Ethics in Data Science I          Lecturer                   }
\CommentTok{\#\textgreater{} 3 ""        Ethics in Data Science II         Lecturer                   }
\CommentTok{\#\textgreater{} 4 ""                                                                  }
\CommentTok{\#\textgreater{} 5 "2020{-}21" MATH562/582: Extreme Value Theory Lecturer                   }
\CommentTok{\#\textgreater{} 6 ""        MATH331: Bayesian Inference       Graduate teaching assistant}
\CommentTok{\#\textgreater{} \# ... with 19 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{tip-for-building-tibbles}{%
\section{Tip for Building Tibbles}\label{tip-for-building-tibbles}}

When scraping data from a webpage, your end-goal is typically going to be constructing a data.frame or a tibble.

If you are following our description of tidy data, you'll want each row to correspond some repeated unit on the HTML page. In this case, you should

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use \texttt{html\_elements()} to select the elements that contain each observation unit;
\item
  Use \texttt{html\_element()} to extract the variables from each of those observations.
\end{enumerate}

Taking this approach guarantees that you'll get the same number of values for each variable, because \texttt{html\_element()} always returns the same number of outputs as inputs. This is vital when you have missing data - when not every observation unit has a value for every variable of interest.

As an example, consider this extract of text about the \href{https://dplyr.tidyverse.org/reference/starwars.html\#ref-examples}{starwars dataset}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars\_html }\OtherTok{\textless{}{-}} \FunctionTok{minimal\_html}\NormalTok{(}\StringTok{"}
\StringTok{  \textless{}ul\textgreater{}}
\StringTok{    \textless{}li\textgreater{}\textless{}b\textgreater{}C{-}3PO\textless{}/b\textgreater{} is a \textless{}i\textgreater{}droid\textless{}/i\textgreater{} that weighs \textless{}span class=\textquotesingle{}weight\textquotesingle{}\textgreater{}167 kg\textless{}/span\textgreater{}\textless{}/li\textgreater{}}
\StringTok{    \textless{}li\textgreater{}\textless{}b\textgreater{}R2{-}D2\textless{}/b\textgreater{} is a \textless{}i\textgreater{}droid\textless{}/i\textgreater{} that weighs \textless{}span class=\textquotesingle{}weight\textquotesingle{}\textgreater{}96 kg\textless{}/span\textgreater{}\textless{}/li\textgreater{}}
\StringTok{    \textless{}li\textgreater{}\textless{}b\textgreater{}Yoda\textless{}/b\textgreater{} weighs \textless{}span class=\textquotesingle{}weight\textquotesingle{}\textgreater{}66 kg\textless{}/span\textgreater{}\textless{}/li\textgreater{}}
\StringTok{    \textless{}li\textgreater{}\textless{}b\textgreater{}R4{-}P17\textless{}/b\textgreater{} is a \textless{}i\textgreater{}droid\textless{}/i\textgreater{}\textless{}/li\textgreater{}}
\StringTok{  \textless{}/ul\textgreater{}}
\StringTok{  "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is an unordered list where each list item corresponds to one observational unit (one character from the starwars universe). The name of the character is given in bold, the character species is specified in italics and the weight of the character is denoted by the \texttt{.weight} class. However, some characters have only a subset of these variables defined: for example Yoda has no species entry.

If we try to extract each element directly, our vectors of variable values are of different lengths. We don't know where the missing values should be, so we can't line them back up to make a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars\_html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "C{-}3PO"  "R2{-}D2"  "Yoda"   "R4{-}P17"}
\NormalTok{starwars\_html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"i"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "droid" "droid" "droid"}
\NormalTok{starwars\_html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{".weight"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "167 kg" "96 kg"  "66 kg"}
\end{Highlighting}
\end{Shaded}

What we should do instead is start by extracting all of the list item elements using \texttt{html\_elements()}. Once we have done this, we can then use \texttt{html\_element()} to extract each variable for all characters. This will pad with NAs, so that we can collate them into a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{starwars\_characters }\OtherTok{\textless{}{-}}\NormalTok{ starwars\_html }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"li"}\NormalTok{)}

\NormalTok{starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "C{-}3PO"  "R2{-}D2"  "Yoda"   "R4{-}P17"}
\NormalTok{starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{"i"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "droid" "droid" NA      "droid"}
\NormalTok{starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{".weight"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "167 kg" "96 kg"  "66 kg"  NA}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tibble}\SpecialCharTok{::}\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{name =}\NormalTok{ starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{(),}
  \AttributeTok{species =}\NormalTok{ starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{"i"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{(),}
  \AttributeTok{weight =}\NormalTok{ starwars\_characters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_element}\NormalTok{(}\StringTok{".weight"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text2}\NormalTok{()}
\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 3}
\CommentTok{\#\textgreater{}   name   species weight}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}  \textless{}chr\textgreater{}   \textless{}chr\textgreater{} }
\CommentTok{\#\textgreater{} 1 C{-}3PO  droid   167 kg}
\CommentTok{\#\textgreater{} 2 R2{-}D2  droid   96 kg }
\CommentTok{\#\textgreater{} 3 Yoda   \textless{}NA\textgreater{}    66 kg }
\CommentTok{\#\textgreater{} 4 R4{-}P17 droid   \textless{}NA\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-apis}{%
\chapter{APIs}\label{data-apis}}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{aquiring-data-via-an-api}{%
\section{Aquiring Data Via an API}\label{aquiring-data-via-an-api}}

We've already established that you can't always rely on tidy, tabular data to land on your desk.

Sometimes you are going to have to go out and gather data for yourself. We have already seen how to scrape information directly from the HTML source of a webpage. But surely there has to be an easer way. Thankfully, there often is!

In this chapter we will cover the basics of obtaining data via an API. This material draws together the \href{https://zapier.com/learn/apis/}{Introduction to APIs} book by Brian Cooksey and the \href{https://stat545.com/diy-web-data.html\#interacting-with-an-api}{DIY web data} section of STAT545 at the University of British Columbia.

\hypertarget{why-do-i-need-to-know-about-apis}{%
\section{Why do I need to know about APIs?}\label{why-do-i-need-to-know-about-apis}}

\begin{quote}
An API, or application programming interface, is a set of rules that allows different software applications to communicate with each other.
\end{quote}

As a data scientist, you will often need to access data that is stored on remote servers or in cloud-based services. APIs provide a convenient way for data scientists to programmatically retrieve this data, without having to manually download data sets or and process them locally on their own computer.

This has multiple benefits including automation and standardisation of data sharing.

\begin{itemize}
\item
  \textbf{Automation:} It is much faster for a machine to process a data request than a human. Having a machine handling data requests also scales much better as either the number or the complexity of data requests grows. Additionally, there is a lower risk of introducing human error. For example, a human might accidentally share the wrong data, which can have serious legal repercussions.
\item
  \textbf{Standardisation:} Having a machine process data requests requires the format of these requests and the associated responses to be standardised. This allows data sharing and retrieval to become a reproducible and programmatic aspect of our work.
\end{itemize}

\hypertarget{what-is-an-api}{%
\section{What is an API?}\label{what-is-an-api}}

So then, if APIs are so great, what exactly are they?

In human-to-human communication, the set of rules governing acceptable behaviour is known as etiquette. Depending on when or where you live, social etiquette can be rather strict. The rules for computer-to-computer communication take this to a whole new level, because with machines there can be no room left for interpretation.

The set of rules governing interactions between computers or programmes is known as a \textbf{protocol}.

APIs provide a standard protocol for different programs to interact with one another. This makes it easier for developers to build complex systems by leveraging the functionality of existing services and platforms. The benefits of working in a standardised and modular way apply equally well to sharing data as they do to writing code or organising files.

There are two sides to communication and when \emph{machines} communicate these are known as the \textbf{server} and the \textbf{client}.

Servers can seem intimidating, because unlike your laptop or mobile phone they don't have their own input and output devices; they have no keyboard, no monitor, and no a mouse. Despite this, servers are just regular computers that are designed to store data and run programmes. Servers don't have their own input or output devices because they are intended to be used \emph{remotely}, via another computer. There is no need for a screen or a mouse if the user is miles away. Nothing scary going on here!

People often find clients much less intimidating - they are simply any other computer or application that might contact the sever.

\hypertarget{http}{%
\section{HTTP}\label{http}}

This leads us one step further down the rabbit-hole. An API is a protocol that defines the rules of how applications communicate with one another. But how does this communication happen?

HTTP (Hypertext Transfer Protocol) is the dominant mode communication on the World Wide Web. You can see the secure version of HTTP, HTTPS, at the start of most web addresses up at the top of your browser. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{https://www.zakvarty.com/blog}
\end{Highlighting}
\end{Shaded}

HTTP is the foundation of data communication on the web and is used to transfer files (such as text, images, and videos) between web servers and clients.

To understand HTTP communications, I find it helpful to imagine the client and the server as being a customer and a waiter at a restaurant. The client makes some request to the server, which then tries to comply before giving a response. The server might respond to confirm that the request was completed successfully. Alternatively, the server might respond with an error message, which is (hopefully) informative about why the request could not be completed.

This request-response model is the basis for HTTP, the communication system used by the majority of APIs.

\hypertarget{http-requests}{%
\section{HTTP Requests}\label{http-requests}}

An HTTP request consists of:

\begin{itemize}
\tightlist
\item
  Uniform Resource Locator (URL) {[}unique identifier for a thing{]}
\item
  Method {[}tells server the type of action requested by client{]}
\item
  Headers {[}meta-information about request, e.g.~device type{]}
\item
  Body {[}Data the client wants to send to the server{]}
\end{itemize}

\hypertarget{url}{%
\subsection{URL}\label{url}}

The URL in a HTTP request specifies where that request is going to be made, for example \texttt{http://example.com}.

\hypertarget{method}{%
\subsection{Method}\label{method}}

The action that the client wants to take is indicated by a set of well-defined methods or HTTP verbs. The most common HTTP verbs are \texttt{GET}, \texttt{POST}, \texttt{PUT}, \texttt{PATCH}, and \texttt{DELETE}.

The \texttt{GET} verb is used to retrieve a resource from the server, such as a web page or an image. The \texttt{POST} verb is used to send data to the server, such as when submitting a form or uploading a file. The \texttt{PUT} verb is used to replace a resource on the server with a new one, while the \texttt{PATCH} verb is used to update a resource on the server without replacing it entirely. Finally, the \texttt{DELETE} verb is used to delete a resource from the server.

In addition to these common HTTP verbs, there are also several less frequently used verbs. These are used for specialized purposes, such as requesting only the headers of a resource, or testing the connectivity between the client and the server.

\hypertarget{header}{%
\subsection{Header}\label{header}}

The request headers contain meta-information about the request. This is where information about the device type would be included within the request.

\hypertarget{body}{%
\subsection{Body}\label{body}}

Finally, the body of the request contains the data that the client is providing to the server.

\hypertarget{http-responses}{%
\section{HTTP Responses}\label{http-responses}}

When the server receives a request it will attempt to fulfil it and then send a response back to the client.

A response has a similar structure to a request apart from:

\begin{itemize}
\tightlist
\item
  responses \textbf{do not have} a URL,
\item
  responses \textbf{do not have} a method,
\item
  responses \textbf{have} a status code.
\end{itemize}

\hypertarget{status-codes}{%
\subsection{Status Codes}\label{status-codes}}

The status code is a 3 digit number, each of which has a specific meaning. Some common error codes that you might (already have) come across are:

\begin{itemize}
\tightlist
\item
  200: Success,
\item
  404: Page not found (all 400s are errors),
\item
  503: Page down.
\end{itemize}

In a data science context, a successful response will return the requested data within the data field. This will most likely be given in JSON or XML format.

\hypertarget{authentication}{%
\section{Authentication}\label{authentication}}

Now that we know \emph{how} applications communicate, you might ask how we can control who has access to the API and what types of request they can make. This can be done by the server setting appropriate permissions for each client. But then how does the server verify that the client is really who is claims to be?

\textbf{Authentication} is a way to ensure that only authorized clients are able to access an API. This is typically done by the server requiring each client to provide some secret information that uniquely identifies them, whenever they make requests to the API. This information allows the API server to validate the authenticity this user before it authorises the request.

\hypertarget{basic-authentication}{%
\subsection{Basic Authentication}\label{basic-authentication}}

There are various ways to implement API authentication.

Basic authentication involves each legitimate client having a username and password. An encrypted version of these is included in the \texttt{Authorization} header of the HTTP request. If the hear matches with the server's records then the request is processed. If not, then a special status code (401) is returned to the client.

Basic authentication is dangerous because it does not put any restrictions on what a client can do once they are authorised. Additional, individualised restrictions can be added by using an alternative authentication scheme.

\hypertarget{api-key-authentication}{%
\subsection{API Key Authentication}\label{api-key-authentication}}

An API key is long, random string of letters and numbers that is assigned to each authorised user. An API key is distinct from the user's password and keys are typically issued by the service that provides an API. Using keys rather than basic authentication allows the API provider to track and limit the usage of their API.

For example, a provider may issue a unique API key to each developer or organization that wants to use the API. The provider can then limit access to certain data. They could also limit the number of requests that each key can make in a given time period or prevent access to certain administrative functions, like changing passwords or deleting accounts.

Unlike Basic Authentication, there is no standard way of a client sharing a key with the server. Depending on the API this might be in the \texttt{Authorization} field of the header, at the end of the URL (\texttt{http://example.com?api\_key=my\_secret\_key}), or within the body of the data.

\hypertarget{api-wrappers}{%
\section{API wrappers}\label{api-wrappers}}

We've learned a lot about how the internet works. Fortunately, a lot of the time we won't have to worry about all of that new information other than for debugging purposes.

In the best case scenario, a very kind developer has written a ``wrapper'' function for the API. These wrappers are functions in R that will construct the HTTP request for you. If you are particularly lucky, the API wrapper will also format the response for you, converting it from XML or JSON back into an R object that is ready for immediate use.

\hypertarget{geonames-wrapper}{%
\section{\texorpdfstring{\texttt{\{geonames\}} wrapper}{\{geonames\} wrapper}}\label{geonames-wrapper}}

\href{https://ropensci.org/}{rOpenSci} has a curated list of many wrappers for accessing scientific data using R. We will focus on the \href{https://www.geonames.org/}{GeoNames API}, which gives open access to a geographical database. To access this data, we will use wrapper functions provided by the \texttt{\{geonames\}} \href{https://docs.ropensci.org/geonames/}{package}.

The aim here is to illustrate the important steps of getting started with a new API.

\hypertarget{set-up}{%
\subsection{Set up}\label{set-up}}

Before we can get any data from the GeoNames API, we first need to do a little bit of set up.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Install and load \texttt{\{geonames\}} from CRAN
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages("geonames")}
\FunctionTok{library}\NormalTok{(geonames)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Create a \href{https://www.geonames.org/login}{user account} for the GeoNames API
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Activate the account (see activation email)
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Enable the free web services for your GeoNames account by logging in at this \href{http://www.geonames.org/enablefreewebservice}{link}.
\item
  Tell R your credentials for GeoNames.
\end{enumerate}

We could use the following code to tell R our credentials, but we absolutely should not.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{geonamesUsername=}\StringTok{"example\_username"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This would save our username as an environment variable, but it \emph{also} puts our API credentials directly into the script. If we share the script with others (internally, externally or publicly) we would be sharing our credentials too. Not good!

\hypertarget{keep-it-secret-keep-it-safe}{%
\subsection{Keep it Secret, Keep it Safe}\label{keep-it-secret-keep-it-safe}}

The solution to this problem is to add our credentials as environment variables in our \texttt{.Rprofile} rather than in this script. The \texttt{.Rprofile} is an R script that is run at the start of every session. IT can be created and edited directly, but can also be created and edited from within R.

To make/open your \texttt{.Rprofile} use the \texttt{edit\_r\_profile()} function from the \texttt{\{usethis\}} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(usethis)}
\NormalTok{usethis}\SpecialCharTok{::}\FunctionTok{edit\_r\_profile}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Within this file, add \texttt{options(geonamesUsername="example\_username")} on a new line, remembering to replace \texttt{example\_username} with your own GeoNames username.

The final step is to \textbf{check this this file ends with a blank line}, save it and restart R. Then we are all set to start using \texttt{\{geonames\}}.

This set up procedure is indicative of most API wrappers, but of course the details will vary between each API. This is why good documentation is important!

\hypertarget{using-geonames}{%
\subsection{\texorpdfstring{Using \texttt{\{geonames\}}}{Using \{geonames\}}}\label{using-geonames}}

GeoNames has a whole host of \href{http://www.geonames.org/export/ws-overview.html}{different geo-datasets} that you can explore.
As a first example, let's get all of the geo-tagged wikipedia articles that are within 1km of Imperial College London.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imperial\_coords }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{lat =} \FloatTok{51.49876}\NormalTok{, }\AttributeTok{lon =} \SpecialCharTok{{-}}\FloatTok{0.1749}\NormalTok{)}
\NormalTok{search\_radius\_km }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{imperial\_neighbours }\OtherTok{\textless{}{-}}\NormalTok{ geonames}\SpecialCharTok{::}\FunctionTok{GNfindNearbyWikipedia}\NormalTok{(}
  \AttributeTok{lat =}\NormalTok{ imperial\_coords}\SpecialCharTok{$}\NormalTok{lat,}
  \AttributeTok{lng =}\NormalTok{ imperial\_coords}\SpecialCharTok{$}\NormalTok{lon, }
  \AttributeTok{radius =}\NormalTok{ search\_radius\_km,}
  \AttributeTok{lang =} \StringTok{"en"}\NormalTok{,                }\CommentTok{\# english language articles}
  \AttributeTok{maxRows =} \DecValTok{500}              \CommentTok{\# maximum number of results to return }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Looking at the structure of \texttt{imperial\_neighbours} we can see that it is a data frame with one row per geo-tagged wikipedia article.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(imperial\_neighbours)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    149 obs. of  13 variables:}
\CommentTok{\#\textgreater{}  $ summary     : chr  "Over the Air is an annual mobile technology{-}focused overnight hack day event held in London since 2008. It deve"| \_\_truncated\_\_ "Imperial College Business School is a business school located in London, United Kingdom and a constituent of Im"| \_\_truncated\_\_ "Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est"| \_\_truncated\_\_ "Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one"| \_\_truncated\_\_ ...}
\CommentTok{\#\textgreater{}  $ elevation   : chr  "18" "18" "19" "24" ...}
\CommentTok{\#\textgreater{}  $ feature     : chr  "event" "edu" "landmark" "edu" ...}
\CommentTok{\#\textgreater{}  $ lng         : chr  "{-}0.1749" "{-}0.1748" "{-}0.17425" "{-}0.1757" ...}
\CommentTok{\#\textgreater{}  $ distance    : chr  "0.0044" "0.0494" "0.0508" "0.0558" ...}
\CommentTok{\#\textgreater{}  $ countryCode : chr  "GB" "GB" "GB" "GB" ...}
\CommentTok{\#\textgreater{}  $ rank        : chr  "52" "84" "83" "95" ...}
\CommentTok{\#\textgreater{}  $ lang        : chr  "en" "en" "en" "en" ...}
\CommentTok{\#\textgreater{}  $ title       : chr  "Over the Air" "Imperial College Business School" "Exhibition Road" "Imperial College School of Medicine" ...}
\CommentTok{\#\textgreater{}  $ lat         : chr  "51.4988" "51.4992" "51.4989722222222" "51.4987" ...}
\CommentTok{\#\textgreater{}  $ wikipediaUrl: chr  "en.wikipedia.org/wiki/Over\_the\_Air" "en.wikipedia.org/wiki/Imperial\_College\_Business\_School" "en.wikipedia.org/wiki/Exhibition\_Road" "en.wikipedia.org/wiki/Imperial\_College\_School\_of\_Medicine" ...}
\CommentTok{\#\textgreater{}  $ thumbnailImg: chr  NA NA NA NA ...}
\CommentTok{\#\textgreater{}  $ geoNameId   : chr  NA NA NA NA ...}
\end{Highlighting}
\end{Shaded}

To confirm we have the correct location we can inspect the title of the first five neighbours.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imperial\_neighbours}\SpecialCharTok{$}\NormalTok{title[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] "Over the Air"                       }
\CommentTok{\#\textgreater{} [2] "Imperial College Business School"   }
\CommentTok{\#\textgreater{} [3] "Exhibition Road"                    }
\CommentTok{\#\textgreater{} [4] "Imperial College School of Medicine"}
\CommentTok{\#\textgreater{} [5] "Skempton Building"}
\end{Highlighting}
\end{Shaded}

Nothing too surprising here, mainly departments of the college and Exhibition Road, which runs along one side of the campus. These sorts of check are important - I initially forgot the minus in the longitude and was getting results in East London!

\hypertarget{what-if-there-is-no-wrapper}{%
\section{What if there is no wrapper?}\label{what-if-there-is-no-wrapper}}

If there is not a wrapper function, we can still access APIs fairly easilty using the \texttt{\{httr\}} package.

We will look at an example using \href{http://www.omdbapi.com/}{OMDb}, which is an open source version of \href{https://www.imdb.com/}{IMDb}, to get information about the movie Mean Girls.

To use the OMDB API you will once again need to \href{http://www.omdbapi.com/apikey.aspx}{request a free API key}, follow a verification link and add your API key to your \texttt{.Rprofile}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add this to .Rprofile, pasting in your own API key}
\FunctionTok{options}\NormalTok{(}\AttributeTok{OMDB\_API\_Key =} \StringTok{"PASTE YOUR KEY HERE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can then restart R and safely access your API key from within your R session.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load your API key into the current R session,}
\NormalTok{ombd\_api\_key }\OtherTok{\textless{}{-}} \FunctionTok{getOption}\NormalTok{(}\StringTok{"OMDB\_API\_Key"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Using the documentation for the API, requests have URLs of the following form, where terms in angular brackets should be replaced by you.

\begin{verbatim}
http://www.omdbapi.com/?t=<TITLE>&y=<YEAR>&plot=<LENGTH>&r=<FORMAT>&apikey=<API_KEY>
\end{verbatim}

With a little bit of effort, we can write a function that composes this type of request URL for us. We will using the \texttt{\{glue\}} package to help us join strings together.

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\#\textquotesingle{} Compose search requests for the OMBD API}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @param title String defining title to search for. Words are separated by "+".}
\CommentTok{\#\textquotesingle{} @param year String defining release year to search for}
\CommentTok{\#\textquotesingle{} @param plot String defining whether "short" or "full" plot is returned}
\CommentTok{\#\textquotesingle{} @param format String defining return format. One of "json" or "xml"}
\CommentTok{\#\textquotesingle{} @param api\_key String defining your OMDb API key.}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @return String giving a OMBD search request URL}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @examples }
\CommentTok{\#\textquotesingle{} omdb\_url("mean+girls", "2004", "short", "json", getOption(OMBD\_API\_Key))}
\CommentTok{\#\textquotesingle{} }
\NormalTok{omdb\_url }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(title, year, plot, format, api\_key) \{}
\NormalTok{  glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}\StringTok{"http://www.omdbapi.com/?t=\{title\}\&y=\{year\}\&plot=\{plot\}\&r=\{format\}\&apikey=\{api\_key\}"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Running the example we get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_girls\_request }\OtherTok{\textless{}{-}} \FunctionTok{omdb\_url}\NormalTok{(}
  \AttributeTok{title =} \StringTok{"mean+girls"}\NormalTok{,}
  \AttributeTok{year =}  \StringTok{"2004"}\NormalTok{,}
  \AttributeTok{plot =} \StringTok{"short"}\NormalTok{,}
  \AttributeTok{format =}  \StringTok{"json"}\NormalTok{,}
  \AttributeTok{api\_key =}  \FunctionTok{getOption}\NormalTok{(}\StringTok{"OMDB\_API\_Key"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We can then use the \texttt{\{httr\}} package to construct our request and store the response we get.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{response }\OtherTok{\textless{}{-}}\NormalTok{ httr}\SpecialCharTok{::}\FunctionTok{GET}\NormalTok{(}\AttributeTok{url =}\NormalTok{ mean\_girls\_request)}
\NormalTok{httr}\SpecialCharTok{::}\FunctionTok{status\_code}\NormalTok{(response)}
\CommentTok{\#\textgreater{} [1] 200}
\end{Highlighting}
\end{Shaded}

Thankfully it was a success! If you get a 401 error code here, check that you have clicked the activation link for your API key.

The full structure of the response is quite complicated, but we can easily extract the requested data using \texttt{content()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{httr}\SpecialCharTok{::}\FunctionTok{content}\NormalTok{(response)}
\CommentTok{\#\textgreater{} $Title}
\CommentTok{\#\textgreater{} [1] "Mean Girls"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Year}
\CommentTok{\#\textgreater{} [1] "2004"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Rated}
\CommentTok{\#\textgreater{} [1] "PG{-}13"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Released}
\CommentTok{\#\textgreater{} [1] "30 Apr 2004"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Runtime}
\CommentTok{\#\textgreater{} [1] "97 min"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Genre}
\CommentTok{\#\textgreater{} [1] "Comedy"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Director}
\CommentTok{\#\textgreater{} [1] "Mark Waters"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Writer}
\CommentTok{\#\textgreater{} [1] "Rosalind Wiseman, Tina Fey"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Actors}
\CommentTok{\#\textgreater{} [1] "Lindsay Lohan, Jonathan Bennett, Rachel McAdams"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Plot}
\CommentTok{\#\textgreater{} [1] "Cady Heron is a hit with The Plastics, the A{-}list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex{-}boyfriend of alpha Plastic Regina George."}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Language}
\CommentTok{\#\textgreater{} [1] "English, German, Vietnamese, Swahili"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Country}
\CommentTok{\#\textgreater{} [1] "United States, Canada"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Awards}
\CommentTok{\#\textgreater{} [1] "7 wins \& 25 nominations"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Poster}
\CommentTok{\#\textgreater{} [1] "https://m.media{-}amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@.\_V1\_SX300.jpg"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ratings}
\CommentTok{\#\textgreater{} $Ratings[[1]]}
\CommentTok{\#\textgreater{} $Ratings[[1]]$Source}
\CommentTok{\#\textgreater{} [1] "Internet Movie Database"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ratings[[1]]$Value}
\CommentTok{\#\textgreater{} [1] "7.1/10"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ratings[[2]]}
\CommentTok{\#\textgreater{} $Ratings[[2]]$Source}
\CommentTok{\#\textgreater{} [1] "Rotten Tomatoes"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ratings[[2]]$Value}
\CommentTok{\#\textgreater{} [1] "84\%"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ratings[[3]]}
\CommentTok{\#\textgreater{} $Ratings[[3]]$Source}
\CommentTok{\#\textgreater{} [1] "Metacritic"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Ratings[[3]]$Value}
\CommentTok{\#\textgreater{} [1] "66/100"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Metascore}
\CommentTok{\#\textgreater{} [1] "66"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $imdbRating}
\CommentTok{\#\textgreater{} [1] "7.1"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $imdbVotes}
\CommentTok{\#\textgreater{} [1] "387,337"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $imdbID}
\CommentTok{\#\textgreater{} [1] "tt0377092"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Type}
\CommentTok{\#\textgreater{} [1] "movie"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $DVD}
\CommentTok{\#\textgreater{} [1] "21 Sep 2004"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $BoxOffice}
\CommentTok{\#\textgreater{} [1] "$86,058,055"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Production}
\CommentTok{\#\textgreater{} [1] "N/A"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Website}
\CommentTok{\#\textgreater{} [1] "N/A"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} $Response}
\CommentTok{\#\textgreater{} [1] "True"}
\end{Highlighting}
\end{Shaded}

\hypertarget{wrapping-up-2}{%
\section{Wrapping up}\label{wrapping-up-2}}

We have learned a bit more about how the internet works, the benefits of using an API to share data and how to request data from Open APIs.

When obtaining data from the internet it's vital that you keep your credentials safe, and that don't do more work than is needed.

\begin{itemize}
\item
  Keep your API keys out of your code. Store them in your \texttt{.Rprofile} (and make sure this is not under version control!)
\item
  Scraping is always a last resort. Is there an API already?
\item
  Writing your own code to access an API can be more painful than necessary.
\item
  Don't repeat other people, if a suitable wrapper exists then use it.
\end{itemize}

\hypertarget{data-checklist}{%
\chapter*{Checklist}\label{data-checklist}}
\addcontentsline{toc}{chapter}{Checklist}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{videos-chapters-1}{%
\section*{Videos / Chapters}\label{videos-chapters-1}}
\addcontentsline{toc}{section}{Videos / Chapters}

\begin{itemize}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d80e9045-22e7-4a0e-a0fc-af8100d3e727}{Tabular Data} (27 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/02-01-tabular-data-and-csvs/02-01-tabular-data.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=239ba39e-8a06-4e7b-a6c1-af7200f91d2b}{Web Scraping} (22 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/02-02-webscraping/02-02-web-scraping.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=e1ed8e4f-cbaa-40c2-8c44-af7200ee2e9f}{APIs} (25 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/02-03-apis/02-03-apis.pdf}{{[}slides{]}}
\end{itemize}

\hypertarget{reading-1}{%
\section*{Reading}\label{reading-1}}
\addcontentsline{toc}{section}{Reading}

Use the \protect\hyperlink{data-reading}{Acquiring and Sharing Data section} of the reading list to support and guide your exploration of this week's topics. Note that these texts are divided into core reading, reference materials and materials of interest.

\hypertarget{tasks-1}{%
\section*{Tasks}\label{tasks-1}}
\addcontentsline{toc}{section}{Tasks}

\emph{Core:}

\begin{itemize}
\item
  Revisit the Projects that you explored on Github last week. This time look for any data or documentation files.

  \begin{itemize}
  \tightlist
  \item
    Are there any file types that are new to you?
  \item
    If so, are there packages or helper function that would let you read this data into R?
  \item
    Why might you not find many data files on Github?
  \end{itemize}
\item
  Play \href{https://flukeout.github.io/}{CSS Diner} to familiarise yourself with some CSS selectors.
\item
  Identify 3 APIs that give access to data on topics that interest you. Write a post on the discussion forum describing the APIs and use one of them to load some data into R.
\item
  Scraping Book Reviews:

  \begin{itemize}
  \tightlist
  \item
    Visit the Amazon page for R for Data Science. Write code to scrape the percentage of customers giving each ``star'' rating (5, \ldots, 1).
  \item
    Turn your code into a function that will return a tibble of the form:
  \end{itemize}
\end{itemize}

\begin{tabular}{l|r|r|r|r|r|r|l}
\hline
product & n\_reviews & percent\_5\_star & percent\_4\_star & percent\_3\_star & percent\_2\_star & percent\_1\_star & url\\
\hline
example\_name & 1000 & 20 & 20 & 20 & 20 & 20 & www.example.com\\
\hline
\end{tabular}

\begin{itemize}
\item
  Generalise your function to work for other Amazon products, where the function takes as input a vector of product names and an associated vector of URLs.
\item
  Use your function to compare the reviews of the following three books: \href{https://www.amazon.com/Data-Science-Transform-Visualize-Model/dp/1491910399/ref=sr_1_1?keywords=r+for+data+science\&qid=1674145765\&s=books\&sprefix=R+for+data+\%2Cstripbooks-intl-ship\%2C157\&sr=1-1}{R for Data Science}, \href{https://www.amazon.com/Packages-Organize-Test-Document-Share/dp/1491910593/ref=sr_1_1?crid=XWR8O7WPKZS9\&keywords=R+packages\&qid=1674145743\&s=books\&sprefix=r+package\%2Cstripbooks-intl-ship\%2C158\&sr=1-1}{R packages} and \href{https://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/331924275X/ref=sr_1_1?crid=24WRUZ93PL2E6\&keywords=ggplot2\&qid=1674145703\&s=books\&sprefix=ggplot2\%2Cstripbooks-intl-ship\%2C190\&sr=1-1}{ggplot2}.
\end{itemize}

\emph{Bonus:}

\begin{itemize}
\tightlist
\item
  Add this function to the R package you made last week, remembering to add tests and documentation.
\end{itemize}

\hypertarget{live-session-1}{%
\section*{Live Session}\label{live-session-1}}
\addcontentsline{toc}{section}{Live Session}

In the live session we will begin with a discussion of this week's tasks. We will then work through some examples of how to read data from non-standard sources.

Please come to the live session prepared to discuss the following points:

\begin{itemize}
\item
  Roger Peng states that files can be imported and exported using \texttt{readRDS()} and \texttt{saveRDS()} for fast and space efficient data storage. What is the downside to doing so?
\item
  What data types have you come across (that we have not discussed already) and in what context are they used?
\item
  What do you have to give greater consideration to when scraping data than when using an API?
\end{itemize}

\hypertarget{part-data-exploration-and-visualisation}{%
\chapter{(PART) Data Exploration and Visualisation}\label{part-data-exploration-and-visualisation}}

\hypertarget{edav-introduction}{%
\chapter*{Introduction}\label{edav-introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

Now that we have read our raw data into R we can start getting our data science project moving and being to see some initial returns on the time and effort that we have invested so far.

In this section we will explore how to wrangle, explore and visualise the data that forms the basis of our projects. These skills are often overlooked by folks coming into data science as being ``soft skills'' compared to modelling. However, I would argue that this is not the case because each of these tasks requires its own specialist knowledge and tools.

Additionally, these task make up the majority of data scientist's work and are often where we can add the most value to an organisation. At this stage in a project we turn useless, messy data into a form that can be used; we derive initial insights from this data while making minimal assumptions; and we communicate all of this in an accurate and engaging way, to drive decision making both within and outwith the organisation.

\hypertarget{edav-wrangling}{%
\chapter{Data Wrangling}\label{edav-wrangling}}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{what-is-data-wrangling}{%
\section{What is Data Wrangling?}\label{what-is-data-wrangling}}

Okay, so you've got some data. That's a great start!

You might have had it handed to you by a collaborator, \protect\hyperlink{data-apis}{requested it via an API} or \protect\hyperlink{data-webscraping}{scraped it from the raw html of a webpage}. In the worst case scenario, you're an \emph{actual} scientist (not just a \emph{data} one) and you spent the last several months of your life painstakingly measuring flower petals or car parts. Now we really want to do something useful with that data.

We've seen already how you can load the data into R and pivot between wider and longer formats, but that probably isn't enough to satisfy your curiosity. You want to be able to view your data, manipulate and subset it, create new variables from existing ones and cross-reference your dataset with others. All of these are things possible in R and are known under various collective names including data manipulation, data munging and data wrangling.

I've decided to use the term data wrangling here. That's because data manipulation sounds boring as heck and data munging is both unpleasant to say and makes me imagine we are squelching through some sort of information swamp.

In what follows, I'll give a fly-by tour of tools for data wrangling in R, showing some examples along the way. I'll focus on some of the most common and useful operations and link out to some more extensive guides for wrangling your data in R, that you can refer back to as you need them.

\hypertarget{example-data-sets}{%
\section{Example Data Sets}\label{example-data-sets}}

To demonstrate some standard skills we will use two datasets. The \texttt{mtcars} data comes built into any R installation. The second data set we will look at is the \texttt{penguins} data from \texttt{\{palmerpenguins\}}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(palmerpenguins)}
\NormalTok{pengins }\OtherTok{\textless{}{-}}\NormalTok{ palmerpenguins}\SpecialCharTok{::}\NormalTok{penguins}
\NormalTok{cars }\OtherTok{\textless{}{-}}\NormalTok{ datasets}\SpecialCharTok{::}\NormalTok{mtcars}
\end{Highlighting}
\end{Shaded}

\hypertarget{viewing-your-data}{%
\section{Viewing Your Data}\label{viewing-your-data}}

\hypertarget{view}{%
\subsection{\texorpdfstring{\texttt{View()}}{View()}}\label{view}}

The \texttt{View()} function can be used to create a spreadsheet-like view of your data. In RStudio this will open as a new tab.

\texttt{View()} will work for any ``matrix-like'' R object, such as a tibble, data frame, vector or matrix. Note the capital letter - the function is called \texttt{View()}, not \texttt{view()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(penguins)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth,height=\textheight]{images/301-edav-wrangling/view-penguins-screenshot.png}
\caption{Screenshot of RStduio files pane, containg a spreadsheet view of the palmer penguins data set.}
\end{figure}

\hypertarget{head}{%
\subsection{\texorpdfstring{\texttt{head()}}{head()}}\label{head}}

For large data sets, you might not want (or be able to) view it all at once. You can then use \texttt{head()} to view the first few rows. The integer argument \texttt{n} specifies the number of rows you would like to return.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pengins, }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 8}
\CommentTok{\#\textgreater{}   species island    bill\_length\_mm bill\_depth\_mm flippe\textasciitilde{}1 body\_\textasciitilde{}2 sex    year}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}              \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{}    \textless{}int\textgreater{}   \textless{}int\textgreater{} \textless{}fct\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007}
\CommentTok{\#\textgreater{} 2 Adelie  Torgersen           39.5          17.4      186    3800 fema\textasciitilde{}  2007}
\CommentTok{\#\textgreater{} 3 Adelie  Torgersen           40.3          18        195    3250 fema\textasciitilde{}  2007}
\CommentTok{\#\textgreater{} \# ... with abbreviated variable names 1: flipper\_length\_mm, 2: body\_mass\_g}
\end{Highlighting}
\end{Shaded}

\hypertarget{str}{%
\subsection{\texorpdfstring{\texttt{str()}}{str()}}\label{str}}

An alternative way to view the a large data set, or one with a complicated format is to examine its structure with \texttt{str()}. This is a useful way to inspect the structure of list-like objects, particularly when they've got a nested structure.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(penguins)}
\CommentTok{\#\textgreater{} tibble [344 x 8] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}  $ species          : Factor w/ 3 levels "Adelie","Chinstrap",..: 1 1 1 1 1 1 1 1 1 1 ...}
\CommentTok{\#\textgreater{}  $ island           : Factor w/ 3 levels "Biscoe","Dream",..: 3 3 3 3 3 3 3 3 3 3 ...}
\CommentTok{\#\textgreater{}  $ bill\_length\_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...}
\CommentTok{\#\textgreater{}  $ bill\_depth\_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...}
\CommentTok{\#\textgreater{}  $ flipper\_length\_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...}
\CommentTok{\#\textgreater{}  $ body\_mass\_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...}
\CommentTok{\#\textgreater{}  $ sex              : Factor w/ 2 levels "female","male": 2 1 1 NA 1 2 1 2 NA NA ...}
\CommentTok{\#\textgreater{}  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{names}{%
\subsection{\texorpdfstring{\texttt{names()}}{names()}}\label{names}}

Finally, if you just want to access the variable names you can do so with the \texttt{names()} function from base R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(penguins)}
\CommentTok{\#\textgreater{} [1] "species"           "island"            "bill\_length\_mm"   }
\CommentTok{\#\textgreater{} [4] "bill\_depth\_mm"     "flipper\_length\_mm" "body\_mass\_g"      }
\CommentTok{\#\textgreater{} [7] "sex"               "year"}
\end{Highlighting}
\end{Shaded}

Similarly, you can explicitly access the row and column names of a data frame or tibble using \texttt{colnames()} or \texttt{rownames()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(cars)}
\CommentTok{\#\textgreater{}  [1] "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"   "gear"}
\CommentTok{\#\textgreater{} [11] "carb"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rownames}\NormalTok{(cars)}
\CommentTok{\#\textgreater{}  [1] "Mazda RX4"           "Mazda RX4 Wag"       "Datsun 710"         }
\CommentTok{\#\textgreater{}  [4] "Hornet 4 Drive"      "Hornet Sportabout"   "Valiant"            }
\CommentTok{\#\textgreater{}  [7] "Duster 360"          "Merc 240D"           "Merc 230"           }
\CommentTok{\#\textgreater{} [10] "Merc 280"            "Merc 280C"           "Merc 450SE"         }
\CommentTok{\#\textgreater{} [13] "Merc 450SL"          "Merc 450SLC"         "Cadillac Fleetwood" }
\CommentTok{\#\textgreater{} [16] "Lincoln Continental" "Chrysler Imperial"   "Fiat 128"           }
\CommentTok{\#\textgreater{} [19] "Honda Civic"         "Toyota Corolla"      "Toyota Corona"      }
\CommentTok{\#\textgreater{} [22] "Dodge Challenger"    "AMC Javelin"         "Camaro Z28"         }
\CommentTok{\#\textgreater{} [25] "Pontiac Firebird"    "Fiat X1{-}9"           "Porsche 914{-}2"      }
\CommentTok{\#\textgreater{} [28] "Lotus Europa"        "Ford Pantera L"      "Ferrari Dino"       }
\CommentTok{\#\textgreater{} [31] "Maserati Bora"       "Volvo 142E"}
\end{Highlighting}
\end{Shaded}

In the \texttt{cars} data, the car model are stored as the row names. This doesn't really jive with our idea of tidy data - we'll see how to fix that shortly.

\hypertarget{renaming-variables}{%
\section{Renaming Variables}\label{renaming-variables}}

\hypertarget{colnames}{%
\subsection{\texorpdfstring{\texttt{colnames()}}{colnames()}}\label{colnames}}

The function \texttt{colnames()} can be used to set, as well as to retrieve, column names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars }
\FunctionTok{colnames}\NormalTok{(cars\_renamed)[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"miles\_per\_gallon"}
\FunctionTok{colnames}\NormalTok{(cars\_renamed)}
\CommentTok{\#\textgreater{}  [1] "miles\_per\_gallon" "cyl"              "disp"            }
\CommentTok{\#\textgreater{}  [4] "hp"               "drat"             "wt"              }
\CommentTok{\#\textgreater{}  [7] "qsec"             "vs"               "am"              }
\CommentTok{\#\textgreater{} [10] "gear"             "carb"}
\end{Highlighting}
\end{Shaded}

\hypertarget{dplyrrename}{%
\subsection{\texorpdfstring{\texttt{dplyr::rename()}}{dplyr::rename()}}\label{dplyrrename}}

We can also use functions from \texttt{\{dplyr\}} to rename columns. Let's alter the second column name.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Attaching package: \textquotesingle{}dplyr\textquotesingle{}}
\CommentTok{\#\textgreater{} The following objects are masked from \textquotesingle{}package:stats\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     filter, lag}
\CommentTok{\#\textgreater{} The following objects are masked from \textquotesingle{}package:base\textquotesingle{}:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     intersect, setdiff, setequal, union}
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(}\AttributeTok{.data =}\NormalTok{ cars\_renamed, }\AttributeTok{cylinders =}\NormalTok{ cyl)}
\FunctionTok{colnames}\NormalTok{(cars\_renamed)}
\CommentTok{\#\textgreater{}  [1] "miles\_per\_gallon" "cylinders"        "disp"            }
\CommentTok{\#\textgreater{}  [4] "hp"               "drat"             "wt"              }
\CommentTok{\#\textgreater{}  [7] "qsec"             "vs"               "am"              }
\CommentTok{\#\textgreater{} [10] "gear"             "carb"}
\end{Highlighting}
\end{Shaded}

This could be done as part of a pipe, if we were making many alterations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{displacement =}\NormalTok{ disp) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{horse\_power =}\NormalTok{ hp) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{rear\_axel\_ratio =}\NormalTok{ drat)}

\FunctionTok{colnames}\NormalTok{(cars\_renamed)}
\CommentTok{\#\textgreater{}  [1] "miles\_per\_gallon" "cylinders"        "displacement"    }
\CommentTok{\#\textgreater{}  [4] "horse\_power"      "rear\_axel\_ratio"  "wt"              }
\CommentTok{\#\textgreater{}  [7] "qsec"             "vs"               "am"              }
\CommentTok{\#\textgreater{} [10] "gear"             "carb"}
\end{Highlighting}
\end{Shaded}

When using the dplyr function you have to remember the format \texttt{new\_name\ =\ old\_name}. This matches the format used to create a data frame or tibble, but is the opposite order to the python function of the same name and often catches people out.

In the section on \protect\hyperlink{creating-new-variables}{creating new variables}, we will see an alternative way of doing this by copying the column and then deleting the original.

\hypertarget{subsetting-1}{%
\section{Subsetting}\label{subsetting-1}}

\hypertarget{base-r-1}{%
\subsection{Base R}\label{base-r-1}}

In base R you can extract rows, columns and combinations thereof using index notation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First row}
\NormalTok{penguins[}\DecValTok{1}\NormalTok{, ]}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 8}
\CommentTok{\#\textgreater{}   species island    bill\_length\_mm bill\_depth\_mm flippe\textasciitilde{}1 body\_\textasciitilde{}2 sex    year}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}              \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{}    \textless{}int\textgreater{}   \textless{}int\textgreater{} \textless{}fct\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007}
\CommentTok{\#\textgreater{} \# ... with abbreviated variable names 1: flipper\_length\_mm, 2: body\_mass\_g}

\CommentTok{\# First Column }
\NormalTok{penguins[ , }\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{} \# A tibble: 344 x 1}
\CommentTok{\#\textgreater{}   species}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}  }
\CommentTok{\#\textgreater{} 1 Adelie }
\CommentTok{\#\textgreater{} 2 Adelie }
\CommentTok{\#\textgreater{} 3 Adelie }
\CommentTok{\#\textgreater{} 4 Adelie }
\CommentTok{\#\textgreater{} 5 Adelie }
\CommentTok{\#\textgreater{} 6 Adelie }
\CommentTok{\#\textgreater{} \# ... with 338 more rows}

\CommentTok{\# Rows 2{-}3 of columns 1, 2 and 4}
\NormalTok{penguins[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)]}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 3}
\CommentTok{\#\textgreater{}   species island    bill\_depth\_mm}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}             \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen          17.4}
\CommentTok{\#\textgreater{} 2 Adelie  Torgersen          18}
\end{Highlighting}
\end{Shaded}

Using negative indexing you can remove rows or columns

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop all but first row}
\NormalTok{penguins[}\SpecialCharTok{{-}}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{344}\NormalTok{), ]}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 8}
\CommentTok{\#\textgreater{}   species island    bill\_length\_mm bill\_depth\_mm flippe\textasciitilde{}1 body\_\textasciitilde{}2 sex    year}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}              \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{}    \textless{}int\textgreater{}   \textless{}int\textgreater{} \textless{}fct\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007}
\CommentTok{\#\textgreater{} \# ... with abbreviated variable names 1: flipper\_length\_mm, 2: body\_mass\_g}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop all but first column }
\NormalTok{penguins[ , }\SpecialCharTok{{-}}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{8}\NormalTok{)]}
\CommentTok{\#\textgreater{} \# A tibble: 344 x 1}
\CommentTok{\#\textgreater{}   species}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}  }
\CommentTok{\#\textgreater{} 1 Adelie }
\CommentTok{\#\textgreater{} 2 Adelie }
\CommentTok{\#\textgreater{} 3 Adelie }
\CommentTok{\#\textgreater{} 4 Adelie }
\CommentTok{\#\textgreater{} 5 Adelie }
\CommentTok{\#\textgreater{} 6 Adelie }
\CommentTok{\#\textgreater{} \# ... with 338 more rows}
\end{Highlighting}
\end{Shaded}

You can also select rows or columns by their names. This can be done using the bracket syntax (\texttt{{[}\ {]}}) or the dollar syntax (\texttt{\$}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pengins[ , }\StringTok{"species"}\NormalTok{]}
\CommentTok{\#\textgreater{} \# A tibble: 344 x 1}
\CommentTok{\#\textgreater{}   species}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}  }
\CommentTok{\#\textgreater{} 1 Adelie }
\CommentTok{\#\textgreater{} 2 Adelie }
\CommentTok{\#\textgreater{} 3 Adelie }
\CommentTok{\#\textgreater{} 4 Adelie }
\CommentTok{\#\textgreater{} 5 Adelie }
\CommentTok{\#\textgreater{} 6 Adelie }
\CommentTok{\#\textgreater{} \# ... with 338 more rows}
\NormalTok{penguins}\SpecialCharTok{$}\NormalTok{species}
\CommentTok{\#\textgreater{}   [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}   [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{}  [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   }
\CommentTok{\#\textgreater{} [148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   }
\CommentTok{\#\textgreater{} [274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap}
\CommentTok{\#\textgreater{} [344] Chinstrap}
\CommentTok{\#\textgreater{} Levels: Adelie Chinstrap Gentoo}
\end{Highlighting}
\end{Shaded}

Since \texttt{penguins} is a tibble, these return different types of object. Subsetting a tibble with bracket syntax will return a tibble, while extracting a column using the dollar syntax returns a vector of values.

\hypertarget{filter-and-select}{%
\subsection{\texorpdfstring{\texttt{filter()} and \texttt{select()}}{filter() and select()}}\label{filter-and-select}}

\texttt{\{dplyr\}} has two functions for subsetting, \texttt{filter()} subsets by rows and \texttt{select()} subsets by column.

In both functions you list what you would like to retain. Filter and select calls can be piped together to subset based on row and column values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(species, island, body\_mass\_g)}
\CommentTok{\#\textgreater{} \# A tibble: 344 x 3}
\CommentTok{\#\textgreater{}   species island    body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}           \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen        3750}
\CommentTok{\#\textgreater{} 2 Adelie  Torgersen        3800}
\CommentTok{\#\textgreater{} 3 Adelie  Torgersen        3250}
\CommentTok{\#\textgreater{} 4 Adelie  Torgersen          NA}
\CommentTok{\#\textgreater{} 5 Adelie  Torgersen        3450}
\CommentTok{\#\textgreater{} 6 Adelie  Torgersen        3650}
\CommentTok{\#\textgreater{} \# ... with 338 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(species, island, body\_mass\_g) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(body\_mass\_g }\SpecialCharTok{\textgreater{}} \DecValTok{6000}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 3}
\CommentTok{\#\textgreater{}   species island body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}        \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Gentoo  Biscoe        6300}
\CommentTok{\#\textgreater{} 2 Gentoo  Biscoe        6050}
\end{Highlighting}
\end{Shaded}

Subsetting rows can be inverted by negating the \texttt{filter()} statement

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(species, island, body\_mass\_g) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(body\_mass\_g }\SpecialCharTok{\textgreater{}} \DecValTok{6000}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 340 x 3}
\CommentTok{\#\textgreater{}   species island    body\_mass\_g}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}           \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie  Torgersen        3750}
\CommentTok{\#\textgreater{} 2 Adelie  Torgersen        3800}
\CommentTok{\#\textgreater{} 3 Adelie  Torgersen        3250}
\CommentTok{\#\textgreater{} 4 Adelie  Torgersen        3450}
\CommentTok{\#\textgreater{} 5 Adelie  Torgersen        3650}
\CommentTok{\#\textgreater{} 6 Adelie  Torgersen        3625}
\CommentTok{\#\textgreater{} \# ... with 334 more rows}
\end{Highlighting}
\end{Shaded}

and dropping columns can done by selecting all columns except the one(s) you want to drop.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(species, island, body\_mass\_g) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(body\_mass\_g }\SpecialCharTok{\textgreater{}} \DecValTok{6000}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{c}\NormalTok{(species, island))}
\CommentTok{\#\textgreater{} \# A tibble: 340 x 1}
\CommentTok{\#\textgreater{}   body\_mass\_g}
\CommentTok{\#\textgreater{}         \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1        3750}
\CommentTok{\#\textgreater{} 2        3800}
\CommentTok{\#\textgreater{} 3        3250}
\CommentTok{\#\textgreater{} 4        3450}
\CommentTok{\#\textgreater{} 5        3650}
\CommentTok{\#\textgreater{} 6        3625}
\CommentTok{\#\textgreater{} \# ... with 334 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{creating-new-variables}{%
\section{Creating New Variables}\label{creating-new-variables}}

\hypertarget{base-r-2}{%
\subsection{Base R}\label{base-r-2}}

We can create new variables in base R by assigning a vector of the correct length to a new column name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed}\SpecialCharTok{$}\NormalTok{weight }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed}\SpecialCharTok{$}\NormalTok{wt}
\end{Highlighting}
\end{Shaded}

If we then drop the original column from the data frame, this gives us an alternative way of renaming columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed[ ,}\SpecialCharTok{{-}}\FunctionTok{which}\NormalTok{(}\FunctionTok{names}\NormalTok{(cars\_renamed) }\SpecialCharTok{==} \StringTok{"wt"}\NormalTok{)]}
\FunctionTok{head}\NormalTok{(cars\_renamed, }\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}                   miles\_per\_gallon cylinders displacement horse\_power}
\CommentTok{\#\textgreater{} Mazda RX4                     21.0         6          160         110}
\CommentTok{\#\textgreater{} Mazda RX4 Wag                 21.0         6          160         110}
\CommentTok{\#\textgreater{} Datsun 710                    22.8         4          108          93}
\CommentTok{\#\textgreater{} Hornet 4 Drive                21.4         6          258         110}
\CommentTok{\#\textgreater{} Hornet Sportabout             18.7         8          360         175}
\CommentTok{\#\textgreater{}                   rear\_axel\_ratio  qsec vs am gear carb weight}
\CommentTok{\#\textgreater{} Mazda RX4                    3.90 16.46  0  1    4    4  2.620}
\CommentTok{\#\textgreater{} Mazda RX4 Wag                3.90 17.02  0  1    4    4  2.875}
\CommentTok{\#\textgreater{} Datsun 710                   3.85 18.61  1  1    4    1  2.320}
\CommentTok{\#\textgreater{} Hornet 4 Drive               3.08 19.44  1  0    3    1  3.215}
\CommentTok{\#\textgreater{} Hornet Sportabout            3.15 17.02  0  0    3    2  3.440}
\end{Highlighting}
\end{Shaded}

One thing to be aware of is that this operation does not preserve column ordering.

Generally speaking, code that relies on columns being in a specific order is fragile - it breaks easily. If possible, you should try to write your code in another way that's robust to column reordering. I've done that here when removing the \texttt{wt} column by looking up the column index as part of my code, rather than assuming it will always be the fourth column.

\hypertarget{dplyrmutate}{%
\subsection{\texorpdfstring{\texttt{dplyr::mutate()}}{dplyr::mutate()}}\label{dplyrmutate}}

The function from \texttt{\{dplyr\}} to create new columns is \texttt{mutate()}. Let's create another column that has the car's weight in kilogrammes rather than tonnes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{weight\_kg =}\NormalTok{ weight }\SpecialCharTok{*} \DecValTok{1000}\NormalTok{)}

\NormalTok{cars\_renamed }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(miles\_per\_gallon, cylinders, displacement, weight, weight\_kg) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}                   miles\_per\_gallon cylinders displacement weight weight\_kg}
\CommentTok{\#\textgreater{} Mazda RX4                     21.0         6          160  2.620      2620}
\CommentTok{\#\textgreater{} Mazda RX4 Wag                 21.0         6          160  2.875      2875}
\CommentTok{\#\textgreater{} Datsun 710                    22.8         4          108  2.320      2320}
\CommentTok{\#\textgreater{} Hornet 4 Drive                21.4         6          258  3.215      3215}
\CommentTok{\#\textgreater{} Hornet Sportabout             18.7         8          360  3.440      3440}
\end{Highlighting}
\end{Shaded}

You can also create new columns that are functions of multiple other columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars\_renamed }\OtherTok{\textless{}{-}}\NormalTok{ cars\_renamed }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cylinder\_adjusted\_mpg =}\NormalTok{ miles\_per\_gallon }\SpecialCharTok{/}\NormalTok{ cylinders)}
\end{Highlighting}
\end{Shaded}

\hypertarget{rownames_to_column}{%
\subsection{\texorpdfstring{\texttt{rownames\_to\_column()}}{rownames\_to\_column()}}\label{rownames_to_column}}

One useful example of adding an additional row to a data frame is to convert its row names to a column of the data fame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{model =} \FunctionTok{rownames}\NormalTok{(cars\_renamed)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(mpg, cyl, model) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}                    mpg cyl             model}
\CommentTok{\#\textgreater{} Mazda RX4         21.0   6         Mazda RX4}
\CommentTok{\#\textgreater{} Mazda RX4 Wag     21.0   6     Mazda RX4 Wag}
\CommentTok{\#\textgreater{} Datsun 710        22.8   4        Datsun 710}
\CommentTok{\#\textgreater{} Hornet 4 Drive    21.4   6    Hornet 4 Drive}
\CommentTok{\#\textgreater{} Hornet Sportabout 18.7   8 Hornet Sportabout}
\end{Highlighting}
\end{Shaded}

There's a neat function called \texttt{rownames\_to\_column()} in \texttt{\{tibble\}} which will add this as the first column and remove the row names all in one step.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tibble}\SpecialCharTok{::}\FunctionTok{rownames\_to\_column}\NormalTok{(}\AttributeTok{var =} \StringTok{"model"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}               model  mpg cyl disp  hp drat    wt  qsec vs am gear carb}
\CommentTok{\#\textgreater{} 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4}
\CommentTok{\#\textgreater{} 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4}
\CommentTok{\#\textgreater{} 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1}
\CommentTok{\#\textgreater{} 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1}
\CommentTok{\#\textgreater{} 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2}
\end{Highlighting}
\end{Shaded}

\hypertarget{rowids_to_column}{%
\subsection{\texorpdfstring{\texttt{rowids\_to\_column()}}{rowids\_to\_column()}}\label{rowids_to_column}}

Another function from \texttt{\{tibble\}} adds the row id of each observation as a new column. This is often useful when ordering or combining tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tibble}\SpecialCharTok{::}\FunctionTok{rowid\_to\_column}\NormalTok{(}\AttributeTok{var =} \StringTok{"row\_id"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{}   row\_id  mpg cyl disp  hp drat    wt  qsec vs am gear carb}
\CommentTok{\#\textgreater{} 1      1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4}
\CommentTok{\#\textgreater{} 2      2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4}
\CommentTok{\#\textgreater{} 3      3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1}
\CommentTok{\#\textgreater{} 4      4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1}
\CommentTok{\#\textgreater{} 5      5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2}
\end{Highlighting}
\end{Shaded}

\hypertarget{summaries}{%
\section{Summaries}\label{summaries}}

The \texttt{summarise()} function allows you to collapse a data frame into a single row, which using a summary statistic of your choosing.

We can calculate the average bill length of all penguins in a single \texttt{summarise()} function call.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summarise}\NormalTok{(penguins, }\AttributeTok{average\_bill\_length\_mm =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm))}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}   average\_bill\_length\_mm}
\CommentTok{\#\textgreater{}                    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1                     NA}
\end{Highlighting}
\end{Shaded}

Since we have missing values, we might instead want to calculate the mean of the recorded values.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summarise}\NormalTok{(penguins, }\AttributeTok{average\_bill\_length\_mm =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}   average\_bill\_length\_mm}
\CommentTok{\#\textgreater{}                    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1                   43.9}
\end{Highlighting}
\end{Shaded}

We can also use \texttt{summarise()} to gather multiple summaries in a single data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bill\_length\_mm\_summary }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{min =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_0 =} \FunctionTok{min}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_1 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_2 =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_3 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_4 =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}

\NormalTok{bill\_length\_mm\_summary}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 8}
\CommentTok{\#\textgreater{}    mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1  43.9   44.4  59.6  32.1  39.2  44.4  39.2  59.6}
\end{Highlighting}
\end{Shaded}

In all, this isn't overly exciting. You might, rightly, wonder why you'd want to use these \texttt{summarise()} calls when we could just use the simpler base R calls directly.

One benefit is that the summarise calls ensure consistent output. However, the main advantage comes when you want to apply these summaries to distinct subgroups of the data.

\hypertarget{grouped-operations}{%
\section{Grouped Operations}\label{grouped-operations}}

The real benefit of \texttt{summarise()} comes from its combination with \texttt{group\_by()}. This allows to you calculate the same summary statistics for each level of a factor with only one additional line of code. Here we're re-calculating the same set of summary statistics we just found for all penguins, but for each individual species.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(species) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{min =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_0 =} \FunctionTok{min}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_1 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_2 =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_3 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_4 =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 9}
\CommentTok{\#\textgreater{}   species    mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie     38.8   38.8  46    32.1  36.8  38.8  36.8  46  }
\CommentTok{\#\textgreater{} 2 Chinstrap  48.8   49.6  58    40.9  46.3  49.6  46.3  58  }
\CommentTok{\#\textgreater{} 3 Gentoo     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6}
\end{Highlighting}
\end{Shaded}

You can group by multiple factors to calculate summaries for each distinct combination of levels within your data set. Here we group by combinations of species and the island to which they belong.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguin\_summary\_stats }\OtherTok{\textless{}{-}}\NormalTok{ penguins }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(species, island) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{min =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_0 =} \FunctionTok{min}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_1 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_2 =} \FunctionTok{median}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_3 =} \FunctionTok{quantile}\NormalTok{(bill\_length\_mm, }\AttributeTok{prob =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q\_4 =} \FunctionTok{max}\NormalTok{(bill\_length\_mm, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} \textasciigrave{}summarise()\textasciigrave{} has grouped output by \textquotesingle{}species\textquotesingle{}. You can override using the}
\CommentTok{\#\textgreater{} \textasciigrave{}.groups\textasciigrave{} argument.}

\NormalTok{penguin\_summary\_stats}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 10}
\CommentTok{\#\textgreater{} \# Groups:   species [3]}
\CommentTok{\#\textgreater{}   species   island     mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}fct\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6}
\CommentTok{\#\textgreater{} 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1}
\CommentTok{\#\textgreater{} 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  }
\CommentTok{\#\textgreater{} 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  }
\CommentTok{\#\textgreater{} 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6}
\end{Highlighting}
\end{Shaded}

\hypertarget{ungrouping}{%
\subsection{Ungrouping}\label{ungrouping}}

By default, each call to \texttt{summarise()} will undo one level of grouping. This means that our previous result was still grouped by species.

(We can see this in the tibble output above, and also by examining the structure of the returned data frame. This tells us that this is an S3 object of class ``grouped\_df'', which inherits its properties from a ``tbl\_df'', ``tbl'', and ``data.frame'' objects.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(penguin\_summary\_stats)}
\CommentTok{\#\textgreater{} [1] "grouped\_df" "tbl\_df"     "tbl"        "data.frame"}
\end{Highlighting}
\end{Shaded}

Since we have grouped by two variables, R expects us to use two summaries before returning a data frame (or tibble) that is not grouped. One way to satisfy this is to use apply a second summary at the species level of grouping.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguin\_summary\_stats }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise\_all}\NormalTok{(mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 10}
\CommentTok{\#\textgreater{}   species   island  mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}      \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie        NA  38.8   38.7  45.2  33.4  37.0  38.7  37.0  45.2}
\CommentTok{\#\textgreater{} 2 Chinstrap     NA  48.8   49.6  58    40.9  46.3  49.6  46.3  58  }
\CommentTok{\#\textgreater{} 3 Gentoo        NA  47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6}
\end{Highlighting}
\end{Shaded}

However, we won't always want to do apply another summary. In that case, we can undo the grouping using \texttt{ungroup()}. Remembering to ungroup is a common gotcha and cause of confusion when working with multiple-group summaries.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ungroup}\NormalTok{(penguin\_summary\_stats)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 10}
\CommentTok{\#\textgreater{}   species   island     mean median   min   q\_0   q\_1   q\_2   q\_3   q\_4}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}fct\textgreater{}     \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6}
\CommentTok{\#\textgreater{} 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1}
\CommentTok{\#\textgreater{} 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  }
\CommentTok{\#\textgreater{} 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  }
\CommentTok{\#\textgreater{} 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6}
\end{Highlighting}
\end{Shaded}

There's an alternative method to achieve the same thing in a single step when using \texttt{\{dplyr\}} versions 1.0.0 and above. This is to to set the \texttt{.groups} parameter of the \texttt{summarise()} function call, which determines the grouping of the returned data frame.

The \texttt{.groups} parameter and can take 4 possible values:

\begin{itemize}
\item
  ``drop\_last'': dropping the last level of grouping (The only option before v1.0.0);
\item
  ``drop'': All levels of grouping are dropped;
\item
  ``keep'': Same grouping structure as \texttt{.data};
\item
  ``rowwise'': Each row is its own group.
\end{itemize}

By default, ``drop\_last'' is used if all the results have 1 row and ``keep'' is used otherwise.

\hypertarget{reordering-factors}{%
\section{Reordering Factors}\label{reordering-factors}}

R stored factors as integer values, which it then maps to a set of labels. Only factor levels that appear in your data will be assigned a coded integer value and the mapping between factor levels and integers will depend on the order that the labels appear in your data.

This can be annoying, particularly when your factor levels relate to properties that aren't numerical but do have an inherent ordering to them. In the example below, we have the t-shirt size of twelve people.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tshirts }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{, }
  \AttributeTok{size =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"L"}\NormalTok{, }\ConstantTok{NA}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"S"}\NormalTok{, }\StringTok{"XS"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"XXL"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"XS"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"S"}\NormalTok{))}
\NormalTok{)}

\FunctionTok{levels}\NormalTok{(tshirts}\SpecialCharTok{$}\NormalTok{size)}
\CommentTok{\#\textgreater{} [1] "L"   "M"   "S"   "XS"  "XXL"}
\end{Highlighting}
\end{Shaded}

Irritatingly, the sizes aren't in order and extra large isn't included because it's not included in this particular sample. This leads to awkward looking summary tables and plots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tshirts }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(size) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 2}
\CommentTok{\#\textgreater{}   size  count}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 L         3}
\CommentTok{\#\textgreater{} 2 M         3}
\CommentTok{\#\textgreater{} 3 S         2}
\CommentTok{\#\textgreater{} 4 XS        2}
\CommentTok{\#\textgreater{} 5 XXL       1}
\CommentTok{\#\textgreater{} 6 \textless{}NA\textgreater{}      1}
\end{Highlighting}
\end{Shaded}

We can fix this by creating a new variable with the factors explicitly coded in the correct order. We also need to specify that we should not drop empty groups as part of \texttt{group\_by()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_tshirt\_levels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"XS"}\NormalTok{, }\StringTok{"S"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"L"}\NormalTok{, }\StringTok{"XL"}\NormalTok{, }\StringTok{"XXL"}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}

\NormalTok{tshirts }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{size\_tidy =} \FunctionTok{factor}\NormalTok{(size, }\AttributeTok{levels =}\NormalTok{ tidy\_tshirt\_levels)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(size\_tidy, }\AttributeTok{.drop =} \ConstantTok{FALSE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 7 x 2}
\CommentTok{\#\textgreater{}   size\_tidy count}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 XS            2}
\CommentTok{\#\textgreater{} 2 S             2}
\CommentTok{\#\textgreater{} 3 M             3}
\CommentTok{\#\textgreater{} 4 L             3}
\CommentTok{\#\textgreater{} 5 XL            0}
\CommentTok{\#\textgreater{} 6 XXL           1}
\CommentTok{\#\textgreater{} \# ... with 1 more row}
\end{Highlighting}
\end{Shaded}

\hypertarget{be-aware-factors}{%
\section{Be Aware: Factors}\label{be-aware-factors}}

As we have seen a little already, categorical variables can cause issues when wrangling and presenting data in R. All of these problems are solvable using base R techniques but the \texttt{\{forcats\}} package provides tools for the most common of these problems. This includes functions for changing the order of factor levels or the values with which they are associated.

Some examples functions from the package include:

\begin{itemize}
\tightlist
\item
  \texttt{fct\_reorder()}: Reordering a factor by another variable.
\item
  \texttt{fct\_infreq()}: Reordering a factor by the frequency of values.
\item
  \texttt{fct\_relevel()}: Changing the order of a factor by hand.
\item
  \texttt{fct\_lump()}: Collapsing the least/most frequent values of a factor into ``other''.
\end{itemize}

Examples of each of these can be found in the \href{https://forcats.tidyverse.org/articles/forcats.html}{forcats vignette} or the \href{(https://r4ds.had.co.nz/factors.html)}{factors chapter} of R for data science.

\hypertarget{be-aware-strings}{%
\section{Be Aware: Strings}\label{be-aware-strings}}

Working with and analysing text data is a skill unto itself. However, it is useful to be able to do some basic manipulation of character strings programatically.

Because R was developed as a statistical programming language, it is well suited to the computational and modelling aspects of working with text data but the base R string manipulation functions can be a bit unwieldy at times.

The \texttt{\{stringr\}} package aims to combat this by providing useful helper functions for a range of text management problems. Even when not analysing text data these can be useful, for example to remove prefixes on a lot of column names.

Suppose we wanted to keep only the text following an underscore in these column names. We could do that by using a regular expression to extract lower-case or upper-case letters which follow an underscore.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(poorly\_named\_df)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 11}
\CommentTok{\#\textgreater{}   observati\textasciitilde{}1   V1\_A   V2\_B   V3\_C   V4\_D    V5\_E   V6\_F   V7\_G   V8\_H   V9\_I}
\CommentTok{\#\textgreater{}         \textless{}int\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1           1  0.843  1.09  {-}0.544 {-}0.861  1.36    0.368  1.79   0.845 {-}0.711}
\CommentTok{\#\textgreater{} 2           2 {-}0.418  0.346  0.531  0.862 {-}0.0447  0.706 {-}1.40   0.616  0.154}
\CommentTok{\#\textgreater{} 3           3 {-}1.19   0.382 {-}0.169  1.77   0.400  {-}0.840  0.132  0.121 {-}2.26 }
\CommentTok{\#\textgreater{} 4           4 {-}0.762  1.08  {-}1.70   1.55  {-}1.37    0.986 {-}1.08   0.430 {-}0.888}
\CommentTok{\#\textgreater{} 5           5  0.846 {-}1.06   0.461 {-}1.30   0.837  {-}1.59   0.604  0.607 {-}1.16 }
\CommentTok{\#\textgreater{} 6           6  0.327 {-}0.281  0.188  0.528 {-}0.875  {-}0.641 {-}0.807 {-}0.447  0.515}
\CommentTok{\#\textgreater{} \# ... with 1 more variable: V10\_J \textless{}dbl\textgreater{}, and abbreviated variable name}
\CommentTok{\#\textgreater{} \#   1: observation\_id}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stringr}\SpecialCharTok{::}\FunctionTok{str\_extract}\NormalTok{(}\FunctionTok{names}\NormalTok{(poorly\_named\_df), }\AttributeTok{pattern =} \StringTok{"(?\textless{}=\_)([a{-}zA{-}Z]+)"}\NormalTok{)}
\CommentTok{\#\textgreater{}  [1] "id" "A"  "B"  "C"  "D"  "E"  "F"  "G"  "H"  "I"  "J"}
\end{Highlighting}
\end{Shaded}

Alternatively, can avoid using regular expressions. We can split each column name at the underscore and keep only the second part of each string.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# split column names at underscores and inspect structure of resuting object}
\NormalTok{split\_strings }\OtherTok{\textless{}{-}}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_split}\NormalTok{(}\FunctionTok{names}\NormalTok{(poorly\_named\_df), }\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(split\_strings)}
\CommentTok{\#\textgreater{} List of 11}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "observation" "id"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V1" "A"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V2" "B"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V3" "C"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V4" "D"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V5" "E"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V6" "F"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V7" "G"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V8" "H"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V9" "I"}
\CommentTok{\#\textgreater{}  $ : chr [1:2] "V10" "J"}

\CommentTok{\# keep only the second element of each character vector in the list}
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{map\_chr}\NormalTok{(split\_strings, }\ControlFlowTok{function}\NormalTok{(x)\{x[}\DecValTok{2}\NormalTok{]\})}
\CommentTok{\#\textgreater{}  [1] "id" "A"  "B"  "C"  "D"  "E"  "F"  "G"  "H"  "I"  "J"}
\end{Highlighting}
\end{Shaded}

Again, unless you plan to work extensively with text data, I would recommend that you look up such string manipulations as you need them. The \href{https://r4ds.had.co.nz/strings.html\#strings}{strings} section of R for Data Science is a useful starting point.

\hypertarget{be-aware-date-times}{%
\section{Be Aware: Date-Times}\label{be-aware-date-times}}

Remember all the fuss we made about storing dates in the ISO standard format? That was because dates and times are complicated enough to work with before adding extra ambiguity.

\[ \text{YYYY} - \text{MM} - \text{DD}\]
Dates, times and time intervals have to reconcile two factors: the physical orbit of the Earth around the Sun and the social and geopolitical mechanisms that determine how we measure and record the passing of time. This makes the history of date and time records fascinating and can make working with this type of data complicated.

Moving from larger to smaller time spans: leap years alter the number of days in a year, months are of variable length (with February's length changing from year to year). If your data are measured in a place that uses daylight saving, then one day a year will be 23 hours long and another will be 25 hours long. To make things worse, the dates and the hour at which the clocks change are not uniform across countries, which might be in distinct time zones that themselves change over time.

Even at the level of minutes and seconds we aren't safe - since the Earth's orbit is gradually slowing down a leap second is added approximately every 21 months. Nor are things any better when looking at longer time scales or across cultures, where we might have to account for different calendars: months are added removed and altered over time, other calendar systems still take different approaches to measuring time and using different units and origin points.

With all of these issues you have to be very careful when working with date and time data. Functions to help you with this can be found in the \texttt{\{lubridate\}} package, with examples in the \href{https://r4ds.had.co.nz/dates-and-times.html\#dates-and-times}{dates and times} chapter of R for data science.

\hypertarget{be-aware-relational-data}{%
\section{Be Aware: Relational Data}\label{be-aware-relational-data}}

When the data you need are stored across two or more data frames you need to be able to cross-reference those and match up values for observational unit. This sort of data is know as relational data, and is used extensively in data science.

The variables you use to match observational units across data frames are known as \emph{keys}. The primary key belongs to the first table and the foreign key belongs to the secondary table. There are various ways to join these data frames, depending on if you want to retain.

\hypertarget{join-types}{%
\subsubsection{Join types}\label{join-types}}

You might want to keep only observational units that have key variables values in both data frames, this is known as an inner join.

\begin{figure}
\centering
\includegraphics{images/301-edav-wrangling/join-inner.png}
\caption{Inner join diagram. Source: R for Data Science}
\end{figure}

You might instead want to keep all units from the primary table but pad with NAs where there is not a corresponding foreign key in the second table. This results in an \textbf{(outer) left-join}.

\begin{figure}
\centering
\includegraphics{images/301-edav-wrangling/join-left-right-full.png}
\caption{Diagram for left, right and outer joins. Source: R for Data Science}
\end{figure}

Conversely, you might keep all units from the second table but pad with NAs where there is not a corresponding foreign key in the primary table. This is imaginatively named an \textbf{(outer) right-join}.

In the \textbf{(outer) full join}, all observational units from either table are retained and all missing values are padded with NAs.

Things get more complicated when keys don't uniquely identify observational units in either one or both of the tables. I'd recommend you start exploring these ideas with the \href{https://r4ds.had.co.nz/relational-data.html}{relational data} chapter of R for Data Science.

\hypertarget{why-and-where-to-learn-more}{%
\subsubsection{Why and where to learn more}\label{why-and-where-to-learn-more}}

Working with relational data is essential to getting any data science up and running out in the wilds of reality. This is because businesses and companies don't store all of their data in a huge single csv file. For one this isn't very efficient, because most cells would be empty. Secondly, it's not a very secure approach, since you can't grant partial access to the data. That's why information is usually stored in many data frames (more generically known as tables) within one or more databases.

These data silos are created, maintained, accessed and destroyed using a relational data base management system. These management systems use code to manage and access the stored data, just like we have seen in the dplyr commands above. You might well have heard of the SQL programming language (and its many variants), which is a popular language for data base management and is the inspiration for the dplyr package and verbs.

If you'd like to learn more then there are many excellent introductory SQL books and courses, I'd recommend picking one that focuses on data analysis or data science unless you really want to dig into efficient storage and querying of databases.

\hypertarget{wrapping-up-3}{%
\section{Wrapping up}\label{wrapping-up-3}}

We have:

\begin{itemize}
\item
  Learned how to wrangle tabular data in R with \texttt{\{dplyr\}}
\item
  Met the idea of relational data and \texttt{\{dplyr\}}'s relationship to SQL
\item
  Become aware of some tricky data types and packages that can help.
\end{itemize}

\hypertarget{edav-analysis}{%
\chapter{Exploratory Data Analysis}\label{edav-analysis}}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

Exploratory data analysis is an essential stage in any data science project. It allows you to become familiar with the data you are working with while also to identify potential strategies for progressing the project and flagging any areas of concern.

In this chapter we will look at three different perspectives on exploratory data analysis: its purpose for you as a data scientist, its purpose for the broader team working on the project and finally its purpose for the project itself.

\hypertarget{get-to-know-your-data}{%
\section{Get to know your data}\label{get-to-know-your-data}}

Let's first focus on an exploratory data analysis from our own point of view, as data scientists.

Exploratory data analysis (or EDA) is a process of examining a data set to understand its overall structure, contents, and the relationships between the variables it contains. EDA is an iterative process that's often done before building a model or making other data-driven decisions within a data science project.

\begin{quote}
Exploratory Data Analysis: quick and simple exerpts, summaries and plots to better understand a data set.
\end{quote}

One key aspect of EDA is generating quick and simple summaries and plots of the data. These plots and summary statistics can help to quickly understand the distribution of and relationships between the recorded variables. Additionally, during an exploratory analysis you will familiarise yourself with the structure of the data you're working with and how that data was collected.

\begin{figure}
\centering
\includegraphics{302-edav-analysis_files/figure-latex/unnamed-chunk-2-1.pdf}
\caption{\label{fig:unnamed-chunk-2}Investigating marginal and pairwise relationships in the Iris dataset.}
\end{figure}

Since EDA is an initial and iterative process, it's rare that any component of the analysis will be put into production. Instead, the goal is to get a general understanding of the data that can inform the next steps of the analysis.

In terms of workflow, this means that using one or more notebooks is often an effective way of organising your work during an exploratory analysis. This allows for rapid iteration and experimentation, while also providing a level of reproducibility and documentation. Since notebooks allow you to combine code, plots, tables and text in a single document, this makes it easy to share your initial findings with stakeholders and project managers.

\hypertarget{start-a-conversation}{%
\section{Start a conversation}\label{start-a-conversation}}

\begin{quote}
An effective EDA sets a precedent for open communication with the stakeholder and project manager.
\end{quote}

We've seen the benefits of an EDA for you as a data scientist, but this isn't the only perspective.

One key benefit of an EDA is that it can kick-start your communication with subject matter experts and project managers. You can build rapport and trust early in a project's life cycle by sharing your preliminary findings with these stakeholders . This can lead to a deeper understanding of both the available data and the problem being addressed for everyone involved. If done well, it also starts to build trust in your work before you even begin the modelling stage of a project.

\hypertarget{communicating-with-specialists}{%
\subsection{Communicating with specialists}\label{communicating-with-specialists}}

Sharing an exploratory analysis will inevitably require a time investment. The graphics, tables, and summaries you produce need to be presented to a higher standard and explained in a way that is clear to a non-specialist. However, this time investment will often pay dividends because of the additional contextual knowledge that the domain-expert can provide. They have a deep understanding of the business or technical domain surrounding the problem. This can provide important insights that aren't in the data itself, but which are vital to the project's success.

As an example, these stakeholder conversations often reveal important features in the data generating or measurement process that should be accounted for when modelling. These details are usually left out of the data documentation because they would be immediately obvious to any specialist in that field.

\hypertarget{communicating-with-project-manager}{%
\subsection{Communicating with project manager}\label{communicating-with-project-manager}}

An EDA can sometimes allow us to identify cases where the strength of signal within the available data is clearly insufficient to answer the question of interest. By clearly communicating this to the project manager, the project can be postponed while different, better quality or simply more data are collected. It's important to note that this data collection is not trivial and can have a high cost in terms of both time and capital. It might be that collecting the data needed to answer a question will cost more than we're likely to gain from knowing that answer. Whether the project is postponed or cancelled, this constitutes a successful outcome for the project, the aim is to to produce insight or profit - not to fit models for their own sake.

\hypertarget{scope-your-project}{%
\section{Scope Your Project}\label{scope-your-project}}

\begin{quote}
EDA is an initial assessment of whether the available data measure the correct values, in sufficient quality and quantity, to answer a particular question.
\end{quote}

A third view on EDA is as an initial assessment of whether the available data measure the correct values, with sufficient quality and quantity, to answer a particular question. In order for EDA to be successful, it's important to take a few key steps.

First, it's important to formulate a specific question of interest or line of investigation and agree on it with the stakeholder. By having a clear question in mind, it will be easier to focus the analysis and identify whether the data at hand can answer it.

Next, it's important to make a record (if one doesn't already exist) of how the data were collected, by whom it was collected, what each recorded variable represents and the units in which they are recorded. This meta-data is often known as a data sheet. Having this information in written form is crucial when adding a new collaborator to a project, so that they can understand the data generating and measurement processes, and are aware of the quality and accuracy of the recorded values.

\hypertarget{investigate-your-data}{%
\section{Investigate Your Data}\label{investigate-your-data}}

\begin{quote}
EDA is an opportunitiy to quantify data completeness and investigate the possibility of informative missingness.
\end{quote}

In addition, it's essential to investigate and document the structure, precision, completeness and quantity of data available. This includes assessing the degree of measurement noise or misclassification in the data, looking for clear linear or non-linear dependencies between any of the variables, and identifying if any data are missing or if there's any structure to this missingness. Other data features to be aware of are the presence of any censoring or whether some values tend to be missing together.

Furthermore, a more advanced EDA might include a simulation study to estimate the amount of data needed to detect the smallest meaningful effect. This is more in-depth than a typical EDA but if you suspect that the signals within your data are weak relative to measurement noise, can help to demonstrate the limitations of the current line of enquiry with the information that is currently available.

\hypertarget{what-is-not-eda}{%
\section{What is not EDA?}\label{what-is-not-eda}}

It's important to understand that an exploratory data analysis is not the same thing as modelling. In particular is \emph{not} the construction of your baseline model, which is sometimes called initial data analysis.

Though it might inform the choice of baseline model, EDA is usually not model based. Simple plots and summaries are used to identify patterns in the data that inform how you approach the rest of the project.

Some degree of statistical rigour can be added through the use of non-parametric techniques; methods like rolling averages, smoothing or partitioning can to help identify trends or patterns while making minimal assumptions about the data generating process.

\begin{figure}
\centering
\includegraphics{302-edav-analysis_files/figure-latex/unnamed-chunk-3-1.pdf}
\caption{\label{fig:unnamed-chunk-3}Daily change in Dow Jones Index with smoothed estimate of mean and 95\% confidence interval.}
\end{figure}

\begin{table}

\caption{\label{tab:unnamed-chunk-4}Mean and standard deviation of daily change in Dow Jones Index, before and after 1st of June 1998.}
\centering
\begin{tabular}[t]{l|r|r}
\hline
after\_june\_98 & mean & sd\\
\hline
FALSE & 5.916798 & 65.19093\\
\hline
TRUE & 3.972929 & 119.56067\\
\hline
\end{tabular}
\end{table}

Though the assumptions in an EDA are often minimal it can help to make them explicit. For example, in this plot a moving averages is shown with a confidence band, but the construction of this band makes the implicit assumption that, at least locally, our observations have the same distribution and so are exchangeable.

Finally, EDA is not a prescriptive process. While I have given a lot of suggestions on what you might usually want to consider, there is no correct way to go about an EDA because it is so heavily dependent on the particular dataset, its interpretation and the task you want to achieve with it. This is one of the parts of data science that make it a craft that you hone with experience, rather than an algorithmic process. When you work in a particular area for a long time you develop a knowledge for common data quirks in that area, which may or may not translate to other applications.

Now that we have a better idea of what is and what is not EDA, let's talk about the issue that an EDA tries to resolve and the other issues that it generates.

\hypertarget{issue-forking-paths}{%
\section{Issue: Forking Paths}\label{issue-forking-paths}}

In any data science project you have a sequence of very many decisions that you must make, each with many potential options and is difficult to decide upon \emph{a priori}.

\includegraphics{images/302-data-exploration/forking-paths.jpg}

Focusing in on only one small part of the process, we might consider picking a null or baseline model, which we will then try and improve on. Should that null model make constant predictions, incorporate a simple linear trend or is something more flexible obviously needed? Do you have the option to try all of these or are you working under time constraints? Are there contextual clues that rule some of these null models out on contextual grounds?

An EDA lets you narrow down your options by looking at your data and helps you to decide what might be reasonable modelling approaches.

\includegraphics{images/302-data-exploration/supervise_learning_schematic.png}

The problem that sneaks in here is data leakage. Formally this is where training data is included in test set, but this sort of information leak can happen informally too. Usually this is because you've seen the data you're trying to model or predict and then selected your modelling approach based on that information.

Standard, frequentist statistical methods for estimation and testing assume no ``peeking'' of this type has occurred. If we use these methods without acknowledging that we have already observed our data then we will artificially inflate the significance of our findings. For example, we might be comparing two models: the first of which makes constant predictions with regard to a predictor, while the second includes a linear trend. We will of course use a statistical test to confirm that what we are seeing is unlikely by chance. However, we must be aware this test was only performed because we had previously examined at the data and noticed what looked to be a trend.

Similar issues arise in Bayesian approaches, particularly when constructing or eliciting prior distributions for our model parameters. One nice thing that we can do in the Bayesian setting is to simulating data from the prior predictive distribution and then get an expert to check that these datasets seem seem reasonable. However, it is often the case this expert is also the person who collected the data we will soon be modelling. It's very difficult for them to ignore what they have seen, which leads to similar, subtle leakage problems.

\hypertarget{correction-methods}{%
\section{Correction Methods}\label{correction-methods}}

There are various methods or corrections that we can apply during our testing and estimation procedures to ensure that our error rates or confidence intervals account for our previous ``peeking'' during EDA.

Examples of these corrections have been developed across many fields of statistics. In medical statistics we have approaches like the Bonferroni correction, to account for carrying out multiple hypothesis tests. In the change-point literature there are techniques for estimating a change location given that a change has been detected somewhere in a time series. While in the extreme value literature there are methods to estimate the required level of protection against rare events, given that the analysis was triggered by the current protections having been compromised.

\includegraphics{302-edav-analysis_files/figure-latex/unnamed-chunk-5-1.pdf}
:::
::::

All of these corrections require us to make assumptions about the nature of the peeking. They are either very specific about the process that has occurred, or else are very pessimistic about how much information has been leaked. Developing such corrections to account for EDA isn't really possible, given its adaptive and non-prescriptive nature.

In addition to being either highly specific or pessimistic, these corrections can also be hard to derive and complicated to implement. This is why in settings where the power of tests or level of estimation is critically important, the entire analysis is pre-registered. In clinical trials, for example, every step of the analysis is specified before any data are collected. In data science this rigorous approach is rarely taken.

As statistically trained data scientists, it is important for us to remain humble about our potential findings and to suggest follow up studies to confirm the presence of any relationships we do find.

\hypertarget{learning-more}{%
\section{Learning More}\label{learning-more}}

In this chapter we have acknowledged that exploratory analyses are an important part of the data science workflow; this is true not only for us as data scientists, but also for the other people who are involved with our projects.

We've also seen that an exploratory analysis can help to guide the progression of our projects, but that in doing so we must take care to prevent and acknowledge the risk of data leakage.

If you want to explore this topic further, it can be quite challenging: examples of good, exploratory data analyses can be difficult to come by. This is because they are not often made publicly available in the same way that papers and technical reports are. Additionally, they are often kept out of public repositories because they are not as ``polished'' as the rest of the project. Personally, I think this is a shame and the culture on this is slowly changing.

For now, your best approach to learning about what makes a good exploratory analysis is to do lots of your own and to talk to you colleagues about their approaches.

There are lots of list-articles out there claiming to give you a comprehensive list of steps for any exploratory analysis. These can be good for inspiration, but I strongly suggest you don't treat these as gospel.

Despite the name of the chapter, \href{https://bookdown.org/rdpeng/exdata/exploratory-data-analysis-checklist.html\#follow-up-questions}{Roger Peng's EDA check-list} gives an excellent worked example of an exploratory analysis in R. The the discussion article ``\href{http://www.stat.columbia.edu/~gelman/research/published/p755.pdf}{Exploratory Data Analysis for Complex Models}'', Andrew Gelman makes a more abstract discussion of both exploratory analyses (which happen before modelling) and confirmatory analyses (which happen afterwards).

\hypertarget{edav-visualisation}{%
\chapter{Data Visualisation}\label{edav-visualisation}}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

\hypertarget{more-than-a-pretty-picture}{%
\subsection{More than a pretty picture}\label{more-than-a-pretty-picture}}

Data visualisation is an integral part of your work as a data scientist.

\begin{figure}
\centering
\includegraphics{images/303-data-visualisation/the-climate-book-jacket.jpg}
\caption{Warming stripes graphic on the cover of ``The Climate Book''}
\end{figure}

You'll use visualisations to rapidly explore new data sets, to understand their structure and to establish which types of model might be suitable for the task at hand. Visualisation is also vital during model evaluation and when check the validity of the assumptions on which that model is based. These are relatively technical uses of visualisation, but graphics have a much broader role within your work of an effective data scientist.

When well designed, plots, tables and animations can tell compelling stories that were once trapped within your data. They can also intuitively communicate the strength of evidence for your findings and draw attention to the most salient parts your argument.

Data visualisation is an amalgamation of science, statistics, graphic design and storytelling. It's multi-disciplinary nature means that we have to draw on all of our skills to ensure success. While there are certainly many ways to go wrong when visualising data, there are many more ways to get it right.

This chapter won't be a a step-by-step tutorial of how to visualise any type of data. Nor will it be a line-up of visualisations gone wrong. Instead, I hope to pose some questions that'll get you thinking critically about exactly what you want from each graphic that you produce.

There are five things that you should think about when producing any sort of data visualisation. We will consider each of these in turn.

\hypertarget{your-tools}{%
\section{Your Tools }\label{your-tools}}

\hypertarget{picking-the-right-tool-for-the-job}{%
\subsection{Picking the right tool for the job}\label{picking-the-right-tool-for-the-job}}

When you think of data visualisation, you might immediately think of impressive animations or complex, interactive dashboards that allow users to explore relationships within the data for themselves.

Such tools are no doubt impressive but they are by no means necessary for an effective data visualisation. In many cases there is no technology is needed at all. The history of data visualisation vastly pre-dates that of computers and some of the most effective visualisations remain analogue creations.

\begin{figure}
\includegraphics[width=1\linewidth]{images/303-data-visualisation/coffee-2} \caption{Coffee consumption, visualised. Jaime Serra Palou.}\label{fig:unnamed-chunk-2}
\end{figure}

This visualisation of a year's coffee consumption is an ideal example. Displaying the number of cups of coffee in a bar chart or line graph would have been a more accurate way to collect and display this data, but that wouldn't have the same resonance or impact and it certainly wouldn't have been as memorable.

\hypertarget{analogue-or-digital}{%
\subsection{Analogue or Digital}\label{analogue-or-digital}}

\hypertarget{analogue-data-viz}{%
\subsubsection{Analogue Data Viz}\label{analogue-data-viz}}

Here we have another example of an analogue data visualisation that is created as part of data collection. Each member of the department is invited to place a Lego brick on a grid to indicate how much caffeine they have consumed and how much sleep they have had. The beauty of using Lego bricks here is that they are stackable and so create a bar plot over two dimensions.

\begin{figure}
\includegraphics[width=1\linewidth]{images/303-data-visualisation/lego-coffee} \caption{Caffeination vs sleep, shown in lego. Elsie Lee-Robbins}\label{fig:unnamed-chunk-3}
\end{figure}

A third example can be found next to the tills in many supermarkets. Each customer is given a token as they pay for their goods. They can then drop this token into one of three large perspex containers as they leave the shop, each representing a different charity. At the end of the month 10,000 is split between the charities in proportion to the number of tokens. Because the containers are made from a transparent material you can see how the tokens are distributed, giving a visualisation of the level of support for each of the charities.

There are many other way of constructing a physical, analogue visualisation of your data and this doesn't need to be done as part of the data collection process. The simplest and perhaps most obvious most obvious is to create a plot of tabular data using a pen and paper.

\hypertarget{digital-data-viz}{%
\subsubsection{Digital Data Viz}\label{digital-data-viz}}

When it comes to digital tools for data visualisation you have a plethora of options. The most similar to pen-and-paper plotting is to draw your visualisations using a Photoshop, or an open source equivalent like Inkscape. The benefit here is that if you misplace a line or dot you can correct this small error without having to start all over again.

There are then more data-focused tools that have point-and-click interfaces. These are things like Excel's chart tools, or specialist visualisation software like Tableau. These are great because they scale with the quantity of data, so that you can plot larger amounts of raw data values that you wouldn't have the time or patience to do by hand - whether that's in an analogue or digital format.

Analogue and point-and-click approaches to visualisation have the shared limitation of not being reproducible, at least not without extensive documentation explaining how the graphic was created.

Using code to create your visualisations can resolve this reproducibility issue, and includes visualisation as a part of your larger, reproducible workflow for data science. Scripted visualisations also scale easily to large data sets and are easy to alter if any changes are required. The downside here is that there is a relatively steep learning curve to creating such visualisations, which is exactly what point-and-click methods are trying to avoid.

No matter how you produce your visualisations, the time cost of developing your skills in that medium is what buys you the ability to control and customise what you create. This upfront time investment will also often make you faster at producing future graphics in that medium.

Whenever you approach a new visualisation problem, you should pick your tools and medium judiciously. You have to balance your immediate needs for speed, accuracy and reproducibility against your current skill level and improving those skills in the medium to long term. Unfortunately, the only way to make good visualisations is to make lots of bad ones and even more mediocre ones first.

\hypertarget{ggplot2}{%
\subsection{ggplot2}\label{ggplot2}}

If your aim is to produce a wide range of high quality data visualisation using R, then the \texttt{\{ggplot2\}} package is one of the most versatile and well documented tools available to you.

The g's at the start of the package name stand for grammar of graphics. This is an opinionated, abstract approach to constructing data visualisations programmatically, by building them up slowly and adding additional plot elements one layer at a time.

\includegraphics{images/303-data-visualisation/grammar-of-graphics-abstract.png}

This idea of a ``grammar of Graphics'' was originally introduced by Leland Wilkinson. The paper shown by Hadley Wickham, and the associated \texttt{\{ggplot2\}} package popularised this approach within the R community. Like many of the tidyverse collection of packages, \texttt{\{ggplot2\}} provides simple but specialised modular functions that can be composed to create complex visualisations.

If you'd like to learn how to use \texttt{\{ggplot2\}}, I wouldn't recommend starting with the paper nor would I recommend trying to get started with the docs alone. Instead, I would suggest you work through an introductory tutorial, or one of the resources linked within the package documentation. Once you have a grasp of the basic principles the best way to improve is to try making your own plots, using reference texts and other people's work as a guide. A great source of inspiration here is the Tidy Tuesday data visualisation challenge. You can search for the challenge on Github to inspect both the plots made by other people and the code that was used to make them.

\textbf{Learning \texttt{ggplot2}:}

\begin{itemize}
\tightlist
\item
  \href{https://ggplot2.tidyverse.org/\#learning-ggplot2}{Resources}
\item
  \href{https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/}{Tutorial}
\item
  \href{https://github.com/search?p=1\&q=tidy+tuesday\&type=Repositories}{Tidy Tuesday Github}
\end{itemize}

\hypertarget{your-medium}{%
\section{Your Medium }\label{your-medium}}

\hypertarget{where-is-your-visualisation-going}{%
\subsection{Where is your visualisation going?}\label{where-is-your-visualisation-going}}

The second aspect that I recommend you think about before starting a data visualisation is where that graphic is going to be used. The intended location for your visualisation will influence both the composition of your graphic and also the amount of effort that you dedicate to it.

\includegraphics{images/303-data-visualisation/visualisation-purposes.png}

For example, consider making an exploratory plot at the start of a project to improve your own understanding of the structure within your data. In this case you do not need to spend much time worrying about refining axis labels, colour schemes or which in file format to save your work.

When working on figure that will be included in a daily stand-up meeting with your team, then you should take a little more care to ensure that your work can be clearly understood by others, for example that the legend and axis labels are both large and sufficiently informative.

Further refinement again will be required of this is for an external presentation. Is the message of the visualisation immediately clear? Will the graphic it still be clear when displayed in a boardroom or conference hall, or will it pixellate? Finally, how long will the audience have to interpret the visualisation while you are speaking? Even if slide decks are made available, very few audience members will actually refer to them before or after the presentation.

The opposing consideration has to be made when preparing a visualisation for a report or scientific paper. In this case plots and tables can be very small, particularly in two-column or grid layouts. You have to be wary about the legibility of your smallest text (think values on axes) and your visualisation can be clearly understood, whether the document is being read zoomed-in on a computer screen or printed out in black and white.

\hypertarget{file-types}{%
\subsection{File Types}\label{file-types}}

\begin{figure}
\centering
\includegraphics{images/303-data-visualisation/dalle-pixellated-image.png}
\caption{A low resoloution bitmap image.}
\end{figure}

To ensure that your graphics are suitable for the intended medium it is helpful to know a little bit about image file types.

There are two dominant types of image file: vector graphics and bitmap graphics.

\textbf{Bitmap graphics} store images as a grid of little squares and each of these pixels takes a single, solid colour. If you make a bitmap image large enough, either by zooming in or by using a really big screen, then these individual pixels become visible. Usually this isn't going to be your intention, so you need to ensure that the resolution of your graphic (its dimensions counted in pixels) is sufficiently large.

\textbf{Vector graphics} create images using continuous paths and then filling the areas that they enclose with flat colour. These vector images can be enlarged as much as you like without image quality becoming compromised. This is great for simple simple designs like logos, which have to be clear when used on both letterhead and billboards.

However, these vector graphics are more memory intensive than bitmap images, particularly when there are many distinct colours or objects within the image. This can be a particular problem in data science, for example when creating a scatter plot with many thousands of data points.

It can often be useful to save both a bitmap and vector version of your graphics. This way you can use bitmap when you need small files that load quickly (like when loading a webpage) and vector graphics when you need your visualisation to stay sharp when enlarged (like when creating a poster or giving a presentation in an auditorium).

\hypertarget{your-audience}{%
\section{Your Audience }\label{your-audience}}

\hypertarget{know-your-audience}{%
\subsection{Know Your Audience}\label{know-your-audience}}

Data visualisations are a tool for communicating information. To make this communication as effective as possible, you have to target your delivery to the intended audience.

\begin{itemize}
\item
  Who is the intended audience for your visualisation?
\item
  What knowledge do they bring with them?
\item
  What assumptions and biases do they hold?
\end{itemize}

Creating \emph{personas} for distinct user groups can be a helpful way to answer these questions, particularly when the user population are heterogeneous.

\includegraphics{images/303-data-visualisation/user-groups.png}

To know \emph{how} to target your delivery to a particular audience, you fist have to identify exactly who that is.

To make a compelling data visualisation you have to have some idea of the background knowledge that the viewer brings. Are they a specialist in statistics or data science, or does their expertise lie in area application? Are the findings that you're presenting going to come as a surprise to them, or act confirmation of their pre-existing body of knowledge.

It's worth considering these prior beliefs and how strongly they are held when constructing your visualisation. Take the time to consider how this existing knowledge could alter or influence their interpretation of what you're showing to them. On the flip-side, you might be presenting information on a topic that the viewer is the best case ambivalent about or in the worst case is actively bored by. In that case, you can take special care to compose engaging visualisations to capture and hold the attention of the audience.

\hypertarget{preattentive-attributes}{%
\subsection{Preattentive Attributes}\label{preattentive-attributes}}

When crafting a visualisation we want to require as little work as possible from the viewer.
To do this, we can use pre-attentive attributes, such as colour, shape size and position to encode our data values.

\begin{figure}
\includegraphics[width=1\linewidth]{images/303-data-visualisation/preattentive-attributes} \caption{Examples of preattentive attributes}\label{fig:unnamed-chunk-4}
\end{figure}

These preattentive attributes are properties of lines and shapes that provide immediate visual contrast without requiring active thought from the viewer. As we will see, care needs to be taken here to ensure that we are don't mislead the viewer with how we use these attributes.

\hypertarget{example-first-impressions-count}{%
\subsection{Example: First Impressions Count}\label{example-first-impressions-count}}

\begin{figure}
\centering
\includegraphics{images/303-data-visualisation/average-male-height.jpg}
\caption{Issues with scales, area and perspective}
\end{figure}

This figure presents a bar chart of the mean height of males in several countries, but has swapped out the bars for human outlines. While the visualisation has an attractive, minimal design and a pleasant colour scheme, it doesn't do a good job of immediately conveying the relevant information to the viewer.

The three main issues with this plot are all caused by swapping the bars of this plot for male silhouettes, and are linked to the difference in how lengths and areas are perceived by humans. Typically, we make immediate pre-attentive comparisons based on area but draw more accurate, considered comparisons when comparing lengths.

By replacing bars with human outlines and not starting the height scale at zero, this plot breaks the proportionality of length and area that is inherent in a bar plot. This causes dissonance between immediate and considered interpretation of this plot. An additional issue is that the silhouettes overlap, creating a forced perspective that makes it seem like the outlines are also further back and therefore even larger if this perspective is taken into account.

These three issues are important to consider when constructing your own visualisations. Are you showing all the values that the data could take, or focusing on a smaller interval to provide better contrast? If you are using the size of a circle to represent a value, are you changing the diameter or area in proportion to the data value? And finally, if you are making a plot that appears three-dimensional, have you done so on purpose and, if so, can one of those dimensions be better represented by an attribute that isn't position?

\hypertarget{visual-perception}{%
\subsection{Visual Perception}\label{visual-perception}}

When reducing the dimensionality of your plot you may wish to represent a data value using colour rather than position. When deciding on how to use colour, you should keep your audience in mind.

Is your aim to two or more categories? In that case, you'll need to select your finite set of colours and ensure that these can be distinguished.

Are you are representing a data value that is continuous or has an associated ordering? Then you will again have to select your palette to provide sufficient contrast to all viewers of your work.

If you are representing a measurement that has a \emph{reference value} (for example 0 for temperature in centigrade) then a diverging colour palette can be used to represent data that are above or below this reference point. This requires some cultural understanding of how the colours will be interpreted, for example you are likely to cause confusion if you an encoding of red for cold and blue for hot.

For colour scales without such a reference point then a gradient in a single colour is likely the best option. In either case, it is important to check that a unit change in data value represents a consistent change in colour across all values. This is not the case for the rainbow palette here (which is neither a single gradient or diverging).

\begin{figure}
\includegraphics[width=1\linewidth]{images/303-data-visualisation/saturated-colour-scales} \caption{Some default colour scales in R}\label{fig:unnamed-chunk-5}
\end{figure}

To ensure accessibility of your designs, I would recommend one of the many on-line tools to simulate colour vision deficiency or using a pre-made palette where this has been considered for you. A good, low-tech rule of thumb is to design your visualisations so that they're still easily understood when printed in grey-scale. This can mean picking appropriate colours or additionally varying the point shape, line width or line types used.

\begin{figure}
\includegraphics[width=1\linewidth]{images/303-data-visualisation/desaturated-colour-scales} \caption{Desatureated colour scales in R}\label{fig:unnamed-chunk-6}
\end{figure}

For a practical guide on setting colours see this \href{https://bookdown.org/rdpeng/exdata/plotting-and-color-in-r.html}{chapter} of exploratory data analysis by Roger Peng.

\hypertarget{alt-text-titles-and-captions}{%
\subsection{Alt-text, Titles and Captions}\label{alt-text-titles-and-captions}}

\begin{quote}
\textbf{Captions} describe a figure or table so that it may be identified in a list of figures and (where appropriate).

\textbf{Alternative text} describes the content of an image for a person who cannot view it. (\href{https://myaccessible.website/infographic/images/images-accessibility-alt-text}{Guide to writing alt-text})

\textbf{Titles} give additional context or identify key findings. Active titles are preferable.
\end{quote}

When visualisations are included in a report, article or website, they are often accompanied by three pieces of text. The title, the caption and the alt-text all help the audience to understand a visualisation but each serves a distinct purpose.

\hypertarget{captions}{%
\subsubsection{Captions}\label{captions}}

A caption is short description of a visualisation. Captions usually displayed directly above or below the figure or table that they describe. These captions serve two purposes: in a report, the caption can be used to look up the visualisation from a list of figures or tables. The second purpose of a caption is to add additional detail that you don't want to add to the plot directly. For example caption might be ``Time series of GDP in the United States of America, 2017-2022. Lines show daily (solid), monthly (dashed) and five-year (dotted) mean values.''

\hypertarget{alt-text}{%
\subsubsection{Alt-text}\label{alt-text}}

Alt text or alternative text is used to describe the content of an image to a person who can't view it. This text is helpful for people with a visual impairment, particularly those who uses a screen reader. Screen reading software reads digital text out loud but can't interpret images. Such software replaces the image with the provided alternative text. Alt text is also valuable in cases when the image can't be found or loaded, for example because of an incorrect file path or a slow internet connection, because it'll be displayed in place of the image.

The purpose of alt-text is different from a caption. It's designed as a replacement for the image, not just a shorthand or to provide additional information. If there is an important trend or conclusion to be drawn from the visualisation (that is not already mentioned in the main text) this should be identified in the alt-text. This sort of interpretation is a key aspect of alt-text that shouldn't be included in a caption.

\hypertarget{titles}{%
\subsubsection{Titles}\label{titles}}

Titles give additional context that is not conveyed by the axis labels or chart annotations. Alternatively the title can be used like a newspaper headline to deliver the key findings of the visualisation. One example of this might be when looking at a visualisation that is composed of many smaller plots, each showing the GDP for a US state over the last five years. Title for each smaller plot would identify the state it is describing, while the overall title might be something like ``All US states have increased GDP in the period 2017-2022''.

If you are including this type of interpretive title, make sure that the same interpretation is clear in the alt-text.

\hypertarget{your-story}{%
\section{Your Story }\label{your-story}}

The fourth aspect of a successful data visualisation is that it must tell a story. This story doesn't need to be a multi-generational novel or even a captivating novella. If a picture speaks a thousand words, really what your aiming for is an engaging anecdote.

Your visualisation should be something the grabs viewers attention and through its through its composition or content alters their knowledge or view of world in some way.

Telling effective stories requires planning. How you construct your narrative depends on what effect you want to have on your audience. I'd encourage you to think like a data journalist and go into your work with the intended effect clear in your mind. Is your purpose to inform them of a fact, to persuade them to use a different methodology or entertain them by presenting dull or commonplace data in a fresh and engaging way?

\begin{figure}
\centering
\includegraphics{images/303-data-visualisation/entertain-persuade-inform.png}
\caption{Three goals of data visualisation: to entertain, persuade and perform}
\end{figure}

In reality your goal will be some mixture of these, at an interior point of this triangle. Clearly identifying this point will help you to present your visual story in a way that works towards your aims, rather than against them.

On the point of presentation, it is important to realise that there is no neutral way to present information. In creating a visualisation you're choosing which aspects of the data to emphasise, what gets summarised and what is not presented at all. This is how you construct a plot that tells a clear and coherent story. However, there is more than one story that you could tell from a single dataset.

As an example of this, let's consider a time-series showing the price of two stocks and in particular the choice of scale on the y-axis. Suppose the two stock have values fluctuating around \$100 per share. Choosing a scale that goes from \$90 to \$110 would emphasise the differences between the two stocks. Setting the lower limit to \$0 would instead emphasise that these variations are small relative to the overall value of the stocks. Both are valid approaches but tell different stories. Be clear and be open about which of these you are telling and why you have chosen that over the alternative.

A final cross-over from data journalism is that your visualisations will be competing for your viewers attention. You have to compete against everything else that is going on in their lives. Establish a clear ``hook'' within your visualisation to attract your viewer's attention and immediately deliver the core message. This might be done with a contrasting trend-line or an intriguing title. Lead their attention first to the key message and then the supporting evidence.

\hypertarget{your-guidelines}{%
\section{Your Guidelines }\label{your-guidelines}}

\hypertarget{standardise-and-document}{%
\subsection{Standardise and Document}\label{standardise-and-document}}

The final consideration when creating visualisations is to reduce the number of considerations that you have to make in the future. This is done by thinking carefully about each of the decisions that you make and writing guidelines so that you make these choices consistently.

The choices that go into making an effective data visualisation are important and deserve careful consideration. However, this consideration comes at a cost. To the employer this is the literal, financial cost of paying for your time. More broadly this is the opportunity cost of all the other things that you could have been doing instead.

To be efficient in our visualisation design, we should extend our DRY coding principles the design processes. Make choices carefully and document your decisions to externalise the cognitive work required of you in the future.

Many companies aware of these financial and opportunity costs and provide style guides for visualisations in a similar manner to a coding or writing style guide. This not only externalises and formalises many decisions, but it also leads to a more uniform style across visualisations and the data scientists producing them. This leads to a unified, house-style for graphic design and a visual brand that is easily identifiable. This is beneficial for large companies or personal projects alike.

\hypertarget{example-style-guides}{%
\subsection{Example Style Guides}\label{example-style-guides}}

I'd highly recommend exploring some visualisation guides to get an idea of how these are constructed and how you might develop your own.

Unsurprisingly some of the best guides come from media outlets and government agencies. These businesses are used to writing style guides for text to create and maintain a distinctive style across all of their writers.

\begin{itemize}
\item
  BBC

  \begin{itemize}
  \tightlist
  \item
    \href{https://www.bbc.co.uk/gel/features/how-to-design-infographics}{Infographics Guidelines}
  \item
    \href{https://bbc.github.io/rcookbook/}{R Cookbook}
  \item
    \href{https://github.com/bbc/bbplot}{\texttt{\{bbplot\}}}
  \end{itemize}
\item
  \href{https://design-system.economist.com/documents/CHARTstyleguide_20170505.pdf}{The Economist}
\item
  \href{https://style.ons.gov.uk/category/data-visualisation/}{The Office for National Statistics}
\item
  \href{https://ec.europa.eu/eurostat/web/products-eurostat-news/-/STYLE-GUIDE_2016}{Eurostat}
\item
  \href{https://urbaninstitute.github.io/graphics-styleguide/}{Urban Institute}
\item
  \href{https://pudding.cool/resources/}{The Pudding} (learning resources)
\end{itemize}

The level of detail and technicality varies wildly between these examples. For instance, the BBC do not provide strong guidelines on the details of the final visualisation but do provide a lot of technical tools and advice on how to construct those in a consistent way across the corporation. They've even gone so far as to write their own theme for ggplot and to publish this as an R package!

\hypertarget{wrapping-up-4}{%
\section{Wrapping Up}\label{wrapping-up-4}}

 Think about your \emph{tools}.

 Think about your \emph{medium}.

 Think about your \emph{audience}.

 Think about your \emph{story}.

 Think about your \emph{guidelines}.

Data visualisation might seem like a soft skill in comparison to data acquisition, wrangling or modelling. However, it is often effective visualisations that have the greatest real world impact.

It is regularly the highly effective figures within reports and presentations that determine which projects are funded or renewed. Similarly, visualisations in press releases can determine whether the result of your study are trusted, correctly interpreted, and remembered by the wider public.

When constructing visualisations it is important to consider whether there are existing guidelines that provide helpful constraints to your work. From there, determine the story that you wish to tell and exactly who it is that your are telling that story to. Once this is decided you can select the medium and the tools that you use to craft your visualisation so that you have the greatest chance of achieving your intended effect.

\hypertarget{edav-checklist}{%
\chapter*{Checklist}\label{edav-checklist}}
\addcontentsline{toc}{chapter}{Checklist}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{videos-chapters-2}{%
\section{Videos / Chapters}\label{videos-chapters-2}}

\begin{itemize}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b25a6d35-1e91-4090-b4f8-af9600b39086}{Data Wrangling} (20 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/03-01-data-wrangling/03-01-data-wrangling.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=a986f50c-90fe-4379-b4a1-af9600ba22fd}{Data Exploration} (25 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/03-02-eda/03-02-data-exploration.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=2ebbfa94-0b5d-45e9-b708-af8100d9664a}{Data Visualisation} (27 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/03-03-data-visualisation/03-03-data-visualisation.pdf}{{[}slides{]}}
\end{itemize}

\hypertarget{reading-2}{%
\section{Reading}\label{reading-2}}

Use the \protect\hyperlink{edav-reading}{Data Exploration and Visualisation} section of the reading list to support and guide your exploration of this week's topics. Note that these texts are divided into core reading, reference materials and materials of interest.

\hypertarget{activities}{%
\section{Activities}\label{activities}}

\emph{Core:}

\begin{itemize}
\item[$\square$]
  \href{https://normconf.com/}{NormConf} is a conference dedicated to the unglamorous but essential aspects of working in the data sciences. The in December 2022 conference talks are available as a \href{https://www.youtube.com/@normconf/videos}{Youtube Playlist}. Find a talk that interests you and watch it, then post a short summary to EdStem, which includes what you learned from the talk and one thing that you still do not understand.
\item[$\square$]
  Work through this ggplot2 tutorial for \href{https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/\#text}{beautiful plotting in R} by Cdric Scherer, recreating the examples for yourself.
\item[$\square$]
  Using your \texttt{rolling\_mean()} function as inspiration, write a \texttt{rolling\_sd()} function that calculates the rolling standard deviation of a numeric vector.

  \begin{itemize}
  \item[$\square$]
    Extend your \texttt{rolling\_sd()} function to optionally return approximate point-wise confidence bands for your rolling standard deviations. These should be \(\pm2\) standard errors by default and may be computed using analytical or re-sampling methods.
  \item[$\square$]
    Create a visualisation using your extended \texttt{rolling\_sd()} function to assess whether the variability in the daily change in Dow Jones Index is changing over time. \href{data/dowjones.csv}{{[}data{]}}
  \end{itemize}
\end{itemize}

\emph{Bonus:}

\begin{itemize}
\item[$\square$]
  Add your \texttt{rolling\_sd()} function to your R package, adding documentation and tests.
\item[$\square$]
  During an exploratory analysis, we often need to assess the validity of an assumed distribution based on a sample of data. Write your own versions of \texttt{qqnorm()} and \texttt{qqplot()}, which add point-wise tolerance intervals to assess whether deviation from the line \(y=x\) are larger than expected.
\item[$\square$]
  Add your own versions of \texttt{qqnorm()} and \texttt{qqplot()} to your R package, along with documentation and tests.
\end{itemize}

\hypertarget{live-session-2}{%
\section{Live Session}\label{live-session-2}}

In the live session we will begin with a discussion of this week's tasks. We will then break into small groups for two data visualisation exercises.

(\textbf{Note:} For one of these exercises, it would be helpful to bring a small selection of coloured pens or pencils, of you have access to some. If not, please don't worry - inventive use of black, blue and shading are perfectly acceptable alternatives!)

Please come to the live session prepared to discuss the following points:

\begin{itemize}
\item
  Which NormConf video did you watch and what did you learn from it?
\item
  Other than \texttt{\{ggplot2\}}, what else have you used to create data visualisations? What are their relative strengths and weaknesses?
\item
  How did you implement your \texttt{rolling\_sd()} function and what conclusions did you draw when applying it to the Dow Jones data?
\end{itemize}

\hypertarget{part-preparing-for-production}{%
\part{Preparing for Production}\label{part-preparing-for-production}}

\hypertarget{production-introduction}{%
\chapter*{Introduction}\label{production-introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{production-reproducibility}{%
\chapter{Reproducibility}\label{production-reproducibility}}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{reproducibility-and-replicability}{%
\section{Reproducibility and Replicability -----------------------------}\label{reproducibility-and-replicability}}

Write chapter following structure of this article

\begin{itemize}
\item
  \url{https://www.kdnuggets.com/2019/11/reproducibility-replicability-data-science.html}
\item
  The Ethical Algorithm M Kearns and A Roth (Chapter 4) \url{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591\&context=L\&vid=44IMP_INST:ICL_VU1\&lang=en\&search_scope=MyInst_and_CI\&adaptor=Local\%20Search\%20Engine\&tab=Everything\&query=any,contains,kearns\%20and\%20roth\&mode=Basic}
\item
  The ASA Statement on \(p\)-values: Context, Process and Purpose \url{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1080_00031305_2016_1154108\&context=PC\&vid=44IMP_INST:ICL_VU1\&lang=en\&search_scope=MyInst_and_CI\&adaptor=Primo\%20Central\&tab=Everything\&query=any,contains,ASA\%20p-value\&offset=0}
\item
  The garden of forking paths: Why multiple comparisons can be a problem,
  even when there is no ``Fishing expedition'' or ``p-hacking'' and the research
  hypothesis was posited ahead of time. A Gelman and E loken (2013) \url{http://stat.columbia.edu/~gelman/research/unpublished/forking.pdf}
\item
  \url{https://docker-curriculum.com/} by Prakhar Srivastav
\end{itemize}

\hypertarget{production-explainability}{%
\chapter{Explainability}\label{production-explainability}}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{explainability--}{%
\section{Explainability -------------------------------------------------}\label{explainability--}}

\begin{itemize}
\item
  LIME paper on ArXiV: \url{https://arxiv.org/abs/1602.04938}. Ribeiro et al (2016) ``Why Should I Trust You?'': Explaining the Predictions of Any Classifier.
\item
  LIME pacakge documentation on CRAN \url{https://cran.r-project.org/web/packages/lime/index.html}
\item
  Understanding LIME tutorial - T Pedersen and M Benesty
\end{itemize}

\url{https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html}

\begin{itemize}
\tightlist
\item
  Reference: Interpretable Machine Learning: A Guide for Making Black Box Models Explainable by Christoph Molnar
\end{itemize}

\hypertarget{production-scalability}{%
\chapter{Scalability}\label{production-scalability}}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{scalability}{%
\section{Scalability --------------------------------------------------}\label{scalability}}

Structure talk around Alex's parallelisation slides

\begin{itemize}
\item
  Code profiling
\item
  vectorise
\item
  parallelise
\item
  Use a better language: C++, Python
\item
  Documentation for
\item
  apply
\item
  purr::map
\item
  furr::pmap
\item
  Advanced R (Second Edition) by Hadley Wickham. Chapters 23 and 24, measuring and improving performance
\item
  Advanced R (Second Edition) by Hadley Wickham. Chapter 25, measuring and improving performance
\end{itemize}

\hypertarget{production-checklist}{%
\chapter*{Checklist}\label{production-checklist}}
\addcontentsline{toc}{chapter}{Checklist}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{videos-chapters-3}{%
\section{Videos / Chapters}\label{videos-chapters-3}}

\begin{itemize}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=f48d43b4-b370-4cfb-a438-af9e00bf79b5}{Reproducibility} (26 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/04-01-reproducibility/04-01-reproducibility.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=f2c64757-faea-470f-9dfc-af9e00ba4929}{Explainability} (16 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/04-02-explainability/04-02-explainability.pdf}{{[}slides{]}}
\item[$\square$]
  \href{https://imperial.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=5305fbb1-8dc9-4232-82d0-afa00187f942}{Scalability} (30 min) \href{https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/04-03-scalability/04-03-scalability.pdf}{{[}slides{]}}
\end{itemize}

\hypertarget{reading-3}{%
\section{Reading}\label{reading-3}}

Use the \protect\hyperlink{production-reading}{Preparing for Production} section of the reading list to support and guide your exploration of this week's topics. Note that these texts are divided into core reading, reference materials and materials of interest.

\hypertarget{activities-1}{%
\section{Activities}\label{activities-1}}

This week has fewer activities, since you will be working on the first assessment.

\emph{Core}

\begin{itemize}
\item
  Read the LIME paper, which we will discuss during the live session.
\item
  Work through the \href{https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html}{understanding LIME R tutorial}
\item
  Use code profiling tools to assess the performance of your \texttt{rolling\_mean()} and \texttt{rolling\_sd()} functions. Identify any efficiencies that can be made.
\end{itemize}

\emph{Bonus:}

\begin{itemize}
\tightlist
\item
  Write two functions to simulate a \href{https://en.wikipedia.org/wiki/Poisson_point_process\#Homogeneous_case_2}{homogeneous Poisson process} with intensity \(\lambda >0\) on the interval \((t_1, t_2) \subset \mathbb{R}\). The first should use the exponential distribution of inter-event times to simulate events in sequence. The second should use the Poisson distribution of the total event count to first simulate the number of events and then randomly allocate locations over the interval. Evaluate and compare the reproducibility and scalability of each implementations.
\end{itemize}

\hypertarget{live-session-3}{%
\section{Live Session}\label{live-session-3}}

In the live session we will begin with a discussion of this week's tasks. We will then break into small groups for a reading group style discussion of the LIME paper that was set as reading for this week.

\hypertarget{part-data-science-ethics}{%
\chapter{(PART) Data Science Ethics}\label{part-data-science-ethics}}

\hypertarget{ethics-introduction}{%
\chapter*{Introduction}\label{ethics-introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{ethics-privacy}{%
\chapter{Privacy}\label{ethics-privacy}}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{ethics-fairness}{%
\chapter{Fairness}\label{ethics-fairness}}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{ethics-conduct}{%
\chapter{Codes of Conduct}\label{ethics-conduct}}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{ethics-checklist}{%
\chapter*{Checklist}\label{ethics-checklist}}
\addcontentsline{toc}{chapter}{Checklist}

Effective Data Science is still a work-in-progress. This chapter is currently a dumping ground for ideas, and we don't recommend reading it.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

\hypertarget{videos-chapters-4}{%
\section{Videos / Chapters}\label{videos-chapters-4}}

\begin{itemize}
\item
  {[} {]}
\item
  {[} {]}
\item
  {[} {]}
\end{itemize}

\hypertarget{reading-4}{%
\section{Reading}\label{reading-4}}

\hypertarget{activities-2}{%
\section{Activities}\label{activities-2}}

\hypertarget{live-session-4}{%
\section{Live Session}\label{live-session-4}}

\hypertarget{reading-list}{%
\chapter{Reading List}\label{reading-list}}

Effective Data Science is still a work-in-progress. This chapter is largely complete and just needs final proof reading.

If you would like to contribute to the development of EDS, you may do so at \url{https://github.com/zakvarty/data_science_notes}.

This reading list is organised by topic, according to each week of the course. These are split into several categories.

\begin{itemize}
\item
  \textbf{Core Materials:} These form a core part of the course activities.
\item
  \textbf{Reference Materials:} These will be used extensively in the course, but should be seen as helpful guides, rather than required reading from cover to cover.
\item
  \textbf{Materials of Interest:} These will not form a core part of the course, but will give you a deeper understanding or interesting perspective on the weekly topic. There might be some fun other stuff in here too.
\end{itemize}

\hypertarget{workflows-reading}{%
\section{Effective Data Science Workflows}\label{workflows-reading}}

\hypertarget{core-materials}{%
\subsection*{Core Materials}\label{core-materials}}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\tightlist
\item
  The \href{https://style.tidyverse.org/}{Tidyverse R Style Guide} by Hadley Wickham.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510\&ref=https://githubhelp.com}{Wilson, et al (2017)}. Good Enough Practices in Scientific Computing. PLOS Computational Biology.
\end{itemize}

\hypertarget{reference-materials}{%
\subsection*{Reference Materials}\label{reference-materials}}
\addcontentsline{toc}{subsection}{Reference Materials}

\begin{itemize}
\item
  \href{https://r4ds.had.co.nz/index.html}{R For Data Science Chapters 2, 6 and 8} by Hadley Wickham and Garrett Grolemund. Chapters covering R workflow basics, a scripting and project based workflow.
\item
  \href{https://here.r-lib.org/articles/here.html}{Documentation} for the \{here\} package
\item
  \href{https://r-pkgs.org/}{R Packages Book} (Second Edition) by Hadley Wickham and Jenny Bryan.
\end{itemize}

\hypertarget{materials-of-interest}{%
\subsection*{Materials of Interest}\label{materials-of-interest}}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\tightlist
\item
  \href{https://stat545.com/index.html}{STAT545, Part 1} by Jennifer Bryan and The STAT 545 TAs
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://rstats.wtf/}{What they forgot to teach you about R, Chapters 2-4} by Jennifer Bryan and Jim Hester.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://www.amstat.org/docs/default-source/amstat-documents/pol-reproducibleresearchrecommendations.pdf}{Broman et al (2017)}. Recommendations to Funding Agencies for Supporting Reproducible Research. American Statistical Association.
\end{itemize}

\begin{itemize}
\item
  \href{https://adv-r.hadley.nz/}{Advanced R} by Hadley Wickham Section introductions on \href{https://adv-r.hadley.nz/fp.html}{functional} and \href{https://adv-r.hadley.nz/oo.html}{object oriented} approaches to programming.
\item
  \href{https://www.atlassian.com/agile/project-management}{Atlassian Article} on Agile Project Management
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/}{The Pragmatic Programmer, 20th Anniversary Edition Edition} by David Thomas and Andrew Hunt. The section on \href{https://media.pragprog.com/titles/tpp20/dry.pdf}{DRY coding} and a few others are freely available.
\end{itemize}

\begin{itemize}
\item
  \href{https://csgillespie.github.io/efficientR/}{Efficient R programming} by Colin Gillespie and Robin Lovelace. Chapter 5 considers \href{https://csgillespie.github.io/efficientR/input-output.html}{Efficient Input/Output} is relevant to this week. Chapter 4 on \href{https://csgillespie.github.io/efficientR/workflow.html}{Efficient Workflows} links nicely with last week's topics.
\item
  \href{https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html\#References}{Towards A Principled Bayesian Workflow} by Michael Betancourt.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://happygitwithr.com/}{Happy Git and GitHub for the useR} by Jennifer Bryan
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{data-reading}{%
\section{Aquiring and Sharing Data}\label{data-reading}}

\hypertarget{core-materials-1}{%
\subsection*{Core Materials}\label{core-materials-1}}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\item
  \href{https://r4ds.had.co.nz/tidy-data.html}{R for Data Science Chapters 9 - 12} by Hadley Wickham. These chapters introduce tibbles as a data structure, how to import data into R and how to wrangle that data into tidy format.
\item
  \href{https://csgillespie.github.io/efficientR/}{Efficient R programming} by Colin Gillespie and Robin Lovelace. Chapter 5 considers \href{https://csgillespie.github.io/efficientR/input-output.html}{Efficient Input/Output} is relevant to this week.
\item
  \href{https://vita.had.co.nz/papers/tidy-data.html}{Wickham (2014)}. Tidy Data. Journal of Statistical Software. The paper that brought tidy data to the mainstream.
\end{itemize}

\hypertarget{reference-materials-1}{%
\subsection*{Reference Materials}\label{reference-materials-1}}
\addcontentsline{toc}{subsection}{Reference Materials}

\begin{itemize}
\item
  The \{readr\} \href{https://readr.tidyverse.org/}{documentation}
\item
  The \{data.table\} \href{https://cran.r-project.org/web/packages/data.table/data.table.pdf}{documentation} and \href{https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html}{vignette}
\item
  The \{rvest\} \href{https://rvest.tidyverse.org/}{documentation}
\item
  The \{tidyr\} \href{https://tidyr.tidyverse.org/}{documentation}
\item
  MDN Web Docs on \href{https://developer.mozilla.org/en-US/docs/Web/HTML}{HTML} and \href{https://developer.mozilla.org/en-US/docs/Web/CSS}{CSS}
\end{itemize}

\hypertarget{materials-of-interest-1}{%
\subsection*{Materials of Interest}\label{materials-of-interest-1}}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\tightlist
\item
  \href{https://zapier.com/learn/apis/chapter-1-introduction-to-apis/}{Introduction to APIs} by Brian Cooksey
\item
  \href{https://r4ds.hadley.nz/}{R for Data Science (Second Edition)} Chapters within the \href{https://r4ds.hadley.nz/import.html}{Import} section.
\end{itemize}

This covers importing data from spreadsheets, databases, using Apache Arrow and importing hierarchical data as well as web scraping.

\hypertarget{edav-reading}{%
\section{Data Exploration and Visualisation}\label{edav-reading}}

\hypertarget{core-materials-2}{%
\subsection*{Core Materials}\label{core-materials-2}}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\tightlist
\item
  \href{https://bookdown.org/rdpeng/exdata/}{Exploratory Data Analysis with R} by Roger Peng.
\end{itemize}

Chapters 3 and 4 are core reading, respectively introducing \href{https://bookdown.org/rdpeng/exdata/managing-data-frames-with-the-dplyr-package.html}{data frame manipulation with \{dplyr\}} and an example \href{https://bookdown.org/rdpeng/exdata/exploratory-data-analysis-checklist.html}{workflow for exploratory data analysis}. Other chapters may be useful as references.

\begin{itemize}
\tightlist
\item
  \href{https://stefvanbuuren.name/fimd/}{Flexible Imputation of Missing Data} by Stef van Buuren. \href{https://stefvanbuuren.name/fimd/ch-introduction.html}{Sections 1.1-1.4} give a thorough introduction to missing data problems.
\end{itemize}

\hypertarget{referene-materials}{%
\subsection*{Referene Materials}\label{referene-materials}}
\addcontentsline{toc}{subsection}{Referene Materials}

\begin{itemize}
\item
  \href{}{A ggplot2 Tutorial for Beautiful Plotting in R} \url{https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/}) by Cdric Scherer.
\item
  The \{dplyr\} \href{https://dplyr.tidyverse.org/}{documentation}
\item
  \href{https://github.com/rstudio/cheatsheets/blob/main/data-transformation.pdf}{RStudio Data Transformation Cheat Sheet}
\item
  \href{https://r4ds.had.co.nz/index.html}{R for Data Science (First Edition)} Chapters on \href{https://r4ds.had.co.nz/transform.html}{Data Transformations}, \href{https://r4ds.had.co.nz/exploratory-data-analysis.html}{Exploratory Data Analysis} and \href{https://r4ds.had.co.nz/relational-data.html}{Relational Data}.
\item
  Equivalent sections in R for Data Science \href{https://r4ds.hadley.nz/}{Second Edition}
\end{itemize}

\hypertarget{materials-of-interest-2}{%
\subsection*{Materials of Interest}\label{materials-of-interest-2}}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1198_jcgs_2009_07098\&context=PC\&vid=44IMP_INST:ICL_VU1\&lang=en\&search_scope=MyInst_and_CI\&adaptor=Primo\%20Central\&tab=Everything\&query=any,contains,layered\%20grammar\%20of\%20graphics\&offset=0}{Wickham, H. (2010)}. A Layered Grammar of Graphics. Journal of Computational and Graphical Statistics.
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000664639501591\&context=L\&vid=44IMP_INST:ICL_VU1\&lang=en\&search_scope=MyInst_and_CI\&adaptor=Local\%20Search\%20Engine\&tab=Everything\&query=any,contains,better\%20data\%20visualisations\&offset=0}{Better Data Visualisations} by Jonathan Schwabish
\end{itemize}

\begin{itemize}
\tightlist
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000211295101591\&context=L\&vid=44IMP_INST:ICL_VU1\&lang=en\&search_scope=MyInst_and_CI\&adaptor=Local\%20Search\%20Engine\&tab=Everything\&query=any,contains,Data\%20Visualization\%20\%E2\%80\%93\%20A\%20Practical\%20Introduction\&offset=0}{Data Visualization: A Practical Introduction} by Kieran Healy
\end{itemize}

\hypertarget{production-reading}{%
\section{Preparing for Production}\label{production-reading}}

\hypertarget{core-materials-3}{%
\subsection*{Core Materials}\label{core-materials-3}}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591\&context=L\&vid=44IMP_INST:ICL_VU1\&lang=en\&search_scope=MyInst_and_CI\&adaptor=Local\%20Search\%20Engine\&tab=Everything\&query=any,contains,kearns\%20and\%20roth\&mode=Basic}{The Ethical Algorithm} M Kearns and A Roth (Chapter 4)
\item
  \href{https://arxiv.org/abs/1602.04938}{Ribeiro et al (2016)}. ``Why Should I Trust You?'': Explaining the Predictions of Any Classifier.
\end{itemize}

\hypertarget{reference-materials-2}{%
\subsection*{Reference Materials}\label{reference-materials-2}}
\addcontentsline{toc}{subsection}{Reference Materials}

\begin{itemize}
\item
  The \href{https://docker-curriculum.com/}{Docker Curriculum} by Prakhar Srivastav.
\item
  LIME \href{https://cran.r-project.org/web/packages/lime/index.html}{package documentation} on CRAN.
\item
  \href{https://christophm.github.io/interpretable-ml-book/}{Interpretable Machine Learning: A Guide for Making Black Box Models Explainable} by Christoph Molnar.
\item
  Documentation for \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/apply}{apply()}, \href{https://purrr.tidyverse.org/reference/map.html}{map()} and \href{https://furrr.futureverse.org/}{pmap()}
\item
  \href{https://adv-r.hadley.nz/index.html}{Advanced R (Second Edition)} by Hadley Wickham. \href{https://adv-r.hadley.nz/perf-measure.html}{Chapter 23} on measuring performance and \href{https://adv-r.hadley.nz/perf-improve.html}{Chapter 24} on improving performance.
\end{itemize}

\hypertarget{materials-of-interest-3}{%
\subsection*{Materials of Interest}\label{materials-of-interest-3}}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1080_00031305_2016_1154108\&context=PC\&vid=44IMP_INST:ICL_VU1\&lang=en\&search_scope=MyInst_and_CI\&adaptor=Primo\%20Central\&tab=Everything\&query=any,contains,ASA\%20p-value\&offset=0}{The ASA Statement on \(p\)-values: Context, Process and Purpose}
\item
  \href{http://stat.columbia.edu/~gelman/research/unpublished/forking.pdf}{The Garden of Forking Paths: Why multiple comparisons can be a problem,
  even when there is no ``Fishing expedition'' or ``p-hacking'' and the research
  hypothesis was posited ahead of time}. A Gelman and E loken (2013)
\item
  \href{https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html}{Understanding LIME tutorial} by T Pedersen and M Benesty.
\item
  \href{https://adv-r.hadley.nz/index.html}{Advanced R (Second Edition)} by Hadley Wickham. \href{https://adv-r.hadley.nz/rcpp.html}{Chapter 25} on writing R code in C++.
\end{itemize}

\hypertarget{data-science-ethics}{%
\section{Data Science Ethics}\label{data-science-ethics}}

\hypertarget{core-materials-4}{%
\subsection*{Core Materials}\label{core-materials-4}}
\addcontentsline{toc}{subsection}{Core Materials}

\begin{itemize}
\item
  \href{https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591\&context=L\&vid=44IMP_INST:ICL_VU1\&lang=en\&search_scope=MyInst_and_CI\&adaptor=Local\%20Search\%20Engine\&tab=Everything\&query=any,contains,kearns\%20and\%20roth\&mode=Basic}{The Ethical Algorithm} M Kearns and A Roth. Chapters 1 and 2 on Algorithmic Privacy and Algortihmic Fairness.
\item
  \href{https://proceedings.mlr.press/v81/buolamwini18a.html}{Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification} by Joy Buolamwini and Timnit Gebru (2018). Proceedings of the 1st Conference on Fairness, Accountability and Transparency.
\item
  \href{https://ieeexplore.ieee.org/document/4531148}{Robust De-anonymization of Large Sparse Datasets} by Arvind Narayanan and Vitaly Shmatikov (2008). IEEE Symposium on Security and Privacy.
\end{itemize}

\hypertarget{reference-materials-3}{%
\subsection*{Reference Materials}\label{reference-materials-3}}
\addcontentsline{toc}{subsection}{Reference Materials}

\begin{itemize}
\item
  \href{https://fairmlbook.org/}{Fairness and machine learning
  Limitations and Opportunities} by Solon Barocas, Moritz Hardt and Arvind Narayanan.
\item
  Professional Guidleines on Data Ethics from:

  \begin{itemize}
  \tightlist
  \item
    \href{http://www.ams.org/about-us/governance/policy-statements/sec-ethics}{The American Mathematical Society}
  \item
    \href{https://op.europa.eu/s/sUPP}{The European Union}
  \item
    \href{https://www.gov.uk/guidance/understanding-artificial-intelligence-ethics-and-safety}{UK Government}
  \item
    \href{https://rss.org.uk/RSS/media/News-and-publications/Publications/Reports\%20and\%20guides/A-Guide-for-Ethical-Data-Science-Final-Oct-2019.pdf}{Royal Statistical Society}
  \item
    \href{https://www.government.nl/documents/reports/2021/07/31/impact-assessment-fundamental-rights-and-algorithms}{Dutch Government}
  \end{itemize}
\end{itemize}

\hypertarget{materials-of-interest-4}{%
\subsection*{Materials of Interest}\label{materials-of-interest-4}}
\addcontentsline{toc}{subsection}{Materials of Interest}

\begin{itemize}
\tightlist
\item
  \href{https://arxiv.org/abs/2001.09784}{Algorithmic Fairness} (2020). Pre-print of review paper by Dana Pessach and Erez Shmueli.
\end{itemize}

  \bibliography{book.bib,packages.bib}

\end{document}
