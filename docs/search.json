[{"path":"index.html","id":"about-this-course","chapter":"About this Course","heading":"About this Course","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"index.html","id":"course-description","chapter":"About this Course","heading":"Course Description","text":"Model building evaluation necessary sufficient skills effective practice data science. module develop technical personal skills required work successfully data scientist within organisation.module critically explore :effectively scope manage data science project;work openly reproducibly;efficiently acquire, manipulate, present data;interpret explain work variety stakeholders;ensure work can put production;assess ethical implications work data scientist.interdisciplinary course draw fields including statistics, computing, management science data ethics. topic investigated selection lecture videos, conference presentations academic papers, hands-lab exercises, readings industry best-practices recognised professional bodies.","code":""},{"path":"index.html","id":"schedule","chapter":"About this Course","heading":"Schedule","text":"notes intended students course MATH70076: Data Science academic year 2022/23.course scheduled take place five weeks, suggested schedule :1st week: effective data science workflows;2nd week: aquiring sharing data;3rd week: exploratory data analysis visualisation;4th week: preparing production;5th week: ethics context data science.pdf version notes may downloaded . Please aware rough updated less frequently course webpage.","code":""},{"path":"index.html","id":"learning-outcomes","chapter":"About this Course","heading":"Learning outcomes","text":"successful completion module students able :Independently scope manage data science project;Source data internet web scraping APIs;Clean, explore visualise data, justifying documenting decisions made;Evaluate need (implement) approaches explainable, reproducible scalable;Appraise ethical implications data science projects, particularly risks compromising privacy fairness potential cause harm.","code":""},{"path":"index.html","id":"allocation-of-study-hours","chapter":"About this Course","heading":"Allocation of Study Hours","text":"Lectures: 10 Hours (2 hours per week)Group Teaching: 5 Hours (1 hour per week)Lab / Practical: 5 hours (1 hour per week)Independent Study: 105 hours (15 hours per week + 30 hours coursework)","code":""},{"path":"index.html","id":"assessment-structure","chapter":"About this Course","heading":"Assessment Structure","text":"course assessed entirely coursework, reflecting practical pragmatic nature course material.Coursework 1 (30%): completed fourth week course.Coursework 2 (70%): released last week course submitted following examination period Summer term.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"About this Course","heading":"Acknowledgements","text":"notes created Dr Zak Varty. inspired previous lecture series Dr Purvasha Chakravarti Imperial College London draw many resource made available R community.","code":""},{"path":"workflows-introduction.html","id":"workflows-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.data scientist never work alone.Within single project data scientist likely interact range people, including limited : one project managers, stakeholders subject matter experts. experts might come single specialism form multidisciplinary team, depending type work .get project put use working scale likely collaborate data engineers. also work closely data scientists, review one another’s work collaborate larger projects.Familiarity skills, processes practices make collaboration instrumental successful data scientist. aim part course provide structure organise perform work, can good collaborator current colleges future self.going require bit effort upfront, benefits compound time. get done wasting less time staring quizzically messy folders indecipherable code. also gain reputation someone good work . promotes better professional relationships greater levels trust, can turn lead working exciting impactful projects.","code":""},{"path":"workflows-organising-your-work.html","id":"workflows-organising-your-work","chapter":"2 Organising your work","heading":"2 Organising your work","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.Welcome course effective data science. week ’ll considering effective data science workflows. workflows ways progressing project help produce high quality work help make good collaborator.Chapter, ’ll kick things looking can structure data\nscience projects organize work. Familiarity skills, processes\npractices collaborative working going instrumental \nbecome successful data scientist.","code":""},{"path":"workflows-organising-your-work.html","id":"what-are-we-trying-to-do","chapter":"2 Organising your work","heading":"2.1 What are we trying to do?","text":"First, let’s consider want provide data science projects sense structure organization.data scientist ’ll never work alone. Within single project ’ll interact whole range people. might project manager, one business stakeholders variety subject matter experts.experts might trained sociologists, chemists, civil servants depending exact type data science work ’re .get project put use working scale ’ll \ncollaborate data engineers. ’ll also likely work closely data scientists. smaller projects might act reviewers one another’s work. larger projects working collaboratively allow tackle larger challenges. sorts project wouldn’t feasible alone, inherent limitations time skill one individual person.Even work small organization, ’re data scientist, adopting way working ’s focused collaborating pay dividends time. inevitably return project ’re working several weeks months years future ’ll forgotten almost everything first time around. ’ll also forgotten made decisions potential options didn’t take.exactly like working current colleague shoddy poor working practices. Nobody wants colleague somebody else, let alone future self. Even working alone, treating future self current collaborator (one want get along well ) makes kind colleague pleasure work .aim week provide guiding structure organize perform work. None going particularly difficult onerous. However require bit effort front daily discipline. Like flossing, daily effort required large benefits compound time.’ll get done wasting less time staring quizzically mess \nfolders indecipherable code. ’ll also get reputation someone ’s well organized good work . promotes better professional relationships greater levels trust within team. can , turn, tead working exciting impactful projects future.","code":""},{"path":"workflows-organising-your-work.html","id":"an-r-focused-approach","chapter":"2 Organising your work","heading":"2.2 An R Focused Approach","text":"structures workflows recommend throughout rest module focused strongly workflow predominantly uses R, markdown LaTeX.Similar techniques, code software can achieve results show coding Python C, writing projects Quarto markup language. Similarly, different organizations variations best practices ’ll go together. Often organisations extensive guidance topics.important thing understand good habits build one programming language business, transferring skills new setting largely matter learning new vocabulary slightly different syntax.said, let’s get going!","code":""},{"path":"workflows-organising-your-work.html","id":"one-project-one-directory","chapter":"2 Organising your work","heading":"2.3 One Project = One Directory","text":"’s one thing take away chapter, ’s one\nGolden Rule:Every individual project work data scientist single, self-contained directory folder.worth repeating. Every single project work self-contained live single directory. analogy might separate ring-binder folder modules degree program.one golden rule deceptively simple.first issue requires predetermined scope \nisn’t going covered particular project. seems straightforward outset project often know exactly project go, link pieces work within organization.second issue second law Thermodynamics, applies equally well project management heatdeath universe. takes continual external effort prevent contents one folder becoming chaotic disordered time.said, single directory several benefits justify additional work.","code":""},{"path":"workflows-organising-your-work.html","id":"properties-of-a-well-orgainsed-project","chapter":"2 Organising your work","heading":"2.4 Properties of a Well-Orgainsed Project","text":"properties like single, well-organized project ? Ideally, ’d like organize projects following\nproperties:PortableVersion Control FriendlyReproducibleIDE friendly.Don’t worry haven’t heard terms already. ’re going look little bit detail.","code":""},{"path":"workflows-organising-your-work.html","id":"portability","chapter":"2 Organising your work","heading":"2.4.1 Portability","text":"project said portable can easily moved without breaking.might small move, like relocating directory different location computer. might also mean moderate move, say another machine dies just big deadline. Alternatively, might large shift - uses another person using different operating system.thought experiment can see ’s full spectrum portable project may may need .","code":""},{"path":"workflows-organising-your-work.html","id":"version-control-friendly","chapter":"2 Organising your work","heading":"2.4.2 Version Control Friendly","text":"project Version Control changes tracked either manually automatically. means snapshots project taken regularly gradually develops evolves time. snapshots many, incremental changes made project allow rolled back specific previous state something goes wrong.version controlled pattern working helps avoid horrendous state found - renaming final_version.doc final_final_version.doc .organising workflow around incremental changes helps acknowledge work ever finally complete. always small changes need done future.","code":""},{"path":"workflows-organising-your-work.html","id":"reproducibility","chapter":"2 Organising your work","heading":"2.4.3 Reproducibility","text":"study reproducible can take original data computer code used analyze data recreate numerical findings study.\nBroman et al (2017). “Recommendations Funding Agencies Supporting Reproducible Research”paper, Broman et al define reproducibility project can take original data code used perform analysis using create numerical findings study.definition leads naturally several follow-questions.exactly definition? specifically mean future someone else access data code able recreate findings ? Also, reproducibility limited just numerical results? also able create associated figures, reports press releases?Another important question project needs reproduced. weeks time 10 years time? need protect project changes dependencies, like new versions packages modules? different versions R Python? Taking time scale even , different operating systems hardware?’s unlikely ’d consider someone handing floppy disk code runs Windows XP acceptably reproducible. Sure, probably find way get work, awful lot effort end.’s perhaps bit extreme example, emphasizes importance clearly defining level reproducibility ’re aiming within every project work . example also highlights amount work can required reproduce analysis, especially quite time. important explicitly think dividing effort original developer person trying reproduce analysis future.","code":""},{"path":"workflows-organising-your-work.html","id":"ide-friendly","chapter":"2 Organising your work","heading":"2.4.4 IDE Friendly","text":"final desirable property ’d like projects play nicely\nintegrated development environments.’re coding document writing data science projects ’d possible work entirely either plain text editor typing code directly command line. approaches data science workflow benefit simplicity, also expect great deal data scientist.workflows expect type everything perfectly accurately every time, recall names argument orders every function use, constantly aware current state objects within working environment.Integrated Development Environments (IDEs) applications help reduce burden, helping make effective programmer data scientist. IDEs offer tools like code completion highlighting make code easier read write. offer tools debugging, fix things going wrong, also offer environment panes don’t hold everything head . Many IDEs also often templating facilities. let save reuse snippets code can avoid typing repetitive, boilerplate code introducing errors process.Even haven’t heard IDEs , ’ve likely already used one. common examples might RStudio R-users, PyCharm python users, Visual Studio language agnostic coding environment.Whichever use, ’d like project play nicely . lets us reap benefits keeping project portable, version controlled, reproducible someone working different set-.","code":""},{"path":"workflows-organising-your-work.html","id":"project-structure","chapter":"2 Organising your work","heading":"2.5 Project Structure","text":"’ve given pretty exhaustive argument single directory project good idea. Let’s now take look inside directory define common starting layout content projects.sort project directory template mean ’ll always know find ’re looking members team . , start ’ll reiterate ’re taking opinionated approach providing sensible starting point organizing many projects.Every project going slightly different might require slight alterations suggest . Indeed, even start suggest might adapt project structure develops grows. think ’s helpful consider tailor making changes. ’m providing one size fits design, ’s great lots projects perfect none . ’s job alter refine design individual case.One final caveat get started: companies businesses many times house style write organize code projects. ’s case, follow style guide business company uses. important thing consistent individual level across entire data science team. ’s consistency reaps benefits.Okay, imagine now ’ve assigned shiny new project created single directory house project. ’ve, quite imaginatively, called directory exciting-new-project. populate folder ?rest video, ’ll define house-style organizing root directory data science projects module.Within project directory subdirectories, can tell folders file structure forward slash following names. also files directly root directory. One called readme.md another called either makefile make.r. ’re going explore files directories turn.","code":""},{"path":"workflows-organising-your-work.html","id":"readme.md","chapter":"2 Organising your work","heading":"2.5.1 README.md","text":"Let’s begin readme file. gives brief introduction project gives information project aims . readme file describe get started using project contribute development.readme written either plain text format readme.txt markdown\nformat readme.md. benefit using markdown allows light\nformatting sections headers lists using plain text characters.\ncan see using hashes mark first second level headers using bullet points unnumbered list. Whichever format use, readme file project always stored root directory typically named uppercase letters.readme file first thing someone ’s new project reads. placing readme root directory capitalising file name increase visibility file increae chances actually happening.additional benefit keeping readme root directory project code hosting services like GitHub, GitLab BitBucket display contents readme file next contents project. services also nicely format markdown use readme file.writing readme, can useful imaginge writing new, junior team member. readme file let get started project make simple contributions reading file. might also link detailed project documentation help new team member toward advanced understanding complex contribution.","code":""},{"path":"workflows-organising-your-work.html","id":"inside-the-readme","chapter":"2 Organising your work","heading":"2.5.2 Inside the README","text":"let’s take quick aside see detail covered within readme file.readme include name project, self-explanatory (nothing like generic choice exciting-new-project). readme also give project status, just couple sentences say whether project still development, version oft current release , end project life-cycle, project deprecated \nclosed.Following , also include description project. state purpose work provide, link , additional context references visitors aren’t assumed familiar .project involves code depends packages give instruction install dependencies run code. might just text also include things like screenshots, code snippets, gifs video whole process.’s also good practice include simple examples use code within project expected results, new users can confirm everything working local instance. Keep examples simple minimal can new usersFor longer complicated examples aren’t necessary short introductory document can add links readme explain \ndetail elsewhere.ideally short description people can report issues project also people can get started resolving issues extend \nproject way.leads one point ’ve forgotten list . section listing authors work license ’s\ndistributed. give credit people ’ve contributed \nproject license file says people may use work. license declates may use project whether give direct\nattribution work modifications use.","code":""},{"path":"workflows-organising-your-work.html","id":"data","chapter":"2 Organising your work","heading":"2.5.3 data","text":"Moving back project structure, next data directory.data directory two subdirectories one called raw one called derived. data generate part project stored raw subdirectory. ensure project reproducible, data Raw folder never edited modified.example ’ve got two different data types: Excel spreadsheet XLS file JSON file. files exacty received project stakeholder.text file metadata.txt plain text file explaining contents interpretation raw data sets. metadata include descriptions measured variables, units recorded , date file created acquired, source obtained.raw data likely isn’t going form ’s amenable analyzing straight away. get data pleasant form work, require data manipulation cleaning. manipulation cleaning applied well documented resulting cleaned files saved within derived data directory.exciting new project, can see clean versions previous data sets ready modelling. ’s also third file folder. data ’ve acquired web scraping, using script within project.","code":""},{"path":"workflows-organising-your-work.html","id":"src","chapter":"2 Organising your work","heading":"2.5.4 src","text":"src source directory contains source code project. typically functions ’ve written make analysis modelling code accessible.’ve saved function R script , project, ’ve used subdirectories organise use case. ’ve got two functions used data cleaning: first replaces NA values given value, second replaces mean non-missing values.also three helper functions: first two calculate rolling mean \ngeometric mean given vector, third function scrapes \nweb data saw derived data subdirectory.","code":""},{"path":"workflows-organising-your-work.html","id":"tests","chapter":"2 Organising your work","heading":"2.5.5 tests","text":"Moving tests directory. structure directory mirrors source directory. function file counterpart file tests.test files provide example sets inputs expected outputs \nfunction. test files used check edge cases function assure\nhaven’t broken anything fixing small bug adding new capabilities function.","code":""},{"path":"workflows-organising-your-work.html","id":"analyses","chapter":"2 Organising your work","heading":"2.5.6 analyses","text":"analyses directory contains probably think bulk \ndata science work. ’s going one subdirectory major analysis ’s performed within project within might series steps collect separate scripts.activity performed step made clear name script, order ’re going perform steps. can see scripts used 2021 annual report. First script used take raw monthly receipts produce cleaned version data set saw earlier.\nfollowed trend analysis cleaned data set.Similarly spending review data cleaning step, followed forecast modelling finally production diagnostic plots compare \nforecasts.","code":""},{"path":"workflows-organising-your-work.html","id":"outputs","chapter":"2 Organising your work","heading":"2.5.7 outputs","text":"outputs directory one subdirectory meta-analysis within project. organized output type whether data, figure, table.Depending nature project, might want use modified subdirectory structure . example, ’re several numerical experiments might want arrange outputs experiment, rather output type.","code":""},{"path":"workflows-organising-your-work.html","id":"reports","chapter":"2 Organising your work","heading":"2.5.8 reports","text":"reports directory everything comes together. written documents form final deliverables project created. final documents written LaTeX markdown, source \ncompiled documents can found within directory.including content report, example figures, ’d recommend \nmaking copies figure files within reports directory. , ’ll manually update files every time modify . Instead can use relative file paths include figures. Relative file paths specify get image, starting TeX document moving levels project directory.’re using markdown LaTeX write reports, instead use online platform like overleaf latex editor Google docs write collaboratively links reports directory using additional readme files. Make sure set read write permissions links appropriately, .using online writing systems, ’ll manually upload update plots whenever modify earlier analysis. ’s one drawbacks online tools traded ease use.exciting new project, can see annual report written markdown format, compiled HTML PDF. spendiing review written LaTeX source , don’t compiled pdf version document.","code":""},{"path":"workflows-organising-your-work.html","id":"make-file","chapter":"2 Organising your work","heading":"2.5.9 make file","text":"final element template project structure make file. aren’t going cover read write make files course. Instead, ’ll give brief description supposed .high level, make file just text file. makes special contains. Similar shell bash script, make file contains code run command line. code create update element project.make file defines shorthand commands full lines code create element project. make file also records order operations happen, steps dependent one another. means one step part project updated changes propagated entire project. done quite clever way part projects re-run need updated.’re omitting make files course ’re fiendishly difficult write read, rather require reasonable foundation working command line understood. suggest instead throughout course create R markdown file called make. file define intended running order dependencies project R file, might also automate parts analysis.","code":""},{"path":"workflows-organising-your-work.html","id":"wrapping-up","chapter":"2 Organising your work","heading":"2.6 Wrapping up","text":"Wrapping , ’s everything chapter.’ve introduced project structure serve well baseline vast majority projects data science.work, remember key standardisation. Working consistently across projects, company group important sticking rigidly particular structure defined .two notable exceptions probably don’t want use project structure. ’s ’re building app ’re building package. require specific organisation files within project directory. ’ll\nexplore project structure used package development live\nsession week.","code":""},{"path":"workflows-naming.html","id":"workflows-naming","chapter":"3 Naming Files","heading":"3 Naming Files","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-naming.html","id":"introduction","chapter":"3 Naming Files","heading":"3.1 Introduction","text":"“two hard things Computer Science: cache invalidation naming things.”Phil Karlton, Netscape DeveloperWhen working data science project can principle name directories, files, functions objects whatever like. reality though, using ad-hoc system naming likely cause confusion, headaches mistakes. obviously want avoid things, spirit kind current colleges also future selves.Coming good names art form. Like art, naming things activity get better practice. Another similarity best naming systems don’t come giving data scientists free reign naming system. Like art, best approaches naming things give strong guidelines boundaries within express creativity skill.lecture ’ll explore boundaries want achieve us. content lecture based largely around talk name given Jennifer Bryan tidyverse style guide, forms basis Google’s style guide R programming.","code":""},{"path":"workflows-naming.html","id":"naming-files","chapter":"3 Naming Files","heading":"3.2 Naming Files","text":"’ll begin focusing call files. , ’ll first focus part file name comes dot. second part video, ’ll cycle back around discuss file extensions.","code":""},{"path":"workflows-naming.html","id":"what-do-we-want-from-our-file-names","chapter":"3 Naming Files","heading":"3.2.1 What do we want from our file names?","text":"dive naming files, first consider want file names choose. three key properties like satisfy.Machine ReadableHuman ReadableOrder FriendlyThee first desirable property file names easily readable computers, second file names easily readable humans finally file names take advantage default ordering imposed files.set current file names sorely lacking across properties:want provide naming conventions move us toward better file names listed .Let’s take minutes examine exactly mean properties.","code":"abstract.docx\nEffective Data Science's module guide 2022.docx \nfig 12.png\nRplot7.png\n1711.05189.pdf\nHR Protocols 2015 FINAL (Nov 2015).pdf2015-10-22_human-resources-protocols.pdf\n2022_effective-data-science-module-guide.docx\n2022_RSS-conference-abstract.docx \nfig12_earthquake-timeseries.png \nfig07_earthquake-location-map.png\nogata_1984_spacetime-clustering.pdf"},{"path":"workflows-naming.html","id":"machine-readable","chapter":"3 Naming Files","heading":"3.2.2 Machine Readable","text":"mean machine readable file names?Easy compute deliberate use delimiters:\nunderscores_separate_metadata, hyphens-separate-words.\nunderscores_separate_metadata, hyphens-separate-words.Play nicely regular expressions globbing:\navoid spaces, punctuation, accents, cases;\nrm Rplot*.png\navoid spaces, punctuation, accents, cases;rm Rplot*.pngMachine readable names useful :managing files: ordering, finding, moving, deleting:managing files: ordering, finding, moving, deleting:extracting information directly file names,extracting information directly file names,working programmatically file names regex.working programmatically file names regex.operating large number files useful able work programmatically.One example might useful downloading assessments marking. might require unzip large number zip files, copying pdf report unzipped folder single directory R scripts unzipped folder another directory. marked scripts code need paired back folders named student, re-zipped ready returned.monotonously dull might work ~50 students ~5000. Working programmatically files way get job done efficiently. requires file names play nicely way computers interpret file names, regard string characters.often helpful metadata included file name, example student’s id number assessment title. use underscore separate elements metadata within file name hyphen separate sub-elements meta-data, example words within assessment title.Regular expressions globbing two ideas string manipulation may met, inform naming conventions. Regular expressions allow search strings (case file names) match particular pattern. Regular expressions can really complicated searches become gnarly worry special characters like spaces, punctuation, accents cases, avoided file names.special type regular expression called globbing star used replace number subsequent characters file name, can delete png images begin Rplot using single line code. Globbing becomes particular powerful use consistent structure create file names.assessment marking example, machine readable file names particularly useful managing files, ordering, finding, moving deleting . Another example analysis requires load large number individual data files.Machine readable file names also useful extracting meta-information files without open memory. particularly useful files might large load memory, want load data certain year.final benefit list scalability, reduction drudgery lowered risk human error operating large number files.","code":""},{"path":"workflows-naming.html","id":"order-friendly","chapter":"3 Naming Files","heading":"3.2.3 Order Friendly","text":"next property focus also links computers operate. ’d like file names exploit default orderings used computers. means starting file names character strings metadata allow us order files meaningful way.","code":""},{"path":"workflows-naming.html","id":"running-order","chapter":"3 Naming Files","heading":"3.2.3.1 Running Order","text":"One example ’s logical order code executed, example analysis .Prepreding numbers file names can make intended ordering immediately obvious.Starting single digit numbers leading 0 good idea prevent script 1 sorted tens, script 2 twenties . might 100 files, example saving output many simulations, use two zeros maintain nice ordering.","code":"diagnositc-plots.R\ndownload.R\nruntime-comparison.R\n...\nmodel-evaluation.R\nwrangle.R00_download.R\n01_wrangle.R\n02_model.R\n...\n09_model-evaluation.R\n10_model-comparison-plots.R"},{"path":"workflows-naming.html","id":"date-order","chapter":"3 Naming Files","heading":"3.2.3.2 Date Order","text":"second example orderable file names file date associated . might version report date data recorded, cleaned updated.using dates, file names elsewhere, conform ISO standard date format.ISO 8601 sets international standard format dates: YYYY-MM-DD.format uses four numbers year, followed two numbers month two numbers day month. structure mirrors nested file structure moving least specific. also avoids confusion ordering date elements. Without using ISO standard date like 04-05-22 might interpreted fourth May 2022, fifth April 2022, 22nd May 2004.","code":"2015-10-22_human-resources-protocols.pdf\n...\n2022-effective-data-science-module-guide.docx"},{"path":"workflows-naming.html","id":"human-readable","chapter":"3 Naming Files","heading":"3.2.4 Human Readable","text":"final property like file names human readability. requires names files meaningful, informative easily read real people.first two handled including appropriate metadata file name. ease read real people determined length file name name formatted.lots formatting options fun names like camelCase, PascalCase, snake_case.’s weak evidence suggests snake skewer cases readable. ’ll use mixture , using snake case metadata items skewer case within . slight cost legibility, trade-making computing file names easier.final aspect control length name. short, evocative useful file names easy skill . hints tips might want look tips writing URL slugs. last part web address intended improve accessibility immediately intuitively meaningful user.","code":"   easilyReadByRealPeople (camelCase)\n   EasilyReadByRealPeople (PascalCase)\n   easily_read_by_real_people (snake_case)\n   easily-read-by-real-people (skewer-case)"},{"path":"workflows-naming.html","id":"naming-files---summary","chapter":"3 Naming Files","heading":"3.2.5 Naming Files - Summary","text":"File names meaningful, informative scripts end .rFile names meaningful, informative scripts end .rStick letters, numbers underscores (_) hyphens (-).Stick letters, numbers underscores (_) hyphens (-).Pay attention capitalisation file.r \\(\\neq\\) File.r operating systems.Pay attention capitalisation file.r \\(\\neq\\) File.r operating systems.Show order left-padded numbers ISO dates.Show order left-padded numbers ISO dates.","code":""},{"path":"workflows-naming.html","id":"file-extensions-and-where-you-work","chapter":"3 Naming Files","heading":"3.3 File Extensions and Where You Work","text":"far focused entirely comes dot, file name.Equally, , important comes dot, file extension.file extension describes information stored file determines software can use, view run file.likely already use file extensions distinguish code scripts, written documents, images, notebook files. ’ll now explore benefits drawbacks various file types respect several important features.","code":"example-script.r\nexample-script.py\n\nproject-writeup.doc\nproject-writeup.tex"},{"path":"workflows-naming.html","id":"open-source-vs-proprietary-file-types","chapter":"3 Naming Files","heading":"3.3.1 Open Source vs Proprietary File Types","text":"first feature ’ll consider whether file type open source, can used anyone without charge, specialist software must paid order interact files.figure , column represents different class file, moving left right example file types tabular data, list-like data text documents. File types closer top open source lower rely proprietary software, may may require payment.make sure work accessible many people possible favour open source options like csv files Google sheets excel, JSON files Matlab data files, tex markdown word Google doc.usually benefit terms project longevity scalability. open source file types often somewhat simpler structure, making robust changes time less memory intensive.see , let’s take look inside data files.","code":""},{"path":"workflows-naming.html","id":"inside-data-files","chapter":"3 Naming Files","heading":"3.3.2 Inside Data Files","text":"","code":""},{"path":"workflows-naming.html","id":"inside-a-csv-file","chapter":"3 Naming Files","heading":"3.3.2.1 Inside a CSV file","text":"CSV comma separated value files used store tabular data.tabular data, row data represents one record column represents data value.csv encodes record separate line using commas separate values record. can see opening csv file text editor notepad.raw data stores line breaks using \\n indicates new rows \\r. backslashed indicae escape characters special meanings, literally interpreted letters n r.viewed text editor, example file look something like .","code":"\nlibrary(readr)\nread_file(file = \"images/102-workflows-naming-files/example.csv\")\n#> [1] \"Name,Number\\r\\nA,1\\r\\nB,2\\r\\nC,3\"Name,Number \nA,1\nB,2\nC,3"},{"path":"workflows-naming.html","id":"inside-a-tsv-file","chapter":"3 Naming Files","heading":"3.3.2.2 Inside a TSV file","text":"TSV tab separated value files also used store tabular data.Like csv record given new line tsv tabs rather commas used separate values record. can also seen opening tsv file text editor notepad.One thing note tabs separate character just multiple spaces. plain text can impossible tell apart, text editors option display tabs differently repeated spaces, though usually enabled default.","code":"\nlibrary(readr)\nread_file(file = \"images/102-workflows-naming-files/example.tsv\")\n#> [1] \"Name\\tNumber\\r\\nA\\t1\\r\\nB\\t2\\r\\nC\\t3\"Name    Number \nA   1\nB   2\nC   3"},{"path":"workflows-naming.html","id":"inside-an-excel-file","chapter":"3 Naming Files","heading":"3.3.2.3 Inside an Excel file","text":"open excel file text editor, immediately see human interpretable file format.entry four digit hexadecimal number lot entries small table.excel files can carry lot additional information csv tsv able , example cell formatting multiple tables (called sheets excel) stored within single file.means excel files take much memory carrying lot information strictly contained within data .","code":"504b 0304 1400 0600 0800 0000 2100 62ee\n9d68 5e01 0000 9004 0000 1300 0802 5b43\n6f6e 7465 6e74 5f54 7970 6573 5d2e 786d\n6c20 a204 0228 a000 0200 0000 0000 0000\n0000 0000 0000 0000 0000 0000 0000 0000\n.... .... .... .... .... .... .... ....\n0000 0000 0000 0000 ac92 4d4f c330 0c86\nef48 fc87 c8f7 d5dd 9010 424b 7741 48bb\n2154 7e80 49dc 0fb5 8da3 241b ddbf 271c\n1054 1a83 0347 7fbd 7efc cadb dd3c 8dea\n.... .... .... .... .... .... .... ...."},{"path":"workflows-naming.html","id":"indise-a-json-file","chapter":"3 Naming Files","heading":"3.3.2.4 Indise a JSON file","text":"JSON, Java Script Object Notation, files open source format list-like data. record represented collection key:value pairs. example table entry two fields, one corresponding Name key one corresponding Number key.list-like structure allows non-tabular data stored using property called nesting: value taken key can single value, vector values another list-like object.ability create nested data structures lead data format used widely range applications require data transfer.","code":"[{\n    \"Name\": \"A\",\n    \"Number\": \"1\"\n}, {\n    \"Name\": \"B\",\n    \"Number\": \"2\"\n}, {\n    \"Name\": \"C\",\n    \"Number\": \"3\"\n}]"},{"path":"workflows-naming.html","id":"inside-an-xml-file","chapter":"3 Naming Files","heading":"3.3.2.5 Inside an XML file","text":"XML files another open source format list-like data, record represented collection key:value pairs.difference JSON file mainly records formatted within file. JSON file designed look like objects Java Script programming language XML formatting done look like html, markup language used write websites.","code":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<root>\n  <row>\n    <Name>A<\/Name>\n    <Number>1<\/Number>\n  <\/row>\n  <row>\n    <Name>B<\/Name>\n    <Number>2<\/Number>\n  <\/row>\n  <row>\n    <Name>C<\/Name>\n    <Number>3<\/Number>\n  <\/row>\n<\/root>"},{"path":"workflows-naming.html","id":"a-note-on-notebooks","chapter":"3 Naming Files","heading":"3.3.3 A Note on Notebooks","text":"two half notebook formats likely use : .rmd (alternatively .qmd) .ipynb.two half notebook formats likely use : .rmd (alternatively .qmd) .ipynb.R markdown documents .rmd plain text files, human friendly.R markdown documents .rmd plain text files, human friendly.JuPyteR notebooks multi-language support human friendly (JSON disguise).JuPyteR notebooks multi-language support human friendly (JSON disguise).Quarto documents offer best worlds extensive language support. yet established format.Quarto documents offer best worlds extensive language support. yet established format.addition files read write, files code largely determine workflow.three main options way code: first typing directly command line, second using text editor IDE write scripts third writing notebook mixes code. text output together single document.’ll compare methods working next slide, first let’s quick review notebooks available might want use .data scientist, two half notebook formats ’re likely met . first two Rmarkdown files working predominantly R interactive python jupyter notebooks working predominantly python. final half format quarto markdown documents, relatively new extend functionality Rmarkdown files.main benefit R markdown documents ’re plain text files, ’re human friendly. JuPyteR notebooks benefit supporting code written Julia, Python R, human friendly - hood documents JSON files edited directly misplaced bracket break .Quarto documents offer best worlds, plain text formatting even extensive language support jupyter notebooks. Quarto recent extension Rmarkdown, rapidly becoming popular data science community.format benefits drawbacks depending context used shared benefits limitations nature notebook documents.","code":""},{"path":"workflows-naming.html","id":"file-extensions-and-where-you-code","chapter":"3 Naming Files","heading":"3.3.4 File Extensions and Where You Code","text":"main benefit notebook documents self-documenting, can mix documentation, code report single document. Notebooks also provide level interactivity coding possible working directly command line using text editor write scripts. second factor easily overcome using integrated development environment scripting.Writing code .r files self-documenting separation code, documentation outputs many benefits. Firstly, resulting scripts provide reproducible automatable workflow, unlike one-lines code run command line. Secondly, using IDE write provides syntax highlighting code linting features help write readable accurate code. Finally, separation code documentation output allows work easily even directly put production.course advocate scripting-first approach data science, though notebooks command line work definitely place.Notebooks great teaching rapid development tools strong limitations put production. Conversely, coding directly command line can leave trace workflow lead analysis replicated future.","code":""},{"path":"workflows-naming.html","id":"summary","chapter":"3 Naming Files","heading":"3.3.5 Summary","text":"Finally, let’s wrap things summarising learned naming files.dot want pick file names machine readable, human friendly play nicely default orderings provided us.Name files :Machine Readable,Human Readable,Order Friendly.dot, want pick file types widely accessible, easily read humans allow entire analysis reproduced.Use document types :Widely accessible,Easy read reproduce,Appropriate task hand.want name files pick file types best match team working task hand.","code":""},{"path":"workflows-code.html","id":"workflows-code","chapter":"4 Code","heading":"4 Code","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-code.html","id":"introduction-1","chapter":"4 Code","heading":"4.1 Introduction","text":"already described might organise effective data science project directory file level. chapter delve one step deeper consider can structure work within files. particular, ’ll focus code files .’ll start comparing two main approaches structuring code, namely functional programming object oriented programming.’ll see order code within scripts conventions name functions objects work code.Rounding chapter, ’ll summarise main points R style guide following course highlight useful packages writing effective code.","code":""},{"path":"workflows-code.html","id":"functional-programming","chapter":"4 Code","heading":"4.2 Functional Programming","text":"functional programming style two major properties:Object immutability,Object immutability,Complex programs written using function composition.Complex programs written using function composition.Firstly, original data objects aren’t modified altered code. met idea making new, cleaner versions raw data leave original messy data intact. Object immutability exact idea code context rather data context.Secondly, functional programming, complex problems solved decomposing series smaller problems. separate, self-contained function written solve sub-problem. individual function , , simple easy understand. makes small functions easy test easy reuse many places. Code complexity built composing functions various ways.can difficult get way thinking, people mathematical training often find quite natural. mathematicians many years experience working function compositions abstract, mathematical sense.\\[y = g(x) = f_3 \\circ f_2 \\circ f_1(x).\\]","code":""},{"path":"workflows-code.html","id":"the-pipe-operator","chapter":"4 Code","heading":"4.2.1 The Pipe Operator","text":"One issue functional programming lots nested functions means also lots nested brackets. start get tricky keep track upwards 3 functions composed. reading difficulty exacerbated functions additional arguments top original inputs.pipe operator %>% magrittr package helps issue. works exactly like function composition: takes whatever left (whether object output function) passes following function call first argument function.pipe operator often referred “syntactic sugar”. doesn’t add anything code rather makes code much palatable.R versions 4.1 greater, ’s built-version pipe operator, written using vertical bar symbol followed greater sign. type vertical bar, can usually find found backslash keyboard.(Just cause confusion, vertical bar symbol also called pipe symbol general programming contexts. )base R pipe usually behaves way pipe magrittr, cases differ. reasons back-compatibility consistency ’ll stick magrittr pipe course.","code":"\nlog(exp(cos(sin(pi))))\n#> [1] 1\nlibrary(magrittr)\npi %>% \n  sin() %>% \n  cos() %>% \n  exp() %>% \n  log()\n#> [1] 1\niris %>% \n  head(n = 3)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\nlibrary(magrittr)\npi |> \n  sin() |> \n  cos() |> \n  exp() |> \n  log()\n#> [1] 1\niris |> \n  head(n = 3)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa"},{"path":"workflows-code.html","id":"when-not-to-pipe","chapter":"4 Code","heading":"4.2.2 When not to pipe","text":"Pipes designed put focus actions performing rather object preforming operations . means two cases almost certainly use pipe.first need manipulate one object time. Using secondary objects reference points (leaving unchanged) course perfectly fine, pipes used applying sequence steps create new, modified version one primary object.Secondly, just can chain together many actions single pipeline, doesn’t mean necessarily . long sequences piped operations easier read nested functions, however still burden reader cognitive load short term memory. kind create meaningful, intermediate objects informative names. ’ll help reader easily understand logic within code.","code":""},{"path":"workflows-code.html","id":"object-oriented-programming","chapter":"4 Code","heading":"4.3 Object Oriented Programming","text":"main alternative functional programming object oriented programming.Solve problems using lots simple objectsR 3 OOP systems: S3, S4 R6.Objects belong class, methods fields.Example: agent based simulation beehive.","code":""},{"path":"workflows-code.html","id":"oop-philosophy","chapter":"4 Code","heading":"4.3.1 OOP Philosophy","text":"functional programming, solve complicated problems using lots simple functions. object oriented programming solve complicated problems using lots simple objects. programming approaches best depend particular type problem trying solve.Functional programming excellent types data science work. Object oriented comes problem many small components interacting one another. makes great things like designing agent-based simulations, ’ll come back moment.R three different systems object oriented programming (called S3, S4, R6), things can get bit complicated. won’t go detail , ’ll give overview main ideas.approach programming might useful future, example want extend base R functions work new types input, user-friendly displays. case (Advanced R)[https://adv-r.hadley.nz/] Hadley Wickham excellent reference text.OOP, object belongs class set methods associated . class defines object methods describe object can . top , object class-specific attributes data fields. fields shared objects class values take give information specific object.","code":""},{"path":"workflows-code.html","id":"oop-example","chapter":"4 Code","heading":"4.3.2 OOP Example","text":"sounding abstract. Let’s consider writing object oriented code simulate beehive. object bee, bee instance one three bee classes: might queen, worker drone example. Different bee classes different methods associated , describe bee can , example bees 6 methods let move , , left, right, forward backward within hive. additional “reproduce” method might defined queen bees pollinate method might defined workers. instance bee fields, give data specific bee. bees x, y z coordinate fields giving location within hive. queen class might additional field number offspring workers might additional field much pollen carrying.simulation progresses, methods applied object altering fields potentially creating destroying objects. different preservation mindset functional programming, hopefully can see natural approach many types problem.","code":""},{"path":"workflows-code.html","id":"structuring-r-script-headers","chapter":"4 Code","heading":"4.4 Structuring R Script Headers","text":"TL;DR\n- Start script comment 1-2 sentences explaining > .\n- setwd() rm(ls()) devil’s work.\n- “Session” > “Restart R” Keyboard shortcut: crtl/cmd + shift > + 0\n- Polite gather library() source() calls.\n- Rude mess people’s set using > install.packages().\n- Portable scripts use paths relative root directory project.First things first, let’s discuss top R scripts.almost always good idea start file commented sentences describing purpose script , work large team, perhaps contact questions script. (comments coming soon, don’t worry!)also good practise move library() source() calls top script. indicate packages helper function dependencies script; ’s useful know need installed trying run code.segues nicely next point, never hard code package installations. extremely bad practise rude script might alter another person’s R installation. don’t know already, precisely difference install.packages() library() call: install.packages() download code package users computer, library() takes downloaded code makes available current R session. avoid messing anyone’s R installation, always type install.package() commands directly console place corresponding library() calls within scripts.Next, likely , someone close , commit felony starting every script setting working directory clearing R’s global environment. bad practice, ’s indicative workflow ’s project based ’s problematic least two reasons. Firstly, path set likely work anyone else’s computer. Secondly, clearing environment like may look like gets back fresh, new R session previously loaded packages still loaded lurking background.Instead, achieve original aim starting new R session, go menu select “Session” drop select “Restart R”. Alternatively, can use keyboard shortcuts . “crtl + shift + 0” Windows “cmd + shift + 0” mac. fact keyboard shortcut exists quite strongly hint , reproducible project oriented workflow, restarting R quite often average working day. scripting equivalent “clear output rerun ” notebook.Finally, let’s circle back point made earlier setting working directory. reason work likely giving file paths specific computer, operating system file organisation system. chances someone else practically zero.","code":""},{"path":"workflows-code.html","id":"portable-file-paths-with-here","chapter":"4 Code","heading":"4.5 Portable File paths with {here}","text":"fix problem person- computer-specific file paths can two options.first use relative file paths. assume R script run current location moving levels project directory point file need.good solves problem paths breaking move project different location laptop. However, fully solve portability problem might move file different location within project. also solve problem windows uses MacOS linux use forward slashes file paths widows uses backslashes.resolve final two issues recommend using () function {} package. package looks .Rproj .git file identify root directory project creates file paths relative root project, suitable operating system code run .really quite marvellous. information use package, explore chapter R - Forgot, R Data Science project oriented workflow blog post.","code":"\n# Bad - breaks if project moved\nsource(\"zaks-mbp/Desktop/exciting-new-project/src/helper_functions/rolling_mean.R\")\n\n# Better - breaks if Windows\nsource(\"../../src/helper_functions/rolling_mean.R\")\n\n# Best - but use here:here() to check root directory correctly identified\nsource(here::here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n# For more info on the here package:\nvignette(\"here\")"},{"path":"workflows-code.html","id":"code-body","chapter":"4 Code","heading":"4.6 Code Body","text":"Moving now, go head body code. well named organised code facilitate reading understanding. Comments sectioning rest work.section designed introduction tidyverse style guide replacement .\n### CommentsComments may either short -line comments end line full lines dedicated comments. create either type comment R, simply type hash followed one space. rest line evaluated function comment. multi-line comments needed simply start multiple lines hash space.purpose comments explain , . explaining comments perhaps need consider writing informative function names, something return general advice section.Comments can also used add structure code, buy using commented lines hyphens equal signs chunk files minor major sections.Markdown-like section titles can added section subsection headers. Many IDEs, RStudio, interpret table contents , can easily navigate code.","code":"\n# This is an example script showing good use of comments and sectioning \n\nlibrary(here)\nsource(here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n#===============================================================================  <- 80 characters max for readability\n# Major Section on Comments ----\n#===============================================================================\n\n#-------------------------------------------------------------------------------\n##  Minor Section on inline comments ---- \n#-------------------------------------------------------------------------------\nx <- 1:10 # this is an inline comment\n\n#-------------------------------------------------------------------------------\n##  Minor Section on full line comments ---- \n#-------------------------------------------------------------------------------\nrolling_mean(x)\n# This is an full line comment"},{"path":"workflows-code.html","id":"objects-are-nouns","chapter":"4 Code","heading":"4.6.1 Objects are Nouns","text":"Object names use lowercase letters, numbers, _.Object names use lowercase letters, numbers, _.Use underscores (_) separate words within name. (snake_case)Use underscores (_) separate words within name. (snake_case)Use nouns, preferring singular plural names.Use nouns, preferring singular plural names.creating naming objects strong guideline objects named using short meaningful nouns. Names include special characters use underscores separate words within object name.similar file naming guide, note hyphens can’t used object names conflicts subtraction operator.naming objects, far possible use singular nouns. main reason plurisation rules English complex eventually trip either user code.","code":"\n# Good\nday_one\nday_1\n\n# Bad\nfirst_day_of_the_month\nDayOne\ndayone\ndjm1"},{"path":"workflows-code.html","id":"functions-are-verbs","chapter":"4 Code","heading":"4.6.2 Functions are Verbs","text":"Function names use lowercase letters, numbers, _.Function names use lowercase letters, numbers, _.Use underscores (_) separate words within name. (snake_case)Use underscores (_) separate words within name. (snake_case)Suggest imperative mood, recipe.Suggest imperative mood, recipe.Break long functions multiple lines. 4 vs 2 spaces.Break long functions multiple lines. 4 vs 2 spaces.guidelines naming functions broadly similar, advice functions verbs rather nouns.Functions named imperative mood, like recipe. consistency; function names range moods tenses leads coding nightmares.object names aim give functions arguments short, evocative names. functions many arguments long name, might able fit function definition single line. case can place argument double indented line function body single indented line.","code":"\n# Good\nadd_row()\npermute()\n\n# Bad\nrow_adder()\npermutation()\n\nlong_function_name <- function(\n    a = \"a long argument\",\n    b = \"another argument\",\n    c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}"},{"path":"workflows-code.html","id":"casing-consistantly","chapter":"4 Code","heading":"4.6.3 Casing Consistantly","text":"mentioned already, many options separating words within names:CamelCasepascalCasesnakecaseunderscore_separated ❤️hyphen-separatedpoint.separated 💀people used working Python tempting use point separation function names, spirit methods object oriented programming. Indeed, base R functions even use convention.However, reason advise already used methods R’s inbuilt OOP functionality. use underscore separation work.","code":""},{"path":"workflows-code.html","id":"style-guide-summary","chapter":"4 Code","heading":"4.6.4 Style Guide Summary","text":"Use comments structure codeObjects = NounsFunctions = VerbsUse snake case consistant grammar","code":""},{"path":"workflows-code.html","id":"further-tips-for-friendly-coding","chapter":"4 Code","heading":"4.7 Further Tips for Friendly Coding","text":"addition naming conventions style guide gives lots guidance writing code way kind future readers code.’m going go repeat guidance , motivation can boiled following points.Write code easily understood humans.Write code easily understood humans.Use informative names, typing cheap.Use informative names, typing cheap.Divide work logical stages, human memory expensive.writing code, keep future reader mind. means using names informative reasonably short, also means adding white space, comments formatting aid comprehension. Adding sort structure code also helps reduce cognitive burden placing human reading code.Informative names important short names. particularly true using flow controls, things like loops loops. loops like encounter approaching deadline urgently fixing bug? Almost surely second one, context immediately clear.computer doesn’t care call variable single letter, random key smash (like aksnbioawb) informative name. computer also doesn’t care include white space code - script still run. However, things friendly practices can help debugging co-workers collaborating.","code":"\n# Bad\nfor(i in dmt){\n  print(i)\n}\n\n# Good\nfor(temperature in daily_max_temperature){\n  print(temperature)\n}"},{"path":"workflows-code.html","id":"reduce-reuse-recycle","chapter":"4 Code","heading":"4.8 Reduce, Reuse, Recycle","text":"final section, ’ll look can make workflow efficient reducing amount code write, well reusing recycling code ’ve already written.","code":""},{"path":"workflows-code.html","id":"dry-coding","chapter":"4 Code","heading":"4.8.1 DRY Coding","text":"idea making workflow efficient reducing, reusing recycling code summarised DRY acronym: don’t repeat .can boiled three main points:something twice single script, write function thing,want use function elsewhere within project, save separate scriptIf want use function across projects, add package.course, like scoping projects first place, requires level clairvoyance: able look future see whether ’ll use function another script project. difficult, bordering impossible. practice, done retrospectively - find second script project needs function pull separate file include package.rule thumb, consider whether make function widely available . takes much less effort work now, ’s fresh mind, refamiliarise code several years time.Let’s now look implement sub-bullet points:\n“write function, document ” “write function, test ”.","code":""},{"path":"workflows-code.html","id":"rememer-how-to-use-your-own-code","chapter":"4 Code","heading":"4.8.2 Rememer how to use your own code","text":"come use function written somebody else, likely refer documentation teach remind things like expected inputs exactly method implemented.writing functions create documentation fills need. Even function just personal use, time ’ll forget exactly works.write function, document .documentation contain?InputsOutputsExample use casesAuthor (obvious working team)documentation describe inputs outputs function, simple example uses. working large team, documentation also indicate wrote function ’s responsible maintaining time.","code":""},{"path":"workflows-code.html","id":"roxygen2-for-documentation","chapter":"4 Code","heading":"4.8.3 {roxygen2} for documentation","text":"way used package simplify file path problems, ’ll use roxygen2 package simplify testing workflow.{roxygen2} package gives us easily insert-able temple documenting functions. means don’t waste time energy typing remembering boilerplate code. also puts documentation format allows us get hints auto-completion functions, just like functions use packages written people.use Roxygen, need install - doesn’t need loaded library call top script. ’ve done , cursor inside function definition, can insert skeleton code document function one two ways: can either use Rstudio menu keyboard short cut operating system.install.packages(\"roxygen2\")cursor inside function: Code > Insert Roxygen SkeletonKeyboard shortcut: cmd + option + shift + r crtl + option + shift + rFill relevant fields","code":""},{"path":"workflows-code.html","id":"an-roxygen2-example","chapter":"4 Code","heading":"4.8.4 An {roxygen2} example","text":", ’ve got example Roxygen skeleton document function calculates geometric mean vector. , hash followed apostrophe special type comment. indicates function documentation rather just regular comment.’ll fill fields skeleton apart export, ’ll remove. put function R package, export field makes available users package. Since just standalone function won’t need export field, though keeping wouldn’t actually cause us problems either.filled skeleton documentation might look something like . described function , expected inputs user can expect output. ’ve also given simple examples function can used.Roxygen, see package documentation chapter R packages function documentation.","code":"\n#' Title\n#'\n#' @param x \n#' @param remove_NA \n#'\n#' @return\n#' @export\n#'\n#' @examples\ngeometric_mean <- function(x, remove_NA = FALSE){\n  # Function body goes here\n}\n#' Calculate the geometric mean of a numeric vector\n#'\n#' @param x numeric vector\n#' @param remove_NA logical scalar, indicating whether NA values should be stripped before computation proceeds. \n#'\n#' @return the geometric mean of the values in `x`, a numeric scalar value. \n#'\n#' @examples\n#' geometric_mean(x = 1:10)\n#' geometric_mean(x = c(1:10, NA), remove_NA = TRUE)\n#' \ngeometric_mean <- function(x, remove_NA = FALSE){\n  # Function body goes here\n}"},{"path":"workflows-code.html","id":"checking-your-code","chapter":"4 Code","heading":"4.8.5 Checking Your Code","text":"write function, test .Testing code two main purposes:warn prevent user misuse (e.g. strange inputs),catch edge cases.top explaining functions work, really check work. job unit testing.Whenever write function test works intended . Additionally, test function robust misused user. Depending context, might accidental malicious misuse. Finally, check function behaves properly strange, still valid, inputs. known edge cases.Testing can bit brutal process, ’ve just created beautiful function now ’re job best break !","code":""},{"path":"workflows-code.html","id":"an-informal-testing-workflow","chapter":"4 Code","heading":"4.8.6 An Informal Testing Workflow","text":"Write functionExperiment function console, try break itFix break repeat.Problems: Time consuming reproducible.informal approach testing code might first write function play around console check behaves well give obvious inputs, edge cases deliberately wrong inputs. time manage break function, edit fix problem start process .testing code, informally. ’s record tried break code already. problem approach return code add new feature, ’ll probably forgotten least one informal tests ran first time around. goes efforts towards reproducibility automation. also makes easy break code used work just fine.","code":""},{"path":"workflows-code.html","id":"a-formal-testing-workflow","chapter":"4 Code","heading":"4.8.7 A Formal Testing Workflow","text":"can formalise testing workflow writing tests R script saving future reference. Remember first lecture saved tests/ directory, structure mirror src/ directory project. tests one function live single file, named function.One way writing tests use lots statements. testthat can syntactic heavy lifting us. lots helpful functions test output function expect.example, error function returns logical NA rather double NA. Yes, R really different types NA different types missing data, usually just handles nicely background .subtle difference probably something spotted , caused trouble much line. rigorous approach one benefits using testthat functions.fix test change expected output NA_real_.’ll revisit testthat package live session week, learn use test functions within packages.","code":"\ngeometric_mean <- function(x , remove_NA = FALSE){prod(x)^(1/length(x))}\ntestthat::expect_equal(\n  object = geometric_mean(x = c(1, NA), remove_NA = FALSE),\n  expected = NA)\n\n# Error: geometric_mean(x = c(1, NA), remove_NA = FALSE) not equal to NA.\n# Types not compatible: double is not logical\ntestthat::expect_equal(\n  object = geometric_mean(x = c(1,NA), remove_NA = FALSE),\n  expected = NA_real_)"},{"path":"workflows-code.html","id":"summary-1","chapter":"4 Code","heading":"4.9 Summary","text":"Functional Object Oriented ProgrammingStructuring scriptsStyling codeReduce, reuse, recycleDocumenting testingLet’s wrap summarising learned chapter.started discussion differences functional object oriented programming. R capable , data science work tends functional flavour .’ve described structure scripts style code make human-friendly easy debug possible.Finally, discussed write DRY code well documented tested.","code":""},{"path":"workflows-checklist.html","id":"workflows-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-checklist.html","id":"videos-chapters","chapter":"Checklist","heading":"4.10 Videos / Chapters","text":"Organising work (30 min) [slides]Naming Files (20 min) [slides]Organising code (27 min) [slides]\n","code":""},{"path":"workflows-checklist.html","id":"reading","chapter":"Checklist","heading":"4.11 Reading","text":"Use workflows section reading list support guide exploration week’s materials. Note texts divided core reading, reference materials materials interest.","code":""},{"path":"workflows-checklist.html","id":"tasks","chapter":"Checklist","heading":"4.12 Tasks","text":"Core:Find 3 data science projects Github explore organise work. Write post EdStem forum links three, couple paragraphs describe content structure one project.Find 3 data science projects Github explore organise work. Write post EdStem forum links three, couple paragraphs describe content structure one project.Create project directory (directories) course assignments.Create project directory (directories) course assignments.Write two R functions. first calculate geometric mean numeric vector. second calculate rolling arithmetic mean numeric vector.Write two R functions. first calculate geometric mean numeric vector. second calculate rolling arithmetic mean numeric vector.Bonus:Re-factor old project match project organisation coding guides course. might small research project, class notes collection homework assignments. Use R-based project possible. python projects, either translate R apply PEP8 style guide. Take care select suitably sized project meaningful exercise take hours.Re-factor old project match project organisation coding guides course. might small research project, class notes collection homework assignments. Use R-based project possible. python projects, either translate R apply PEP8 style guide. Take care select suitably sized project meaningful exercise take hours.able , host re-factored project publicly share rest class EdStem Discussion forum.able , host re-factored project publicly share rest class EdStem Discussion forum.","code":""},{"path":"workflows-checklist.html","id":"live-session","chapter":"Checklist","heading":"4.13 Live Session","text":"live session begin discussion week’s tasks. create minimal R package organise test functions written.Please come live session prepared discuss following points:make assignment projects subdirectories stand alone projects? ?make assignment projects subdirectories stand alone projects? ?terms met readings? find meanings?terms met readings? find meanings?consider writing rolling mean function?consider writing rolling mean function?","code":""},{"path":"data-introduction.html","id":"data-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.Data can difficult acquire gnarly get .raw material work data scientist , unsurprisingly, data. part course focus different ways data can stored, distributed obtained.able obtain read dataset often surprisingly large hurdle getting new data science project ground. skill able source read data many locations usually sanitised statistics programme: ’re given ready--go, cleaned CSV file focus placed modelling. week aims remedy equipping skills acquire manage data.begin week explore different file types. dictates type information can store, can access information read R. turn attention case data given directly. learn obtain data raw webpage request data via service known API.","code":""},{"path":"data-tabular.html","id":"data-tabular","chapter":"5 Tabular Data","heading":"5 Tabular Data","text":"Effective Data Science still work--progress. chapter undergoing heavy restructuring may confusing incomplete.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-tabular.html","id":"loading-tabular-data","chapter":"5 Tabular Data","heading":"5.1 Loading Tabular Data","text":"Recall simpler, open source formats improve accessibility reproducibility. begin reading three open data formats tabular data.random-data.csvrandom-data.csvrandom-data.tsvrandom-data.tsvrandom-data.txtrandom-data.txtEach data sets contains 26 observations 4 variables:id, Roman letter identifier;gaussian, standard normal random variates;gamma, gamma(1,1) random variates;uniform, uniform(0,1) random variates.","code":""},{"path":"data-tabular.html","id":"base-r","chapter":"5 Tabular Data","heading":"5.1.1 Base R","text":"Output data.frame object. (List vectors nice methods)","code":"\nrandom_df <- read.csv(file = 'random-data.csv')\nprint(random_df)\n#>    id    gaussian      gamma    uniform\n#> 1   a -1.20706575 0.98899970 0.22484576\n#> 2   b  0.27742924 0.03813386 0.08498474\n#> 3   c  1.08444118 1.09462335 0.63729826\n#> 4   d -2.34569770 1.49301101 0.43101637\n#> 5   e  0.42912469 5.40361248 0.07271609\n#> 6   f  0.50605589 1.72386539 0.80240202\n#> 7   g -0.57473996 1.95357133 0.32527830\n#> 8   h -0.54663186 0.07807803 0.75728904\n#> 9   i -0.56445200 0.21198194 0.58427152\n#> 10  j -0.89003783 0.20803673 0.70883941\n#> 11  k -0.47719270 2.08607862 0.42697577\n#> 12  l -0.99838644 0.49463708 0.34357270\n#> 13  m -0.77625389 0.77171305 0.75911999\n#> 14  n  0.06445882 0.37216648 0.42403021\n#> 15  o  0.95949406 1.88207991 0.56088725\n#> 16  p -0.11028549 0.76622568 0.11613577\n#> 17  q -0.51100951 0.50488585 0.30302180\n#> 18  r -0.91119542 0.22979791 0.47880269\n#> 19  s -0.83717168 0.75637275 0.34483055\n#> 20  t  2.41583518 0.62435969 0.60071414\n#> 21  u  0.13408822 0.64638373 0.07608332\n#> 22  v -0.49068590 0.11247545 0.95599261\n#> 23  w -0.44054787 0.11924307 0.02220682\n#> 24  x  0.45958944 4.91805535 0.84171063\n#> 25  y -0.69372025 0.60282666 0.63244245\n#> 26  z -1.44820491 0.64446571 0.31009417"},{"path":"data-tabular.html","id":"readr","chapter":"5 Tabular Data","heading":"5.1.2 {readr}","text":"Output tibble object. (List vectors nicer methods)","code":"\nrandom_tbl <- readr::read_csv(file = 'random-data.csv')\n#> Rows: 26 Columns: 4\n#> ── Column specification ─────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): id\n#> dbl (3): gaussian, gamma, uniform\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nprint(random_tbl)\n#> # A tibble: 26 × 4\n#>   id    gaussian  gamma uniform\n#>   <chr>    <dbl>  <dbl>   <dbl>\n#> 1 a       -1.21  0.989   0.225 \n#> 2 b        0.277 0.0381  0.0850\n#> 3 c        1.08  1.09    0.637 \n#> 4 d       -2.35  1.49    0.431 \n#> 5 e        0.429 5.40    0.0727\n#> 6 f        0.506 1.72    0.802 \n#> # … with 20 more rows"},{"path":"data-tabular.html","id":"benefits-of-readrread_csv","chapter":"5 Tabular Data","heading":"5.1.2.1 Benefits of readr::read_csv()","text":"Increased speed (approx. 10x) progress bar.Increased speed (approx. 10x) progress bar.Strings coerced factors. stringsAsFactors = FALSEStrings coerced factors. stringsAsFactors = FALSENo row names nice column names.row names nice column names.Reproducibility bonus: depend operating system.Reproducibility bonus: depend operating system.","code":""},{"path":"data-tabular.html","id":"wtf-tibbles","chapter":"5 Tabular Data","heading":"5.1.3 WTF: Tibbles","text":"","code":""},{"path":"data-tabular.html","id":"printing","chapter":"5 Tabular Data","heading":"5.1.3.1 Printing","text":"Default first 10 rows many columns comfortably fit screen.Default first 10 rows many columns comfortably fit screen.Can adjust behaviour print call:Can adjust behaviour print call:Bonus: Colour formatting IDE column tells ’s type.","code":"\n# print first three rows and all columns\nprint(random_tbl, n = 3, width = Inf)\n#> # A tibble: 26 × 4\n#>   id    gaussian  gamma uniform\n#>   <chr>    <dbl>  <dbl>   <dbl>\n#> 1 a       -1.21  0.989   0.225 \n#> 2 b        0.277 0.0381  0.0850\n#> 3 c        1.08  1.09    0.637 \n#> # … with 23 more rows"},{"path":"data-tabular.html","id":"subsetting","chapter":"5 Tabular Data","heading":"5.1.3.2 Subsetting","text":"Subsetting tibbles always return another tibble.helps avoids edge cases associated working data frames.","code":"\n# Row Subsetting\nrandom_tbl[1, ] # returns tibble\nrandom_df[1, ]  # returns data.frame\n\n# Column Subsetting\nrandom_tbl[ , 1]      # returns tibble\nrandom_df[ , 1]       # returns vector\n\n# Combined Subsetting\nrandom_tbl[1, 1]      # returns 1x1 tibble\nrandom_df[1, 1]       # returns single value"},{"path":"data-tabular.html","id":"other-readr-functions","chapter":"5 Tabular Data","heading":"5.1.4 Other {readr} functions","text":"See readr documentation, lots useful additional arguments can help reading messy data.Functions reading writing types tabular data work analogously.","code":""},{"path":"data-tabular.html","id":"reading-tabular-data","chapter":"5 Tabular Data","heading":"5.1.4.1 Reading Tabular Data","text":"","code":"\nlibrary(readr)\nread_tsv(\"random-data.tsv\")\nread_delim(\"random-data.txt\", delim = \" \")"},{"path":"data-tabular.html","id":"writing-tabular-data","chapter":"5 Tabular Data","heading":"5.1.4.2 Writing Tabular Data","text":"","code":"\nwrite_csv(random_tbl, \"random-data-2.csv\")\nwrite_tsv(random_tbl, \"random-data-2.tsv\")\nwrite_delim(random_tbl, \"random-data-2.tsv\", delim = \" \")"},{"path":"data-tabular.html","id":"need-for-speed","chapter":"5 Tabular Data","heading":"5.1.5 Need for Speed","text":"times load lots large data sets, case 10x speed-might sufficient.data set still fits inside RAM, check data.table::fread() optimised speed. (Alternatives exist optimal memory usage data large working memory, covered .)Note: can much faster, resulting data.table object lacks consistancy properties tibble sure check edge cases, returned value might expect.","code":""},{"path":"data-tabular.html","id":"tidy-data","chapter":"5 Tabular Data","heading":"5.2 Tidy Data","text":"","code":""},{"path":"data-tabular.html","id":"wide-vs.-tall-data","chapter":"5 Tabular Data","heading":"5.2.1 Wide vs. Tall Data","text":"","code":""},{"path":"data-tabular.html","id":"wide-data","chapter":"5 Tabular Data","heading":"5.2.1.1 Wide Data","text":"First column unique entriesFirst column unique entriesEasier humans read compute onEasier humans read compute onHarder machines compute onHarder machines compute ","code":""},{"path":"data-tabular.html","id":"tall-data","chapter":"5 Tabular Data","heading":"5.2.1.2 Tall Data","text":"First column repeating entriesFirst column repeating entriesHarder humans read compute onHarder humans read compute onEasier machines compute onEasier machines compute ","code":""},{"path":"data-tabular.html","id":"examples","chapter":"5 Tabular Data","heading":"5.2.1.3 Examples","text":"Example 1 (Wide)Example 1 (Tall)[Source: Wikipedia - Wide narrow data]Example 2 (Wide)Example 2 (Tall)[Source: Statology - Long vs wide data]","code":""},{"path":"data-tabular.html","id":"pivoting-wider-and-longer","chapter":"5 Tabular Data","heading":"5.2.1.4 Pivoting Wider and Longer","text":"Error control input analysis format-dependent.Error control input analysis format-dependent.Switching long wide formats useful control errors.Switching long wide formats useful control errors.Easy tidyr package functionsEasy tidyr package functions","code":"\ntidyr::pivot_longer()\ntidyr::pivot_wider()"},{"path":"data-tabular.html","id":"tidy-what","chapter":"5 Tabular Data","heading":"5.2.2 Tidy What?","text":"[Image: R4DS - Chapter 12]Tidy Data opinionated way store tabular data.Image Source: Chapter 12 R Data Science.column corresponds exactly one measured variableEach row corresponds exactly one observational unitEach cell contains exactly one value.Benefits tidy dataConsistent data format: Reduces cognitive load allows specialised tools (functions) efficiently work tabular data.Consistent data format: Reduces cognitive load allows specialised tools (functions) efficiently work tabular data.Vectorisation: Keeping variables columns allows efficient data manipulation. (goes back data frames tibbles lists vectors)Vectorisation: Keeping variables columns allows efficient data manipulation. (goes back data frames tibbles lists vectors)","code":""},{"path":"data-tabular.html","id":"example---tidy-longer","chapter":"5 Tabular Data","heading":"5.2.3 Example - Tidy Longer","text":"Consider trying plot data time series. year variable trapped column names!tidy data, need pivot_longer(). turn column names new year variable retaining cell contents new variable called cases.Much better!","code":"\ncountries\n#> # A tibble: 3 × 3\n#>   country     `1999` `2000`\n#>   <chr>        <dbl>  <dbl>\n#> 1 Afghanistan    745   2666\n#> 2 Brazil       37737  80488\n#> 3 China       212258 213766\nlibrary(magrittr)\ncountries %>% \n  tidyr::pivot_longer(cols = c(`1999`,`2000`), names_to = \"year\", values_to = \"cases\")\n#> # A tibble: 6 × 3\n#>   country     year   cases\n#>   <chr>       <chr>  <dbl>\n#> 1 Afghanistan 1999     745\n#> 2 Afghanistan 2000    2666\n#> 3 Brazil      1999   37737\n#> 4 Brazil      2000   80488\n#> 5 China       1999  212258\n#> 6 China       2000  213766"},{"path":"data-tabular.html","id":"example---tidy-wider","chapter":"5 Tabular Data","heading":"5.2.4 Example - Tidy Wider","text":"times might widen data tidy .example tidy. ?observational unit team. However, variable stored separate column, cells containing values.tidy data first generate tibble. use tribble() function, allows us create tibble row-wise rather column-wise.can tidy creating new columns value current Variable column taking values current Value column.","code":"\ntournament <- tibble::tribble(\n~Team  , ~Variable , ~Value,\n\"A\"    , \"Points\"  , 88    ,\n\"A\"    , \"Assists\" , 12    ,\n\"A\"    , \"Rebounds\", 22    ,\n\"B\"    , \"Points\"  , 91    ,\n\"B\"    , \"Assists\" , 17    ,\n\"B\"    , \"Rebounds\", 28    ,\n\"C\"    , \"Points\"  , 99    ,\n\"C\"    , \"Assists\" , 24    ,\n\"C\"    , \"Rebounds\", 30    ,\n\"D\"    , \"Points\"  , 94    ,\n\"D\"    , \"Assists\" , 28    ,\n\"D\"    , \"Rebounds\", 31    )\ntournament %>% \n  tidyr::pivot_wider(\n    id_cols = \"Team\", \n    names_from = \"Variable\",\n    values_from = \"Value\")\n#> # A tibble: 4 × 4\n#>   Team  Points Assists Rebounds\n#>   <chr>  <dbl>   <dbl>    <dbl>\n#> 1 A         88      12       22\n#> 2 B         91      17       28\n#> 3 C         99      24       30\n#> 4 D         94      28       31"},{"path":"data-tabular.html","id":"other-helpful-functions","chapter":"5 Tabular Data","heading":"5.2.5 Other helpful functions","text":"pivot_*() family functions resolve issues rows (many observations per row rows per observation).similar helper functions solve column issues:Multiple variables per column: tidyr::separate(),Multiple variables per column: tidyr::separate(),Multiple columns per variable: tidyr::unite().Multiple columns per variable: tidyr::unite().","code":""},{"path":"data-tabular.html","id":"missing-data","chapter":"5 Tabular Data","heading":"5.2.6 Missing Data","text":"tidy data, every cell contains value. Including cells missing values.Missing values coded NA (generic) type-specific NA, NA_character_.Missing values coded NA (generic) type-specific NA, NA_character_.readr family read_*() function good defaults helpful na argument.readr family read_*() function good defaults helpful na argument.Explicitly code NA values collecting data, avoid ambiguity: ” “, -999 worst 0.Explicitly code NA values collecting data, avoid ambiguity: ” “, -999 worst 0.missing values EDA videos…missing values EDA videos…","code":""},{"path":"data-tabular.html","id":"wrapping-up-1","chapter":"5 Tabular Data","heading":"5.3 Wrapping Up","text":"Reading tabular data range methodsReading tabular data range methodsIntroduced tibble tidy data (+ tidy always best)Introduced tibble tidy data (+ tidy always best)Tools tidying messy tabular dataTools tidying messy tabular data","code":""},{"path":"data-webscraping.html","id":"data-webscraping","chapter":"6 Web Scraping","heading":"6 Web Scraping","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-webscraping.html","id":"scraping-webpage-data-using-rvest","chapter":"6 Web Scraping","heading":"6.1 Scraping webpage data using {rvest}","text":"can’t always rely tidy, tabular data land desk. Sometimes going go gather data .’m suggesting need manually, likely need get data internet ’s made publicly privately available .might information webpage gather , data shared collaborator using API.chapter cover basics scraping webpages, following vignette {rvest} package.","code":""},{"path":"data-webscraping.html","id":"what-is-a-webpage","chapter":"6 Web Scraping","heading":"6.2 What is a webpage?","text":"can even hope get data webpage, first need understand webpage .Webpages written similar way LaTeX: content styling webpages handled separately coded using plain text files.fact, websites go one step LaTeX. content styling websites written different files different languages. HTML (HyperText Markup Language) used write content CSS (Cascading Style Sheets) used control appearance content ’s displayed user.","code":""},{"path":"data-webscraping.html","id":"html","chapter":"6 Web Scraping","heading":"6.3 HTML","text":"basic HTML page styling applied might look something like :","code":"<html>\n<head>\n  <title>Page title<\/title>\n<\/head>\n<body>\n  <h1 id='first'>A level 1 heading<\/h1>\n  <p>Hello World!<\/p>\n  <p>Here is some plain text &amp; <b>some bold text.<\/b><\/p>\n  <img src='myimg.png' width='100' height='100'>\n<\/body>"},{"path":"data-webscraping.html","id":"html-elements","chapter":"6 Web Scraping","heading":"6.3.1 HTML elements","text":"Just like XML data files, HTML hierarchical structure. structure crafted using HTML elements. HTML element made start tag, optional attributes, end tag.can see first level header, <h1> opening tag, id='first' additional attribute <\/h1> closing tag. Everything opening closing tag contents element. also special elements consist single tag optional attributes. example <img> tag.Since < > used start end tags, can’t write directly HTML document. Instead, use escape characters. sounds fancy, ’s just alternative way write characters serve special function within language.can write greater &gt; less &lt;. might notice escapes use ampersand (&). means want literal ampersand webpage, escape using &amp;.wide range possible HTML tags escapes. ’ll cover common tags lecture don’t need worry escapes much rvest automatically handle .","code":""},{"path":"data-webscraping.html","id":"important-html-elements","chapter":"6 Web Scraping","heading":"6.3.2 Important HTML Elements","text":", excess 100 HTML elements. important ones know :<html> element, must enclose every HTML page. <html> element must two child elements within . <head> element contains metadata document, like page title shown browser tab CSS style sheet applied. <body> element contains content see browser.<html> element, must enclose every HTML page. <html> element must two child elements within . <head> element contains metadata document, like page title shown browser tab CSS style sheet applied. <body> element contains content see browser.Block elements used give structure page. elements like headings, sub-headings <h1> way <h6>. category also contains paragraph elements <p>, ordered lists <ol> unordered lists <ul>.Block elements used give structure page. elements like headings, sub-headings <h1> way <h6>. category also contains paragraph elements <p>, ordered lists <ol> unordered lists <ul>.Finally, inline tags like <b> bold, <> italics, <> hyperlinks used format text inside block elements.Finally, inline tags like <b> bold, <> italics, <> hyperlinks used format text inside block elements.come across tag ’ve never seen , can find just little bit googling. good resource MDN Web Docs produced Mozilla, company makes Firefox web browser. W3schools website another great resource web development coding resources generally.","code":""},{"path":"data-webscraping.html","id":"html-attributes","chapter":"6 Web Scraping","heading":"6.4 HTML Attributes","text":"’ve seen one example header additional attribute. generally, tags can named attributes. attributes contained within opening tag look something like:Two important attributes id class. attributes used conjunction CSS file control visual appearance page. often useful identify elements interested scraping data page.","code":"<tag attribute1='value1' attribute2='value2'>element contents<\/tag>"},{"path":"data-webscraping.html","id":"css-selectors","chapter":"6 Web Scraping","heading":"6.5 CSS Selectors","text":"Cascading Style Sheet used describe HTML content displayed. , CSS ’s system selecting elements webpage, called CSS selectors.CSS selectors define patterns locating HTML elements particular style applied . happy side-effect can sometimes useful scraping, provide concise way describing elements want extract.CSS Selectors can work level element type, class, tag can used nested (cascading) way.p selector select paragraph <p> elements.p selector select paragraph <p> elements..title selector select elements class “title”..title selector select elements class “title”.p.special selector select <p> elements class “special”.p.special selector select <p> elements class “special”.#title selector select element id attribute “title”.#title selector select element id attribute “title”.want select single element id attributes particularly useful must unique within html document. Unfortunately, helpful developer added id attribute element(s) want scrape!want learn CSS selectors recommend starting fun CSS dinner tutorial build base knowledge using W3schools resources reference explore webpages wild.","code":""},{"path":"data-webscraping.html","id":"which-attributes-and-selectors-do-you-need","chapter":"6 Web Scraping","heading":"6.6 Which Attributes and Selectors Do You Need?","text":"scrape data webpage, first identify tag attribute combinations interested gathering.find elements interest, three options. go hardest easiest also least robust.right click + “inspect page source” (F12)right click + “inspect”Rvest Selector Gadget (useful fallible)Inspecting source familiar websites can useful way get head around concepts. Beware though sophisticated webpages can quite intimidating. good place start simpler, static websites personal websites, rather dynamic webpages online retailers social media platforms.","code":""},{"path":"data-webscraping.html","id":"reading-html-with-rvest","chapter":"6 Web Scraping","heading":"6.7 Reading HTML with {rvest}","text":"rvest, reading html page can simple loading tabular data.class resulting object xml_document. type object low-level package xml2, allows read xml files R.can see object split several components: first metadata type document scraped, followed head body html document.several possible approaches extracting information document.","code":"\nhtml <- rvest::read_html(\"https://www.zakvarty.com/professional/teaching.html\")\nclass(html)\n#> [1] \"xml_document\" \"xml_node\"\nhtml\n#> {html_document}\n#> <html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#> [2] <body class=\"nav-fixed\">\\n\\n<div id=\"quarto-search-results\"><\/div>\\n   ..."},{"path":"data-webscraping.html","id":"extracting-html-elements","chapter":"6 Web Scraping","heading":"6.8 Extracting HTML elements","text":"rvest can extract single element html_element(), matching elements html_elements(). functions take document object one CSS selectors inputs.can also combine nest selectors. example might want extract links within paragraphs second level headers.","code":"\nlibrary(rvest)\n\nhtml %>% html_elements(\"h1\")\n#> {xml_nodeset (1)}\n#> [1] <h1>Teaching<\/h1>\nhtml %>% html_elements(\"h2\")\n#> {xml_nodeset (2)}\n#> [1] <h2 id=\"toc-title\">On this page<\/h2>\n#> [2] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History<\/h2>\nhtml %>% html_elements(\"p\")\n#> {xml_nodeset (7)}\n#> [1] <p>I am fortunate to have had the opportunity to teach in a variety of ...\n#> [2] <p>Developing and teaching a number of modules in statistics, data sci ...\n#> [3] <p>Supervising undergraduate, postgraduate and doctoral research proje ...\n#> [4] <p>Adapting and leading short courses on scientific writing and commun ...\n#> [5] <p>Running workshops and computer labs for undergraduate and postgradu ...\n#> [6] <p>Speaking at univerisity open days and providing one-to-one tuition  ...\n#> [7] <p>I am an associate fellow of the Higher Education Academy, which you ...\nhtml %>% html_elements(\"p a,h2\")\n#> {xml_nodeset (3)}\n#> [1] <h2 id=\"toc-title\">On this page<\/h2>\n#> [2] <a href=\"https://www.advance-he.ac.uk/fellowship/associate-fellowship\" ...\n#> [3] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History<\/h2>"},{"path":"data-webscraping.html","id":"extracting-data-from-html-elements","chapter":"6 Web Scraping","heading":"6.9 Extracting Data From HTML Elements","text":"Now ’ve got elements care extracted complete document. get data need elements?’ll usually get data either contents HTML element else one ’s attributes. ’re really lucky, data need already formatted HTML table list.","code":""},{"path":"data-webscraping.html","id":"extracting-text","chapter":"6 Web Scraping","heading":"6.9.1 Extracting text","text":"functions rvest::html_text() rvest::html_text2() can used extract plain text contents HTML element.difference html_text() html_text2() handle whitespace. HTML whitespace line breaks little influence code interpreted computer (similar R different Python). html_text() extract text raw html, html_text2() best extract text way gives something similar ’d see browser.","code":"\nhtml %>% \n  html_elements(\"#teaching li\") %>% \n  html_text2()\n#> [1] \"Developing and teaching a number of modules in statistics, data science and data ethics. These were predominantly at the postgradute-level and include courses designed for in-person and remote learning.\"\n#> [2] \"Supervising undergraduate, postgraduate and doctoral research projects.\"                                                                                                                                   \n#> [3] \"Adapting and leading short courses on scientific writing and communication.\"                                                                                                                               \n#> [4] \"Running workshops and computer labs for undergraduate and postgraduate modules.\"                                                                                                                           \n#> [5] \"Speaking at univerisity open days and providing one-to-one tuition to high school students.\""},{"path":"data-webscraping.html","id":"extracting-attributes","chapter":"6 Web Scraping","heading":"6.9.2 Extracting Attributes","text":"Attributes also used record information might like collect. example, destination links stored href attribute source images stored src attribute.example , consider trying extract twitter link icon page footer. quite tricky locate html source, used Selector Gadget help find correct combination elements.extract href attribute scraped element, use rvest::html_attr() function.Note: rvest::html_attr() always return character string (list character strings). extracting attribute describes quantity, width image, ’ll need convert string required data type. example, width measures pixels might use .integer().","code":"\nhtml %>% html_element(\".compact:nth-child(1) .nav-link\")\n#> {html_node}\n#> <a class=\"nav-link\" href=\"https://www.twitter.com/zakvarty\">\n#> [1] <i class=\"bi bi-twitter\" role=\"img\">\\n<\/i>\nhtml %>% \n  html_elements(\".compact:nth-child(1) .nav-link\") %>% \n  html_attr(\"href\")\n#> [1] \"https://www.twitter.com/zakvarty\""},{"path":"data-webscraping.html","id":"extracting-tables","chapter":"6 Web Scraping","heading":"6.9.3 Extracting tables","text":"HTML tables composed similar, nested manner LaTeX tables.four main elements know make HTML table:<table>,<tr> (table row),<th> (table heading),<td> (table data).’s simple example data, formatted HTML table:Since tables common way store data, rvest includes useful function html_table() converts directly HTML table tibble.Applying real scraped data can easily extract table taught courses.","code":"\nhtml_2 <- minimal_html(\"\n  <table>\n    <tr>\n      <th>Name<\/th>\n      <th>Number<\/th>\n    <\/tr>\n    <tr>\n      <td>A<\/td>\n      <td>1<\/td>\n    <\/tr>\n    <tr>\n      <td>B<\/td>\n      <td>2<\/td>\n    <\/tr>\n    <tr>\n      <td>C<\/td>\n      <td>3<\/td>\n    <\/tr>\n  <\/table>\n  \")\nhtml_2 %>% \n  html_element(\"table\") %>% \n  html_table()\n#> # A tibble: 3 × 2\n#>   Name  Number\n#>   <chr>  <int>\n#> 1 A          1\n#> 2 B          2\n#> 3 C          3\nhtml %>% \n  html_element(\"table\") %>% \n  html_table()\n#> # A tibble: 31 × 3\n#>   Year      Course                                     Role    \n#>   <chr>     <chr>                                      <chr>   \n#> 1 \"2022-23\" Data Science                               Lecturer\n#> 2 \"\"        Ethics in Data Science I, II and III       Lecturer\n#> 3 \"\"        Data Ethics for Digital Chemistry          Lecturer\n#> 4 \"\"        Y1 research projects: point process models Lecturer\n#> 5 \"2021-22\" Supervised Learning                        Lecturer\n#> 6 \"\"        Ethics in Data Science I                   Lecturer\n#> # … with 25 more rows"},{"path":"data-webscraping.html","id":"tip-for-building-tibbles","chapter":"6 Web Scraping","heading":"6.10 Tip for Building Tibbles","text":"scraping data webpage, end-goal typically going constructing data.frame tibble.following description tidy data, ’ll want row correspond repeated unit HTML page. case, shouldUse html_elements() select elements contain observation unit;Use html_element() extract variables observations.Taking approach guarantees ’ll get number values variable, html_element() always returns number outputs inputs. vital missing data - every observation unit value every variable interest.example, consider extract text starwars dataset.unordered list list item corresponds one observational unit (one character starwars universe). name character given bold, character species specified italics weight character denoted .weight class. However, characters subset variables defined: example Yoda species entry.try extract element directly, vectors variable values different lengths. don’t know missing values , can’t line back make tibble.instead start extracting list item elements using html_elements(). done , can use html_element() extract variable characters. pad NAs, can collate tibble.","code":"\nstarwars_html <- minimal_html(\"\n  <ul>\n    <li><b>C-3PO<\/b> is a <i>droid<\/i> that weighs <span class='weight'>167 kg<\/span><\/li>\n    <li><b>R2-D2<\/b> is a <i>droid<\/i> that weighs <span class='weight'>96 kg<\/span><\/li>\n    <li><b>Yoda<\/b> weighs <span class='weight'>66 kg<\/span><\/li>\n    <li><b>R4-P17<\/b> is a <i>droid<\/i><\/li>\n  <\/ul>\n  \")\nstarwars_html %>% html_elements(\"b\") %>% html_text2()\n#> [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_html %>% html_elements(\"i\") %>% html_text2()\n#> [1] \"droid\" \"droid\" \"droid\"\nstarwars_html %>% html_elements(\".weight\") %>% html_text2()\n#> [1] \"167 kg\" \"96 kg\"  \"66 kg\"\nstarwars_characters <- starwars_html %>% html_elements(\"li\")\n\nstarwars_characters %>% html_element(\"b\") %>% html_text2()\n#> [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_characters %>% html_element(\"i\") %>% html_text2()\n#> [1] \"droid\" \"droid\" NA      \"droid\"\nstarwars_characters %>% html_element(\".weight\") %>% html_text2()\n#> [1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA\ntibble::tibble(\n  name = starwars_characters %>% html_element(\"b\") %>% html_text2(),\n  species = starwars_characters %>% html_element(\"i\") %>% html_text2(),\n  weight = starwars_characters %>% html_element(\".weight\") %>% html_text2()\n)\n#> # A tibble: 4 × 3\n#>   name   species weight\n#>   <chr>  <chr>   <chr> \n#> 1 C-3PO  droid   167 kg\n#> 2 R2-D2  droid   96 kg \n#> 3 Yoda   <NA>    66 kg \n#> 4 R4-P17 droid   <NA>"},{"path":"data-apis.html","id":"data-apis","chapter":"7 APIs","heading":"7 APIs","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-apis.html","id":"aquiring-data-via-an-api","chapter":"7 APIs","heading":"7.1 Aquiring Data Via an API","text":"’ve already established can’t always rely tidy, tabular data land desk.Sometimes going go gather data . already seen scrape information directly HTML source webpage. surely easer way. Thankfully, often !chapter cover basics obtaining data via API. material draws together Introduction APIs book Brian Cooksey DIY web data section STAT545 University British Columbia.","code":""},{"path":"data-apis.html","id":"why-do-i-need-to-know-about-apis","chapter":"7 APIs","heading":"7.2 Why do I need to know about APIs?","text":"API, application programming interface, set rules allows different software applications communicate .data scientist, often need access data stored remote servers cloud-based services. APIs provide convenient way data scientists programmatically retrieve data, without manually download data sets process locally computer.multiple benefits including automation standardisation data sharing.Automation: much faster machine process data request human. machine handling data requests also scales much better either number complexity data requests grows. Additionally, lower risk introducing human error. example, human might accidentally share wrong data, can serious legal repercussions.Automation: much faster machine process data request human. machine handling data requests also scales much better either number complexity data requests grows. Additionally, lower risk introducing human error. example, human might accidentally share wrong data, can serious legal repercussions.Standardisation: machine process data requests requires format requests associated responses standardised. allows data sharing retrieval become reproducible programmatic aspect work.Standardisation: machine process data requests requires format requests associated responses standardised. allows data sharing retrieval become reproducible programmatic aspect work.","code":""},{"path":"data-apis.html","id":"what-is-an-api","chapter":"7 APIs","heading":"7.3 What is an API?","text":", APIs great, exactly ?human--human communication, set rules governing acceptable behaviour known etiquette. Depending live, social etiquette can rather strict. rules computer--computer communication take whole new level, machines can room left interpretation.set rules governing interactions computers programmes known protocol.APIs provide standard protocol different programs interact one another. makes easier developers build complex systems leveraging functionality existing services platforms. benefits working standardised modular way apply equally well sharing data writing code organising files.two sides communication machines communicate known server client.Servers can seem intimidating, unlike laptop mobile phone don’t input output devices; keyboard, monitor, mouse. Despite , servers just regular computers designed store data run programmes. Servers don’t input output devices intended used remotely, via another computer. need screen mouse user miles away. Nothing scary going !People often find clients much less intimidating - simply computer application might contact sever.","code":""},{"path":"data-apis.html","id":"http","chapter":"7 APIs","heading":"7.4 HTTP","text":"leads us one step rabbit-hole. API protocol defines rules applications communicate one another. communication happen?HTTP (Hypertext Transfer Protocol) dominant mode communication World Wide Web. can see secure version HTTP, HTTPS, start web addresses top browser. example:HTTP foundation data communication web used transfer files (text, images, videos) web servers clients.understand HTTP communications, find helpful imagine client server customer waiter restaurant. client makes request server, tries comply giving response. server might respond confirm request completed successfully. Alternatively, server might respond error message, (hopefully) informative request completed.request-response model basis HTTP, communication system used majority APIs.","code":"https://www.zakvarty.com/blog"},{"path":"data-apis.html","id":"http-requests","chapter":"7 APIs","heading":"7.5 HTTP Requests","text":"HTTP request consists :Uniform Resource Locator (URL) [unique identifier thing]Method [tells server type action requested client]Headers [meta-information request, e.g. device type]Body [Data client wants send server]","code":""},{"path":"data-apis.html","id":"url","chapter":"7 APIs","heading":"7.5.1 URL","text":"URL HTTP request specifies request going made, example http://example.com.","code":""},{"path":"data-apis.html","id":"method","chapter":"7 APIs","heading":"7.5.2 Method","text":"action client wants take indicated set well-defined methods HTTP verbs. common HTTP verbs GET, POST, PUT, PATCH, DELETE.GET verb used retrieve resource server, web page image. POST verb used send data server, submitting form uploading file. PUT verb used replace resource server new one, PATCH verb used update resource server without replacing entirely. Finally, DELETE verb used delete resource server.addition common HTTP verbs, also several less frequently used verbs. used specialized purposes, requesting headers resource, testing connectivity client server.","code":""},{"path":"data-apis.html","id":"header","chapter":"7 APIs","heading":"7.5.3 Header","text":"request headers contain meta-information request. information device type included within request.","code":""},{"path":"data-apis.html","id":"body","chapter":"7 APIs","heading":"7.5.4 Body","text":"Finally, body request contains data client providing server.","code":""},{"path":"data-apis.html","id":"http-responses","chapter":"7 APIs","heading":"7.6 HTTP Responses","text":"server receives request attempt fulfil send response back client.response similar structure request apart :responses URL,responses method,responses status code.","code":""},{"path":"data-apis.html","id":"status-codes","chapter":"7 APIs","heading":"7.6.1 Status Codes","text":"status code 3 digit number, specific meaning. common error codes might (already ) come across :200: Success,404: Page found (400s errors),503: Page .data science context, successful response return requested data within data field. likely given JSON XML format.","code":""},{"path":"data-apis.html","id":"authentication","chapter":"7 APIs","heading":"7.7 Authentication","text":"Now know applications communicate, might ask can control access API types request can make. can done server setting appropriate permissions client. server verify client really claims ?Authentication way ensure authorized clients able access API. typically done server requiring client provide secret information uniquely identifies , whenever make requests API. information allows API server validate authenticity user authorises request.","code":""},{"path":"data-apis.html","id":"basic-authentication","chapter":"7 APIs","heading":"7.7.1 Basic Authentication","text":"various ways implement API authentication.Basic authentication involves legitimate client username password. encrypted version included Authorization header HTTP request. hear matches server’s records request processed. , special status code (401) returned client.Basic authentication dangerous put restrictions client can authorised. Additional, individualised restrictions can added using alternative authentication scheme.","code":""},{"path":"data-apis.html","id":"api-key-authentication","chapter":"7 APIs","heading":"7.7.2 API Key Authentication","text":"API key long, random string letters numbers assigned authorised user. API key distinct user’s password keys typically issued service provides API. Using keys rather basic authentication allows API provider track limit usage API.example, provider may issue unique API key developer organization wants use API. provider can limit access certain data. also limit number requests key can make given time period prevent access certain administrative functions, like changing passwords deleting accounts.Unlike Basic Authentication, standard way client sharing key server. Depending API might Authorization field header, end URL (http://example.com?api_key=my_secret_key), within body data.","code":""},{"path":"data-apis.html","id":"api-wrappers","chapter":"7 APIs","heading":"7.8 API wrappers","text":"’ve learned lot internet works. Fortunately, lot time won’t worry new information debugging purposes.best case scenario, kind developer written “wrapper” function API. wrappers functions R construct HTTP request . particularly lucky, API wrapper also format response , converting XML JSON back R object ready immediate use.","code":""},{"path":"data-apis.html","id":"geonames-wrapper","chapter":"7 APIs","heading":"7.9 {geonames} wrapper","text":"rOpenSci curated list many wrappers accessing scientific data using R. focus GeoNames API, gives open access geographical database. access data, use wrapper functions provided {geonames} package.aim illustrate important steps getting started new API.","code":""},{"path":"data-apis.html","id":"set-up","chapter":"7 APIs","heading":"7.9.1 Set up","text":"can get data GeoNames API, first need little bit set .Install load {geonames} CRANCreate user account GeoNames APIActivate account (see activation email)Enable free web services GeoNames account logging link.Enable free web services GeoNames account logging link.Tell R credentials GeoNames.Tell R credentials GeoNames.use following code tell R credentials, absolutely .save username environment variable, also puts API credentials directly script. share script others (internally, externally publicly) sharing credentials . good!","code":"\n#install.packages(\"geonames\")\nlibrary(geonames)\noptions(geonamesUsername=\"example_username\")"},{"path":"data-apis.html","id":"keep-it-secret-keep-it-safe","chapter":"7 APIs","heading":"7.9.2 Keep it Secret, Keep it Safe","text":"solution problem add credentials environment variables .Rprofile rather script. .Rprofile R script run start every session. can created edited directly, can also created edited within R.make/open .Rprofile use edit_r_profile() function usethis package.Within file, add options(geonamesUsername=\"example_username\") new line, remembering replace example_username GeoNames username.final step check file ends blank line, save restart R. set start using {geonames}.set procedure indicative API wrappers, course details vary API. good documentation important!","code":"\nlibrary(usethis)\nusethis::edit_r_profile()"},{"path":"data-apis.html","id":"using-geonames","chapter":"7 APIs","heading":"7.9.3 Using {geonames}","text":"GeoNames whole host different geo-datasets can explore.\nfirst example, let’s get geo-tagged wikipedia articles within 1km Imperial College London.Looking structure imperial_neighbours can see data frame one row per geo-tagged wikipedia article.confirm correct location can inspect title first five neighbours.Nothing surprising , mainly departments college Exhibition Road, runs along one side campus. sorts check important - initially forgot minus longitude getting results East London!","code":"\nimperial_coords <- list(lat = 51.49876, lon = -0.1749)\nsearch_radius_km <- 1\n\nimperial_neighbours <- geonames::GNfindNearbyWikipedia(\n  lat = imperial_coords$lat,\n  lng = imperial_coords$lon, \n  radius = search_radius_km,\n  lang = \"en\",                # english language articles\n  maxRows = 500              # maximum number of results to return \n)\nstr(imperial_neighbours)\n#> 'data.frame':    204 obs. of  13 variables:\n#>  $ summary     : chr  \"The Department of Mechanical Engineering is responsible for teaching and research in mechanical engineering at \"| __truncated__ \"Imperial College Business School is a global business school located in London. The business school was opened \"| __truncated__ \"Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est\"| __truncated__ \"Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one\"| __truncated__ ...\n#>  $ elevation   : chr  \"20\" \"18\" \"19\" \"24\" ...\n#>  $ feature     : chr  \"edu\" \"edu\" \"landmark\" \"edu\" ...\n#>  $ lng         : chr  \"-0.1746\" \"-0.1748\" \"-0.17425\" \"-0.1757\" ...\n#>  $ distance    : chr  \"0.0335\" \"0.0494\" \"0.0508\" \"0.0558\" ...\n#>  $ rank        : chr  \"81\" \"91\" \"90\" \"96\" ...\n#>  $ lang        : chr  \"en\" \"en\" \"en\" \"en\" ...\n#>  $ title       : chr  \"Department of Mechanical Engineering, Imperial College London\" \"Imperial College Business School\" \"Exhibition Road\" \"Imperial College School of Medicine\" ...\n#>  $ lat         : chr  \"51.498524\" \"51.4992\" \"51.4989722222222\" \"51.4987\" ...\n#>  $ wikipediaUrl: chr  \"en.wikipedia.org/wiki/Department_of_Mechanical_Engineering%2C_Imperial_College_London\" \"en.wikipedia.org/wiki/Imperial_College_Business_School\" \"en.wikipedia.org/wiki/Exhibition_Road\" \"en.wikipedia.org/wiki/Imperial_College_School_of_Medicine\" ...\n#>  $ countryCode : chr  NA \"AE\" NA \"GB\" ...\n#>  $ thumbnailImg: chr  NA NA NA NA ...\n#>  $ geoNameId   : chr  NA NA NA NA ...\nimperial_neighbours$title[1:5]\n#> [1] \"Department of Mechanical Engineering, Imperial College London\"             \n#> [2] \"Imperial College Business School\"                                          \n#> [3] \"Exhibition Road\"                                                           \n#> [4] \"Imperial College School of Medicine\"                                       \n#> [5] \"Department of Civil and Environmental Engineering, Imperial College London\""},{"path":"data-apis.html","id":"what-if-there-is-no-wrapper","chapter":"7 APIs","heading":"7.10 What if there is no wrapper?","text":"wrapper function, can still access APIs fairly easilty using httr package.look example using OMDb, open source version IMDb, get information movie Mean Girls.use OMDB API need request free API key, follow verification link add API key .Rprofile.can restart R safely access API key within R session.Using documentation API, requests URLs following form, terms angular brackets replaced .little bit effort, can write function composes type request URL us. using glue package help us join strings together.Running example get:can use httr package construct request store response get.Thankfully success! get 401 error code , check clicked activation link API key.full structure response quite complicated, can easily extract requested data using content()","code":"\n# Add this to .Rprofile, pasting in your own API key\noptions(OMDB_API_Key = \"PASTE YOUR KEY HERE\")\n# Load your API key into the current R session,\nombd_api_key <- getOption(\"OMDB_API_Key\")http://www.omdbapi.com/?t=<TITLE>&y=<YEAR>&plot=<LENGTH>&r=<FORMAT>&apikey=<API_KEY>\n\n#' Compose search requests for the OMBD API\n#'\n#' @param title String defining title to search for. Words are separated by \"+\".\n#' @param year String defining release year to search for\n#' @param plot String defining whether \"short\" or \"full\" plot is returned\n#' @param format String defining return format. One of \"json\" or \"xml\"\n#' @param api_key String defining your OMDb API key.\n#'\n#' @return String giving a OMBD search request URL\n#'\n#' @examples \n#' omdb_url(\"mean+girls\", \"2004\", \"short\", \"json\", getOption(OMBD_API_Key))\n#' \nomdb_url <- function(title, year, plot, format, api_key) {\n  glue::glue(\"http://www.omdbapi.com/?t={title}&y={year}&plot={plot}&r={format}&apikey={api_key}\")\n}\nmean_girls_request <- omdb_url(\n  title = \"mean+girls\",\n  year =  \"2004\",\n  plot = \"short\",\n  format =  \"json\",\n  api_key =  getOption(\"OMDB_API_Key\"))\nresponse <- httr::GET(url = mean_girls_request)\nhttr::status_code(response)\n#> [1] 200\nhttr::content(response)\n#> $Title\n#> [1] \"Mean Girls\"\n#> \n#> $Year\n#> [1] \"2004\"\n#> \n#> $Rated\n#> [1] \"PG-13\"\n#> \n#> $Released\n#> [1] \"30 Apr 2004\"\n#> \n#> $Runtime\n#> [1] \"97 min\"\n#> \n#> $Genre\n#> [1] \"Comedy\"\n#> \n#> $Director\n#> [1] \"Mark Waters\"\n#> \n#> $Writer\n#> [1] \"Rosalind Wiseman, Tina Fey\"\n#> \n#> $Actors\n#> [1] \"Lindsay Lohan, Jonathan Bennett, Rachel McAdams\"\n#> \n#> $Plot\n#> [1] \"Cady Heron is a hit with The Plastics, the A-list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex-boyfriend of alpha Plastic Regina George.\"\n#> \n#> $Language\n#> [1] \"English, German, Vietnamese, Swahili\"\n#> \n#> $Country\n#> [1] \"United States, Canada\"\n#> \n#> $Awards\n#> [1] \"7 wins & 25 nominations\"\n#> \n#> $Poster\n#> [1] \"https://m.media-amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@._V1_SX300.jpg\"\n#> \n#> $Ratings\n#> $Ratings[[1]]\n#> $Ratings[[1]]$Source\n#> [1] \"Internet Movie Database\"\n#> \n#> $Ratings[[1]]$Value\n#> [1] \"7.1/10\"\n#> \n#> \n#> $Ratings[[2]]\n#> $Ratings[[2]]$Source\n#> [1] \"Rotten Tomatoes\"\n#> \n#> $Ratings[[2]]$Value\n#> [1] \"84%\"\n#> \n#> \n#> $Ratings[[3]]\n#> $Ratings[[3]]$Source\n#> [1] \"Metacritic\"\n#> \n#> $Ratings[[3]]$Value\n#> [1] \"66/100\"\n#> \n#> \n#> \n#> $Metascore\n#> [1] \"66\"\n#> \n#> $imdbRating\n#> [1] \"7.1\"\n#> \n#> $imdbVotes\n#> [1] \"385,107\"\n#> \n#> $imdbID\n#> [1] \"tt0377092\"\n#> \n#> $Type\n#> [1] \"movie\"\n#> \n#> $DVD\n#> [1] \"21 Sep 2004\"\n#> \n#> $BoxOffice\n#> [1] \"$86,058,055\"\n#> \n#> $Production\n#> [1] \"N/A\"\n#> \n#> $Website\n#> [1] \"N/A\"\n#> \n#> $Response\n#> [1] \"True\""},{"path":"data-apis.html","id":"wrapping-up-2","chapter":"7 APIs","heading":"7.11 Wrapping up","text":"learned bit internet works, benefits using API share data request data Open APIs.obtaining data internet ’s vital keep credentials safe, don’t work needed.Keep API keys code. Store .Rprofile (make sure version control!)Keep API keys code. Store .Rprofile (make sure version control!)Scraping always last resort. API already?Scraping always last resort. API already?Writing code access API can painful necessary.Writing code access API can painful necessary.Don’t repeat people, suitable wrapper exists use .Don’t repeat people, suitable wrapper exists use .","code":""},{"path":"data-checklist.html","id":"data-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-checklist.html","id":"videos-chapters-1","chapter":"Checklist","heading":"Videos / Chapters","text":"Tabular Data (27 min) [slides]Tabular Data (27 min) [slides]Web Scraping (22 min) [slides]Web Scraping (22 min) [slides]APIs (25 min) [slides]APIs (25 min) [slides]","code":""},{"path":"data-checklist.html","id":"reading-1","chapter":"Checklist","heading":"Reading","text":"Use Acquiring Sharing Data section reading list support guide exploration week’s topics. Note texts divided core reading, reference materials materials interest.","code":""},{"path":"data-checklist.html","id":"tasks-1","chapter":"Checklist","heading":"Tasks","text":"Core:Revisit Projects explored Github last week. time look data documentation files.\nfile types new ?\n, packages helper function let read data R?\nmight find many data files Github?\nRevisit Projects explored Github last week. time look data documentation files.file types new ?, packages helper function let read data R?might find many data files Github?Play CSS Diner familiarise CSS selectors.Play CSS Diner familiarise CSS selectors.Identify 3 APIs give access data topics interest . Write post discussion forum describing APIs use one load data R.Identify 3 APIs give access data topics interest . Write post discussion forum describing APIs use one load data R.Scraping Book Reviews:\nVisit Amazon page R Data Science. Write code scrape percentage customers giving “star” rating (5⭐, …, 1⭐).\nTurn code function return tibble form:\nScraping Book Reviews:Visit Amazon page R Data Science. Write code scrape percentage customers giving “star” rating (5⭐, …, 1⭐).Turn code function return tibble form:Generalise function work Amazon products, function takes input vector product names associated vector URLs.Generalise function work Amazon products, function takes input vector product names associated vector URLs.Use function compare reviews following three books: R Data Science, R packages ggplot2.Use function compare reviews following three books: R Data Science, R packages ggplot2.Bonus:Add function R package made last week, remembering add tests documentation.","code":""},{"path":"data-checklist.html","id":"live-session-1","chapter":"Checklist","heading":"Live Session","text":"live session begin discussion week’s tasks. work examples read data non-standard sources.Please come live session prepared discuss following points:Roger Peng states files can imported exported using readRDS() saveRDS() fast space efficient data storage. downside ?Roger Peng states files can imported exported using readRDS() saveRDS() fast space efficient data storage. downside ?data types come across (discussed already) context used?data types come across (discussed already) context used?give greater consideration scraping data using API?give greater consideration scraping data using API?","code":""},{"path":"edav-introduction.html","id":"edav-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.Now read raw data R can start getting data science project moving see initial returns time effort invested far.section explore wrangle, explore visualise data forms basis projects. skills often overlooked folks coming data science “soft skills” compared modelling. However, argue case tasks requires specialist knowledge tools.Additionally, task make majority data scientist’s work often can add value organisation. stage project turn useless, messy data form can used; derive initial insights data making minimal assumptions; communicate accurate engaging way, drive decision making within outwith organisation.","code":""},{"path":"edav-wrangling.html","id":"edav-wrangling","chapter":"9 Data Wrangling","heading":"9 Data Wrangling","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-wrangling.html","id":"what-is-data-wrangling","chapter":"9 Data Wrangling","heading":"9.1 What is Data Wrangling?","text":"Okay, ’ve got data. ’s great start!might handed collaborator, requested via API scraped raw html webpage. worst case scenario, ’re actual scientist (just data one) spent last several months life painstakingly measuring flower petals car parts. Now really want something useful data.’ve seen already can load data R pivot wider longer formats, probably isn’t enough satisfy curiosity. want able view data, manipulate subset , create new variables existing ones cross-reference dataset others. things possible R known various collective names including data manipulation, data munging data wrangling.’ve decided use term data wrangling . ’s data manipulation sounds boring heck data munging unpleasant say makes imagine squelching sort information swamp.follows, ’ll give fly-tour tools data wrangling R, showing examples along way. ’ll focus common useful operations link extensive guides wrangling data R, can refer back need .","code":""},{"path":"edav-wrangling.html","id":"example-data-sets","chapter":"9 Data Wrangling","heading":"9.2 Example Data Sets","text":"demonstrate standard skills use two datasets. mtcars data comes built R installation. second data set look penguins data palmerpenguins.","code":"\nlibrary(palmerpenguins)\npengins <- palmerpenguins::penguins\ncars <- datasets::mtcars"},{"path":"edav-wrangling.html","id":"viewing-your-data","chapter":"9 Data Wrangling","heading":"9.3 Viewing Your Data","text":"","code":""},{"path":"edav-wrangling.html","id":"view","chapter":"9 Data Wrangling","heading":"9.3.1 View()","text":"View() function can used create spreadsheet-like view data. RStudio open new tab.View() work “matrix-like” R object, tibble, data frame, vector matrix. Note capital letter - function called View(), view().Screenshot RStduio files pane, containg spreadsheet view palmer penguins data set.","code":"\nView(penguins)"},{"path":"edav-wrangling.html","id":"head","chapter":"9 Data Wrangling","heading":"9.3.2 head()","text":"large data sets, might want (able ) view . can use head() view first rows. integer argument n specifies number rows like return.","code":"\nhead(x = pengins, n = 3)\n#> # A tibble: 3 × 8\n#>   species island    bill_length_mm bill_depth_mm flippe…¹ body_…² sex    year\n#>   <fct>   <fct>              <dbl>         <dbl>    <int>   <int> <fct> <int>\n#> 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007\n#> 2 Adelie  Torgersen           39.5          17.4      186    3800 fema…  2007\n#> 3 Adelie  Torgersen           40.3          18        195    3250 fema…  2007\n#> # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g"},{"path":"edav-wrangling.html","id":"str","chapter":"9 Data Wrangling","heading":"9.3.3 str()","text":"alternative way view large data set, one complicated format examine structure str(). useful way inspect structure list-like objects, particularly ’ve got nested structure.","code":"\nstr(penguins)\n#> tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n#>  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#>  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n#>  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n#>  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n#>  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n#>  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n#>  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ..."},{"path":"edav-wrangling.html","id":"names","chapter":"9 Data Wrangling","heading":"9.3.4 names()","text":"Finally, just want access variable names can names() function base R.Similarly, can explicitly access row column names data frame tibble using colnames() rownames().cars data, car model stored row names. doesn’t really jive idea tidy data - ’ll see fix shortly.","code":"\nnames(penguins)\n#> [1] \"species\"           \"island\"            \"bill_length_mm\"   \n#> [4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n#> [7] \"sex\"               \"year\"\ncolnames(cars)\n#>  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n#> [11] \"carb\"\nrownames(cars)\n#>  [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n#>  [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n#>  [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n#> [10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n#> [13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n#> [16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n#> [19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n#> [22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n#> [25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n#> [28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n#> [31] \"Maserati Bora\"       \"Volvo 142E\""},{"path":"edav-wrangling.html","id":"renaming-variables","chapter":"9 Data Wrangling","heading":"9.4 Renaming Variables","text":"","code":""},{"path":"edav-wrangling.html","id":"colnames","chapter":"9 Data Wrangling","heading":"9.4.1 colnames()","text":"function colnames() can used set, well retrieve, column names.","code":"\ncars_renamed <- cars \ncolnames(cars_renamed)[1] <- \"miles_per_gallon\"\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cyl\"              \"disp\"            \n#>  [4] \"hp\"               \"drat\"             \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\""},{"path":"edav-wrangling.html","id":"dplyrrename","chapter":"9 Data Wrangling","heading":"9.4.2 dplyr::rename()","text":"can also use functions dplyr rename columns. Let’s alter second column name.done part pipe, making many alterations.using dplyr function remember format new_name = old_name. matches format used create data frame tibble, opposite order python function name often catches people .section creating new variables, see alternative way copying column deleting original.","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\ncars_renamed <- rename(.data = cars_renamed, cylinders = cyl)\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cylinders\"        \"disp\"            \n#>  [4] \"hp\"               \"drat\"             \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\"\ncars_renamed <- cars_renamed %>% \n  rename(displacement = disp) %>% \n  rename(horse_power = hp) %>% \n  rename(rear_axel_ratio = drat)\n\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cylinders\"        \"displacement\"    \n#>  [4] \"horse_power\"      \"rear_axel_ratio\"  \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\""},{"path":"edav-wrangling.html","id":"subsetting-1","chapter":"9 Data Wrangling","heading":"9.5 Subsetting","text":"","code":""},{"path":"edav-wrangling.html","id":"base-r-1","chapter":"9 Data Wrangling","heading":"9.5.1 Base R","text":"base R can extract rows, columns combinations thereof using index notation.Using negative indexing can remove rows columnsYou can also select rows columns names. can done using bracket syntax ([ ]) dollar syntax ($).Since penguins tibble, return different types object. Subsetting tibble bracket syntax return tibble, extracting column using dollar syntax returns vector values.","code":"\n# First row\npenguins[1, ]\n#> # A tibble: 1 × 8\n#>   species island    bill_length_mm bill_depth_mm flippe…¹ body_…² sex    year\n#>   <fct>   <fct>              <dbl>         <dbl>    <int>   <int> <fct> <int>\n#> 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007\n#> # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n# First Column \npenguins[ , 1]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # … with 338 more rows\n\n# Rows 2-3 of columns 1, 2 and 4\npenguins[2:3, c(1, 2, 4)]\n#> # A tibble: 2 × 3\n#>   species island    bill_depth_mm\n#>   <fct>   <fct>             <dbl>\n#> 1 Adelie  Torgersen          17.4\n#> 2 Adelie  Torgersen          18\n# Drop all but first row\npenguins[-(2:344), ]\n#> # A tibble: 1 × 8\n#>   species island    bill_length_mm bill_depth_mm flippe…¹ body_…² sex    year\n#>   <fct>   <fct>              <dbl>         <dbl>    <int>   <int> <fct> <int>\n#> 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007\n#> # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n# Drop all but first column \npenguins[ , -(2:8)]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # … with 338 more rows\npengins[ , \"species\"]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # … with 338 more rows\npenguins$species\n#>   [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>   [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n#> [155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n#> [281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [344] Chinstrap\n#> Levels: Adelie Chinstrap Gentoo"},{"path":"edav-wrangling.html","id":"filter-and-select","chapter":"9 Data Wrangling","heading":"9.5.2 filter() and select()","text":"dplyr two functions subsetting, filter() subsets rows select() subsets column.functions list like retain. Filter select calls can piped together subset based row column values.Subsetting rows can inverted negating filter() statementand dropping columns can done selecting columns except one(s) want drop.","code":"\npenguins %>% \n  select(species, island, body_mass_g)\n#> # A tibble: 344 × 3\n#>   species island    body_mass_g\n#>   <fct>   <fct>           <int>\n#> 1 Adelie  Torgersen        3750\n#> 2 Adelie  Torgersen        3800\n#> 3 Adelie  Torgersen        3250\n#> 4 Adelie  Torgersen          NA\n#> 5 Adelie  Torgersen        3450\n#> 6 Adelie  Torgersen        3650\n#> # … with 338 more rows\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(body_mass_g > 6000)\n#> # A tibble: 2 × 3\n#>   species island body_mass_g\n#>   <fct>   <fct>        <int>\n#> 1 Gentoo  Biscoe        6300\n#> 2 Gentoo  Biscoe        6050\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(!(body_mass_g > 6000))\n#> # A tibble: 340 × 3\n#>   species island    body_mass_g\n#>   <fct>   <fct>           <int>\n#> 1 Adelie  Torgersen        3750\n#> 2 Adelie  Torgersen        3800\n#> 3 Adelie  Torgersen        3250\n#> 4 Adelie  Torgersen        3450\n#> 5 Adelie  Torgersen        3650\n#> 6 Adelie  Torgersen        3625\n#> # … with 334 more rows\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(!(body_mass_g > 6000)) %>% \n  select(!c(species, island))\n#> # A tibble: 340 × 1\n#>   body_mass_g\n#>         <int>\n#> 1        3750\n#> 2        3800\n#> 3        3250\n#> 4        3450\n#> 5        3650\n#> 6        3625\n#> # … with 334 more rows"},{"path":"edav-wrangling.html","id":"creating-new-variables","chapter":"9 Data Wrangling","heading":"9.6 Creating New Variables","text":"","code":""},{"path":"edav-wrangling.html","id":"base-r-2","chapter":"9 Data Wrangling","heading":"9.6.1 Base R","text":"can create new variables base R assigning vector correct length new column name.drop original column data frame, gives us alternative way renaming columns.One thing aware operation preserve column ordering.Generally speaking, code relies columns specific order fragile - breaks easily. possible, try write code another way ’s robust column reordering. ’ve done removing wt column looking column index part code, rather assuming always fourth column.","code":"\ncars_renamed$weight <- cars_renamed$wt\ncars_renamed <- cars_renamed[ ,-which(names(cars_renamed) == \"wt\")]\nhead(cars_renamed, n = 5)\n#>                   miles_per_gallon cylinders displacement horse_power\n#> Mazda RX4                     21.0         6          160         110\n#> Mazda RX4 Wag                 21.0         6          160         110\n#> Datsun 710                    22.8         4          108          93\n#> Hornet 4 Drive                21.4         6          258         110\n#> Hornet Sportabout             18.7         8          360         175\n#>                   rear_axel_ratio  qsec vs am gear carb weight\n#> Mazda RX4                    3.90 16.46  0  1    4    4  2.620\n#> Mazda RX4 Wag                3.90 17.02  0  1    4    4  2.875\n#> Datsun 710                   3.85 18.61  1  1    4    1  2.320\n#> Hornet 4 Drive               3.08 19.44  1  0    3    1  3.215\n#> Hornet Sportabout            3.15 17.02  0  0    3    2  3.440"},{"path":"edav-wrangling.html","id":"dplyrmutate","chapter":"9 Data Wrangling","heading":"9.6.2 dplyr::mutate()","text":"function dplyr create new columns mutate(). Let’s create another column car’s weight kilogrammes rather tonnes.can also create new columns functions multiple columns.","code":"\ncars_renamed <- cars_renamed %>% \n  mutate(weight_kg = weight * 1000)\n\ncars_renamed %>% \n  select(miles_per_gallon, cylinders, displacement, weight, weight_kg) %>% \n  head(n = 5)\n#>                   miles_per_gallon cylinders displacement weight weight_kg\n#> Mazda RX4                     21.0         6          160  2.620      2620\n#> Mazda RX4 Wag                 21.0         6          160  2.875      2875\n#> Datsun 710                    22.8         4          108  2.320      2320\n#> Hornet 4 Drive                21.4         6          258  3.215      3215\n#> Hornet Sportabout             18.7         8          360  3.440      3440\ncars_renamed <- cars_renamed %>% \n  mutate(cylinder_adjusted_mpg = miles_per_gallon / cylinders)"},{"path":"edav-wrangling.html","id":"rownames_to_column","chapter":"9 Data Wrangling","heading":"9.6.3 rownames_to_column()","text":"One useful example adding additional row data frame convert row names column data fame.’s neat function called rownames_to_column() tibble add first column remove row names one step.","code":"\ncars %>% \n  mutate(model = rownames(cars_renamed)) %>% \n  select(mpg, cyl, model) %>% \n  head(n = 5)\n#>                    mpg cyl             model\n#> Mazda RX4         21.0   6         Mazda RX4\n#> Mazda RX4 Wag     21.0   6     Mazda RX4 Wag\n#> Datsun 710        22.8   4        Datsun 710\n#> Hornet 4 Drive    21.4   6    Hornet 4 Drive\n#> Hornet Sportabout 18.7   8 Hornet Sportabout\ncars %>% \n  tibble::rownames_to_column(var = \"model\") %>% \n  head(n = 5)\n#>               model  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"},{"path":"edav-wrangling.html","id":"rowids_to_column","chapter":"9 Data Wrangling","heading":"9.6.4 rowids_to_column()","text":"Another function tibble adds row id observation new column. often useful ordering combining tables.","code":"\ncars %>% \n  tibble::rowid_to_column(var = \"row_id\") %>% \n  head(n = 5)\n#>   row_id  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> 1      1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> 2      2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> 3      3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> 4      4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> 5      5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"},{"path":"edav-wrangling.html","id":"summaries","chapter":"9 Data Wrangling","heading":"9.7 Summaries","text":"summarise() function allows collapse data frame single row, using summary statistic choosing.can calculate average bill length penguins single summarise() function call.Since missing values, might instead want calculate mean recorded values.can also use summarise() gather multiple summaries single data frame., isn’t overly exciting. might, rightly, wonder ’d want use summarise() calls just use simpler base R calls directly.One benefit summarise calls ensure consistent output. However, main advantage comes want apply summaries distinct subgroups data.","code":"\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm))\n#> # A tibble: 1 × 1\n#>   average_bill_length_mm\n#>                    <dbl>\n#> 1                     NA\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm, na.rm = TRUE))\n#> # A tibble: 1 × 1\n#>   average_bill_length_mm\n#>                    <dbl>\n#> 1                   43.9\nbill_length_mm_summary <- penguins %>% \n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\nbill_length_mm_summary\n#> # A tibble: 1 × 8\n#>    mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1  43.9   44.4  59.6  32.1  39.2  44.4  39.2  59.6"},{"path":"edav-wrangling.html","id":"grouped-operations","chapter":"9 Data Wrangling","heading":"9.8 Grouped Operations","text":"real benefit summarise() comes combination group_by(). allows calculate summary statistics level factor one additional line code. ’re re-calculating set summary statistics just found penguins, individual species.can group multiple factors calculate summaries distinct combination levels within data set. group combinations species island belong.","code":"\npenguins %>% \n  group_by(species) %>%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n#> # A tibble: 3 × 9\n#>   species    mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie     38.8   38.8  46    32.1  36.8  38.8  36.8  46  \n#> 2 Chinstrap  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 3 Gentoo     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\npenguin_summary_stats <- penguins %>% \n  group_by(species, island) %>%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n#> `summarise()` has grouped output by 'species'. You can override using the\n#> `.groups` argument.\n\npenguin_summary_stats\n#> # A tibble: 5 × 10\n#> # Groups:   species [3]\n#>   species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n#> 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n#> 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n#> 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6"},{"path":"edav-wrangling.html","id":"ungrouping","chapter":"9 Data Wrangling","heading":"9.8.1 Ungrouping","text":"default, call summarise() undo one level grouping. means previous result still grouped species.(can see tibble output , also examining structure returned data frame. tells us S3 object class “grouped_df”, inherits properties “tbl_df”, “tbl”, “data.frame” objects.)Since grouped two variables, R expects us use two summaries returning data frame (tibble) grouped. One way satisfy use apply second summary species level grouping.However, won’t always want apply another summary. case, can undo grouping using ungroup(). Remembering ungroup common gotcha cause confusion working multiple-group summaries.’s alternative method achieve thing single step using dplyr versions 1.0.0 . set .groups parameter summarise() function call, determines grouping returned data frame..groups parameter can take 4 possible values:“drop_last”: dropping last level grouping (option v1.0.0);“drop_last”: dropping last level grouping (option v1.0.0);“drop”: levels grouping dropped;“drop”: levels grouping dropped;“keep”: grouping structure .data;“keep”: grouping structure .data;“rowwise”: row group.“rowwise”: row group.default, “drop_last” used results 1 row “keep” used otherwise.","code":"\nclass(penguin_summary_stats)\n#> [1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\npenguin_summary_stats %>% \n  summarise_all(mean, na.rm = TRUE)\n#> # A tibble: 3 × 10\n#>   species   island  mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>      <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie        NA  38.8   38.7  45.2  33.4  37.0  38.7  37.0  45.2\n#> 2 Chinstrap     NA  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 3 Gentoo        NA  47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\nungroup(penguin_summary_stats)\n#> # A tibble: 5 × 10\n#>   species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n#> 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n#> 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n#> 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6"},{"path":"edav-wrangling.html","id":"reordering-factors","chapter":"9 Data Wrangling","heading":"9.9 Reordering Factors","text":"R stored factors integer values, maps set labels. factor levels appear data assigned coded integer value mapping factor levels integers depend order labels appear data.can annoying, particularly factor levels relate properties aren’t numerical inherent ordering . example , t-shirt size twelve people.Irritatingly, sizes aren’t order extra large isn’t included ’s included particular sample. leads awkward looking summary tables plots.can fix creating new variable factors explicitly coded correct order. also need specify drop empty groups part group_by().","code":"\ntshirts <- tibble::tibble(\n  id = 1:12, \n  size = as.factor(c(\"L\", NA, \"M\", \"S\", \"XS\", \"M\", \"XXL\", \"L\", \"XS\", \"M\", \"L\", \"S\"))\n)\n\nlevels(tshirts$size)\n#> [1] \"L\"   \"M\"   \"S\"   \"XS\"  \"XXL\"\ntshirts %>% group_by(size) %>% summarise(count = n())\n#> # A tibble: 6 × 2\n#>   size  count\n#>   <fct> <int>\n#> 1 L         3\n#> 2 M         3\n#> 3 S         2\n#> 4 XS        2\n#> 5 XXL       1\n#> 6 <NA>      1\ntidy_tshirt_levels <- c(\"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\", NA)\n\ntshirts %>% \n  mutate(size_tidy = factor(size, levels = tidy_tshirt_levels)) %>% \n  group_by(size_tidy, .drop = FALSE ) %>% \n  summarise(count = n())\n#> # A tibble: 7 × 2\n#>   size_tidy count\n#>   <fct>     <int>\n#> 1 XS            2\n#> 2 S             2\n#> 3 M             3\n#> 4 L             3\n#> 5 XL            0\n#> 6 XXL           1\n#> # … with 1 more row"},{"path":"edav-wrangling.html","id":"be-aware-factors","chapter":"9 Data Wrangling","heading":"9.10 Be Aware: Factors","text":"seen little already, categorical variables can cause issues wrangling presenting data R. problems solvable using base R techniques forcats package provides tools common problems. includes functions changing order factor levels values associated.examples functions package include:fct_reorder(): Reordering factor another variable.fct_infreq(): Reordering factor frequency values.fct_relevel(): Changing order factor hand.fct_lump(): Collapsing least/frequent values factor “”.Examples can found forcats vignette factors chapter R data science.","code":""},{"path":"edav-wrangling.html","id":"be-aware-strings","chapter":"9 Data Wrangling","heading":"9.11 Be Aware: Strings","text":"Working analysing text data skill unto . However, useful able basic manipulation character strings programatically.R developed statistical programming language, well suited computational modelling aspects working text data base R string manipulation functions can bit unwieldy times.stringr package aims combat providing useful helper functions range text management problems. Even analysing text data can useful, example remove prefixes lot column names.Suppose wanted keep text following underscore column names. using regular expression extract lower-case upper-case letters follow underscore.Alternatively, can avoid using regular expressions. can split column name underscore keep second part string., unless plan work extensively text data, recommend look string manipulations need . strings section R Data Science useful starting point.","code":"\nhead(poorly_named_df)\n#> # A tibble: 6 × 11\n#>   observa…¹    V1_A   V2_B    V3_C   V4_D   V5_E   V6_F   V7_G   V8_H    V9_I\n#>       <int>   <dbl>  <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n#> 1         1  1.21   -1.63  -2.31    0.162  0.492  0.662 -0.125 -0.725 -0.0244\n#> 2         2 -0.802  -1.58   0.456  -0.257  1.02  -1.62   0.738 -0.470  0.706 \n#> 3         3  1.54    0.331 -0.317  -1.90   0.920 -0.428 -0.674  0.174 -0.526 \n#> 4         4 -1.04   -0.218 -1.73   -0.383  1.80   0.404  0.761  1.79   1.13  \n#> 5         5  0.0474  0.409 -0.367  -1.45  -1.37   2.09   0.217  0.452 -0.0536\n#> 6         6 -0.809  -0.991 -0.0595  0.487  0.844 -0.217  0.396  0.568  0.184 \n#> # … with 1 more variable: V10_J <dbl>, and abbreviated variable name\n#> #   ¹​observation_id\nstringr::str_extract(names(poorly_named_df), pattern = \"(?<=_)([a-zA-Z]+)\")\n#>  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\"\n# split column names at underscores and inspect structure of resuting object\nsplit_strings <- stringr::str_split(names(poorly_named_df), pattern = \"_\")\nstr(split_strings)\n#> List of 11\n#>  $ : chr [1:2] \"observation\" \"id\"\n#>  $ : chr [1:2] \"V1\" \"A\"\n#>  $ : chr [1:2] \"V2\" \"B\"\n#>  $ : chr [1:2] \"V3\" \"C\"\n#>  $ : chr [1:2] \"V4\" \"D\"\n#>  $ : chr [1:2] \"V5\" \"E\"\n#>  $ : chr [1:2] \"V6\" \"F\"\n#>  $ : chr [1:2] \"V7\" \"G\"\n#>  $ : chr [1:2] \"V8\" \"H\"\n#>  $ : chr [1:2] \"V9\" \"I\"\n#>  $ : chr [1:2] \"V10\" \"J\"\n\n# keep only the second element of each character vector in the list\npurrr::map_chr(split_strings, function(x){x[2]})\n#>  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\""},{"path":"edav-wrangling.html","id":"be-aware-date-times","chapter":"9 Data Wrangling","heading":"9.12 Be Aware: Date-Times","text":"Remember fuss made storing dates ISO standard format? dates times complicated enough work adding extra ambiguity.\\[ \\text{YYYY} - \\text{MM} - \\text{DD}\\]\nDates, times time intervals reconcile two factors: physical orbit Earth around Sun social geopolitical mechanisms determine measure record passing time. makes history date time records fascinating can make working type data complicated.Moving larger smaller time spans: leap years alter number days year, months variable length (February’s length changing year year). data measured place uses daylight saving, one day year 23 hours long another 25 hours long. make things worse, dates hour clocks change uniform across countries, might distinct time zones change time.Even level minutes seconds aren’t safe - since Earth’s orbit gradually slowing leap second added approximately every 21 months. things better looking longer time scales across cultures, might account different calendars: months added removed altered time, calendar systems still take different approaches measuring time using different units origin points.issues careful working date time data. Functions help can found lubridate package, examples dates times chapter R data science.","code":""},{"path":"edav-wrangling.html","id":"be-aware-relational-data","chapter":"9 Data Wrangling","heading":"9.13 Be Aware: Relational Data","text":"data need stored across two data frames need able cross-reference match values observational unit. sort data know relational data, used extensively data science.variables use match observational units across data frames known keys. primary key belongs first table foreign key belongs secondary table. various ways join data frames, depending want retain.","code":""},{"path":"edav-wrangling.html","id":"join-types","chapter":"9 Data Wrangling","heading":"9.13.0.1 Join types","text":"might want keep observational units key variables values data frames, known inner join.Inner join diagram. Source: R Data ScienceYou might instead want keep units primary table pad NAs corresponding foreign key second table. results (outer) left-join.Diagram left, right outer joins. Source: R Data ScienceConversely, might keep units second table pad NAs corresponding foreign key primary table. imaginatively named (outer) right-join.(outer) full join, observational units either table retained missing values padded NAs.Things get complicated keys don’t uniquely identify observational units either one tables. ’d recommend start exploring ideas relational data chapter R Data Science.","code":""},{"path":"edav-wrangling.html","id":"why-and-where-to-learn-more","chapter":"9 Data Wrangling","heading":"9.13.0.2 Why and where to learn more","text":"Working relational data essential getting data science running wilds reality. businesses companies don’t store data huge single csv file. one isn’t efficient, cells empty. Secondly, ’s secure approach, since can’t grant partial access data. ’s information usually stored many data frames (generically known tables) within one databases.data silos created, maintained, accessed destroyed using relational data base management system. management systems use code manage access stored data, just like seen dplyr commands . might well heard SQL programming language (many variants), popular language data base management inspiration dplyr package verbs.’d like learn many excellent introductory SQL books courses, ’d recommend picking one focuses data analysis data science unless really want dig efficient storage querying databases.","code":""},{"path":"edav-wrangling.html","id":"wrapping-up-3","chapter":"9 Data Wrangling","heading":"9.14 Wrapping up","text":":Learned wrangle tabular data R dplyrLearned wrangle tabular data R dplyrMet idea relational data dplyr’s relationship SQLMet idea relational data dplyr’s relationship SQLBecome aware tricky data types packages can help.Become aware tricky data types packages can help.","code":""},{"path":"edav-analysis.html","id":"edav-analysis","chapter":"10 Exploratory Data Analysis","heading":"10 Exploratory Data Analysis","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-analysis.html","id":"introduction-2","chapter":"10 Exploratory Data Analysis","heading":"10.1 Introduction","text":"Exploratory data analysis essential stage data science project. allows become familiar data working also identify potential strategies progressing project flagging areas concern.chapter look three different perspectives exploratory data analysis: purpose data scientist, purpose broader team working project finally purpose project .","code":""},{"path":"edav-analysis.html","id":"get-to-know-your-data","chapter":"10 Exploratory Data Analysis","heading":"10.2 Get to know your data","text":"Let’s first focus exploratory data analysis point view, data scientists.Exploratory data analysis (EDA) process examining data set understand overall structure, contents, relationships variables contains. EDA iterative process ’s often done building model making data-driven decisions within data science project.Exploratory Data Analysis: quick simple exerpts, summaries plots better understand data set.One key aspect EDA generating quick simple summaries plots data. plots summary statistics can help quickly understand distribution relationships recorded variables. Additionally, exploratory analysis familiarise structure data ’re working data collected.\nFigure 10.1: Investigating marginal pairwise relationships Iris dataset.\nSince EDA initial iterative process, ’s rare component analysis put production. Instead, goal get general understanding data can inform next steps analysis.terms workflow, means using one notebooks often effective way organising work exploratory analysis. allows rapid iteration experimentation, also providing level reproducibility documentation. Since notebooks allow combine code, plots, tables text single document, makes easy share initial findings stakeholders project managers.","code":""},{"path":"edav-analysis.html","id":"start-a-conversation","chapter":"10 Exploratory Data Analysis","heading":"10.3 Start a conversation","text":"effective EDA sets precedent open communication stakeholder project manager.’ve seen benefits EDA data scientist, isn’t perspective.One key benefit EDA can kick-start communication subject matter experts project managers. can build rapport trust early project’s life cycle sharing preliminary findings stakeholders . can lead deeper understanding available data problem addressed everyone involved. done well, also starts build trust work even begin modelling stage project.","code":""},{"path":"edav-analysis.html","id":"communicating-with-specialists","chapter":"10 Exploratory Data Analysis","heading":"10.3.1 Communicating with specialists","text":"Sharing exploratory analysis inevitably require time investment. graphics, tables, summaries produce need presented higher standard explained way clear non-specialist. However, time investment often pay dividends additional contextual knowledge domain-expert can provide. deep understanding business technical domain surrounding problem. can provide important insights aren’t data , vital project’s success.example, stakeholder conversations often reveal important features data generating measurement process accounted modelling. details usually left data documentation immediately obvious specialist field.","code":""},{"path":"edav-analysis.html","id":"communicating-with-project-manager","chapter":"10 Exploratory Data Analysis","heading":"10.3.2 Communicating with project manager","text":"EDA can sometimes allow us identify cases strength signal within available data clearly insufficient answer question interest. clearly communicating project manager, project can postponed different, better quality simply data collected. ’s important note data collection trivial can high cost terms time capital. might collecting data needed answer question cost ’re likely gain knowing answer. Whether project postponed cancelled, constitutes successful outcome project, aim produce insight profit - fit models sake.","code":""},{"path":"edav-analysis.html","id":"scope-your-project","chapter":"10 Exploratory Data Analysis","heading":"10.4 Scope Your Project","text":"EDA initial assessment whether available data measure correct values, sufficient quality quantity, answer particular question.third view EDA initial assessment whether available data measure correct values, sufficient quality quantity, answer particular question. order EDA successful, ’s important take key steps.First, ’s important formulate specific question interest line investigation agree stakeholder. clear question mind, easier focus analysis identify whether data hand can answer .Next, ’s important make record (one doesn’t already exist) data collected, collected, recorded variable represents units recorded. meta-data often known data sheet. information written form crucial adding new collaborator project, can understand data generating measurement processes, aware quality accuracy recorded values.","code":""},{"path":"edav-analysis.html","id":"investigate-your-data","chapter":"10 Exploratory Data Analysis","heading":"10.5 Investigate Your Data","text":"EDA opportunitiy quantify data completeness investigate possibility informative missingness.addition, ’s essential investigate document structure, precision, completeness quantity data available. includes assessing degree measurement noise misclassification data, looking clear linear non-linear dependencies variables, identifying data missing ’s structure missingness. data features aware presence censoring whether values tend missing together.Furthermore, advanced EDA might include simulation study estimate amount data needed detect smallest meaningful effect. -depth typical EDA suspect signals within data weak relative measurement noise, can help demonstrate limitations current line enquiry information currently available.","code":""},{"path":"edav-analysis.html","id":"what-is-not-eda","chapter":"10 Exploratory Data Analysis","heading":"10.6 What is not EDA?","text":"’s important understand exploratory data analysis thing modelling. particular construction baseline model, sometimes called initial data analysis.Though might inform choice baseline model, EDA usually model based. Simple plots summaries used identify patterns data inform approach rest project.degree statistical rigour can added use non-parametric techniques; methods like rolling averages, smoothing partitioning can help identify trends patterns making minimal assumptions data generating process.\nFigure 10.2: Daily change Dow Jones Index smoothed estimate mean 95% confidence interval.\nTable 10.1: Mean standard deviation daily change Dow Jones Index, 1st June 1998.Though assumptions EDA often minimal can help make explicit. example, plot moving averages shown confidence band, construction band makes implicit assumption , least locally, observations distribution exchangeable.Finally, EDA prescriptive process. given lot suggestions might usually want consider, correct way go EDA heavily dependent particular dataset, interpretation task want achieve . one parts data science make craft hone experience, rather algorithmic process. work particular area long time develop knowledge common data quirks area, may may translate applications.Now better idea EDA, let’s talk issue EDA tries resolve issues generates.","code":""},{"path":"edav-analysis.html","id":"issue-forking-paths","chapter":"10 Exploratory Data Analysis","heading":"10.7 Issue: Forking Paths","text":"data science project sequence many decisions must make, many potential options difficult decide upon priori.Focusing one small part process, might consider picking null baseline model, try improve . null model make constant predictions, incorporate simple linear trend something flexible obviously needed? option try working time constraints? contextual clues rule null models contextual grounds?EDA lets narrow options looking data helps decide might reasonable modelling approaches.problem sneaks data leakage. Formally training data included test set, sort information leak can happen informally . Usually ’ve seen data ’re trying model predict selected modelling approach based information.Standard, frequentist statistical methods estimation testing assume “peeking” type occurred. use methods without acknowledging already observed data artificially inflate significance findings. example, might comparing two models: first makes constant predictions regard predictor, second includes linear trend. course use statistical test confirm seeing unlikely chance. However, must aware test performed previously examined data noticed looked trend.Similar issues arise Bayesian approaches, particularly constructing eliciting prior distributions model parameters. One nice thing can Bayesian setting simulating data prior predictive distribution get expert check datasets seem seem reasonable. However, often case expert also person collected data soon modelling. ’s difficult ignore seen, leads similar, subtle leakage problems.","code":""},{"path":"edav-analysis.html","id":"correction-methods","chapter":"10 Exploratory Data Analysis","heading":"10.8 Correction Methods","text":"various methods corrections can apply testing estimation procedures ensure error rates confidence intervals account previous “peeking” EDA.Examples corrections developed across many fields statistics. medical statistics approaches like Bonferroni correction, account carrying multiple hypothesis tests. change-point literature techniques estimating change location given change detected somewhere time series. extreme value literature methods estimate required level protection rare events, given analysis triggered current protections compromised.\nFigure 10.3: Example sea-height datasets analysis triggered extreme value () visually identified change mean ().\n:::\n::::corrections require us make assumptions nature peeking. either specific process occurred, else pessimistic much information leaked. Developing corrections account EDA isn’t really possible, given adaptive non-prescriptive nature.addition either highly specific pessimistic, corrections can also hard derive complicated implement. settings power tests level estimation critically important, entire analysis pre-registered. clinical trials, example, every step analysis specified data collected. data science rigorous approach rarely taken.statistically trained data scientists, important us remain humble potential findings suggest follow studies confirm presence relationships find.","code":""},{"path":"edav-analysis.html","id":"learning-more","chapter":"10 Exploratory Data Analysis","heading":"10.9 Learning More","text":"chapter acknowledged exploratory analyses important part data science workflow; true us data scientists, also people involved projects.’ve also seen exploratory analysis can help guide progression projects, must take care prevent acknowledge risk data leakage.want explore topic , can quite challenging: examples good, exploratory data analyses can difficult come . often made publicly available way papers technical reports . Additionally, often kept public repositories “polished” rest project. Personally, think shame culture slowly changing.now, best approach learning makes good exploratory analysis lots talk colleagues approaches.lots list-articles claiming give comprehensive list steps exploratory analysis. can good inspiration, strongly suggest don’t treat gospel.Despite name chapter, Roger Peng’s EDA check-list gives excellent worked example exploratory analysis R. discussion article “Exploratory Data Analysis Complex Models”, Andrew Gelman makes abstract discussion exploratory analyses (happen modelling) confirmatory analyses (happen afterwards).","code":""},{"path":"edav-visualisation.html","id":"edav-visualisation","chapter":"11 Data Visualisation","heading":"11 Data Visualisation","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-visualisation.html","id":"introduction-3","chapter":"11 Data Visualisation","heading":"11.1 Introduction","text":"","code":""},{"path":"edav-visualisation.html","id":"more-than-a-pretty-picture","chapter":"11 Data Visualisation","heading":"11.1.1 More than a pretty picture","text":"Data visualisation integral part work data scientist.Warming stripes graphic cover “Climate Book”’ll use visualisations rapidly explore new data sets, understand structure establish types model might suitable task hand. Visualisation also vital model evaluation check validity assumptions model based. relatively technical uses visualisation, graphics much broader role within work effective data scientist.well designed, plots, tables animations can tell compelling stories trapped within data. can also intuitively communicate strength evidence findings draw attention salient parts argument.Data visualisation amalgamation science, statistics, graphic design storytelling. ’s multi-disciplinary nature means draw skills ensure success. certainly many ways go wrong visualising data, many ways get right.chapter won’t step--step tutorial visualise type data. line-visualisations gone wrong. Instead, hope pose questions ’ll get thinking critically exactly want graphic produce.five things think producing sort data visualisation. consider turn.","code":""},{"path":"edav-visualisation.html","id":"your-tools","chapter":"11 Data Visualisation","heading":"11.2 Your Tools 🔨","text":"","code":""},{"path":"edav-visualisation.html","id":"picking-the-right-tool-for-the-job","chapter":"11 Data Visualisation","heading":"11.2.1 Picking the right tool for the job","text":"think data visualisation, might immediately think impressive animations complex, interactive dashboards allow users explore relationships within data .tools doubt impressive means necessary effective data visualisation. many cases technology needed . history data visualisation vastly pre-dates computers effective visualisations remain analogue creations.\nFigure 10.1: Coffee consumption, visualised. Jaime Serra Palou.\nvisualisation year’s coffee consumption ideal example. Displaying number cups coffee bar chart line graph accurate way collect display data, wouldn’t resonance impact certainly wouldn’t memorable.","code":""},{"path":"edav-visualisation.html","id":"analogue-or-digital","chapter":"11 Data Visualisation","heading":"11.2.2 Analogue or Digital","text":"","code":""},{"path":"edav-visualisation.html","id":"analogue-data-viz","chapter":"11 Data Visualisation","heading":"11.2.2.1 Analogue Data Viz","text":"another example analogue data visualisation created part data collection. member department invited place Lego brick grid indicate much caffeine consumed much sleep . beauty using Lego bricks stackable create bar plot two dimensions.\nFigure 10.2: Caffeination vs sleep, shown lego. Elsie Lee-Robbins\nthird example can found next tills many supermarkets. customer given token pay goods. can drop token one three large perspex containers leave shop, representing different charity. end month £10,000 split charities proportion number tokens. containers made transparent material can see tokens distributed, giving visualisation level support charities.many way constructing physical, analogue visualisation data doesn’t need done part data collection process. simplest perhaps obvious obvious create plot tabular data using pen paper.","code":""},{"path":"edav-visualisation.html","id":"digital-data-viz","chapter":"11 Data Visualisation","heading":"11.2.2.2 Digital Data Viz","text":"comes digital tools data visualisation plethora options. similar pen--paper plotting draw visualisations using Photoshop, open source equivalent like Inkscape. benefit misplace line dot can correct small error without start .data-focused tools point--click interfaces. things like Excel’s chart tools, specialist visualisation software like Tableau. great scale quantity data, can plot larger amounts raw data values wouldn’t time patience hand - whether ’s analogue digital format.Analogue point--click approaches visualisation shared limitation reproducible, least without extensive documentation explaining graphic created.Using code create visualisations can resolve reproducibility issue, includes visualisation part larger, reproducible workflow data science. Scripted visualisations also scale easily large data sets easy alter changes required. downside relatively steep learning curve creating visualisations, exactly point--click methods trying avoid.matter produce visualisations, time cost developing skills medium buys ability control customise create. upfront time investment also often make faster producing future graphics medium.Whenever approach new visualisation problem, pick tools medium judiciously. balance immediate needs speed, accuracy reproducibility current skill level improving skills medium long term. Unfortunately, way make good visualisations make lots bad ones even mediocre ones first.","code":""},{"path":"edav-visualisation.html","id":"ggplot2","chapter":"11 Data Visualisation","heading":"11.2.3 ggplot2","text":"aim produce wide range high quality data visualisation using R, ggplot2 package one versatile well documented tools available .g’s start package name stand grammar graphics. opinionated, abstract approach constructing data visualisations programmatically, building slowly adding additional plot elements one layer time.idea “grammar Graphics” originally introduced Leland Wilkinson. paper shown Hadley Wickham, associated ggplot2 package popularised approach within R community. Like many tidyverse collection packages, ggplot2 provides simple specialised modular functions can composed create complex visualisations.’d like learn use ggplot2, wouldn’t recommend starting paper recommend trying get started docs alone. Instead, suggest work introductory tutorial, one resources linked within package documentation. grasp basic principles best way improve try making plots, using reference texts people’s work guide. great source inspiration Tidy Tuesday data visualisation challenge. can search challenge Github inspect plots made people code used make .Learning ggplot2:ResourcesTutorialTidy Tuesday Github","code":""},{"path":"edav-visualisation.html","id":"your-medium","chapter":"11 Data Visualisation","heading":"11.3 Your Medium 📽","text":"","code":""},{"path":"edav-visualisation.html","id":"where-is-your-visualisation-going","chapter":"11 Data Visualisation","heading":"11.3.1 Where is your visualisation going?","text":"second aspect recommend think starting data visualisation graphic going used. intended location visualisation influence composition graphic also amount effort dedicate .example, consider making exploratory plot start project improve understanding structure within data. case need spend much time worrying refining axis labels, colour schemes file format save work.working figure included daily stand-meeting team, take little care ensure work can clearly understood others, example legend axis labels large sufficiently informative.refinement required external presentation. message visualisation immediately clear? graphic still clear displayed boardroom conference hall, pixellate? Finally, long audience interpret visualisation speaking? Even slide decks made available, audience members actually refer presentation.opposing consideration made preparing visualisation report scientific paper. case plots tables can small, particularly two-column grid layouts. wary legibility smallest text (think values axes) visualisation can clearly understood, whether document read zoomed-computer screen printed black white.","code":""},{"path":"edav-visualisation.html","id":"file-types","chapter":"11 Data Visualisation","heading":"11.3.2 File Types","text":"low resoloution bitmap image.ensure graphics suitable intended medium helpful know little bit image file types.two dominant types image file: vector graphics bitmap graphics.Bitmap graphics store images grid little squares pixels takes single, solid colour. make bitmap image large enough, either zooming using really big screen, individual pixels become visible. Usually isn’t going intention, need ensure resolution graphic (dimensions counted pixels) sufficiently large.Vector graphics create images using continuous paths filling areas enclose flat colour. vector images can enlarged much like without image quality becoming compromised. great simple simple designs like logos, clear used letterhead billboards.However, vector graphics memory intensive bitmap images, particularly many distinct colours objects within image. can particular problem data science, example creating scatter plot many thousands data points.can often useful save bitmap vector version graphics. way can use bitmap need small files load quickly (like loading webpage) vector graphics need visualisation stay sharp enlarged (like creating poster giving presentation auditorium).","code":""},{"path":"edav-visualisation.html","id":"your-audience","chapter":"11 Data Visualisation","heading":"11.4 Your Audience 👥","text":"","code":""},{"path":"edav-visualisation.html","id":"know-your-audience","chapter":"11 Data Visualisation","heading":"11.4.1 Know Your Audience","text":"Data visualisations tool communicating information. make communication effective possible, target delivery intended audience.intended audience visualisation?intended audience visualisation?knowledge bring ?knowledge bring ?assumptions biases hold?assumptions biases hold?Creating personas distinct user groups can helpful way answer questions, particularly user population heterogeneous.know target delivery particular audience, fist identify exactly .make compelling data visualisation idea background knowledge viewer brings. specialist statistics data science, expertise lie area application? findings ’re presenting going come surprise , act confirmation pre-existing body knowledge.’s worth considering prior beliefs strongly held constructing visualisation. Take time consider existing knowledge alter influence interpretation ’re showing . flip-side, might presenting information topic viewer best case ambivalent worst case actively bored . case, can take special care compose engaging visualisations capture hold attention audience.","code":""},{"path":"edav-visualisation.html","id":"preattentive-attributes","chapter":"11 Data Visualisation","heading":"11.4.2 Preattentive Attributes","text":"crafting visualisation want require little work possible viewer.\n, can use pre-attentive attributes, colour, shape size position encode data values.\nFigure 11.1: Examples preattentive attributes\npreattentive attributes properties lines shapes provide immediate visual contrast without requiring active thought viewer. see, care needs taken ensure don’t mislead viewer use attributes.","code":""},{"path":"edav-visualisation.html","id":"example-first-impressions-count","chapter":"11 Data Visualisation","heading":"11.4.3 Example: First Impressions Count","text":"Issues scales, area perspectiveThis figure presents bar chart mean height males several countries, swapped bars human outlines. visualisation attractive, minimal design pleasant colour scheme, doesn’t good job immediately conveying relevant information viewer.three main issues plot caused swapping bars plot male silhouettes, linked difference lengths areas perceived humans. Typically, make immediate pre-attentive comparisons based area draw accurate, considered comparisons comparing lengths.replacing bars human outlines starting height scale zero, plot breaks proportionality length area inherent bar plot. causes dissonance immediate considered interpretation plot. additional issue silhouettes overlap, creating forced perspective makes seem like outlines also back therefore even larger perspective taken account.three issues important consider constructing visualisations. showing values data take, focusing smaller interval provide better contrast? using size circle represent value, changing diameter area proportion data value? finally, making plot appears three-dimensional, done purpose , , can one dimensions better represented attribute isn’t position?","code":""},{"path":"edav-visualisation.html","id":"visual-perception","chapter":"11 Data Visualisation","heading":"11.4.4 Visual Perception","text":"reducing dimensionality plot may wish represent data value using colour rather position. deciding use colour, keep audience mind.aim two categories? case, ’ll need select finite set colours ensure can distinguished.representing data value continuous associated ordering? select palette provide sufficient contrast viewers work.representing measurement reference value (example 0 temperature centigrade) diverging colour palette can used represent data reference point. requires cultural understanding colours interpreted, example likely cause confusion encoding red cold blue hot.colour scales without reference point gradient single colour likely best option. either case, important check unit change data value represents consistent change colour across values. case rainbow palette (neither single gradient diverging).\nFigure 10.3: default colour scales R\nensure accessibility designs, recommend one many -line tools simulate colour vision deficiency using pre-made palette considered . good, low-tech rule thumb design visualisations ’re still easily understood printed grey-scale. can mean picking appropriate colours additionally varying point shape, line width line types used.\nFigure 11.2: Desatureated colour scales R\npractical guide setting colours see chapter exploratory data analysis Roger Peng.","code":""},{"path":"edav-visualisation.html","id":"alt-text-titles-and-captions","chapter":"11 Data Visualisation","heading":"11.4.5 Alt-text, Titles and Captions","text":"Captions describe figure table may identified list figures (appropriate).Alternative text describes content image person view . (Guide writing alt-text)Titles give additional context identify key findings. Active titles preferable.visualisations included report, article website, often accompanied three pieces text. title, caption alt-text help audience understand visualisation serves distinct purpose.","code":""},{"path":"edav-visualisation.html","id":"captions","chapter":"11 Data Visualisation","heading":"11.4.5.1 Captions","text":"caption short description visualisation. Captions usually displayed directly figure table describe. captions serve two purposes: report, caption can used look visualisation list figures tables. second purpose caption add additional detail don’t want add plot directly. example caption might “Time series GDP United States America, 2017-2022. Lines show daily (solid), monthly (dashed) five-year (dotted) mean values.”","code":""},{"path":"edav-visualisation.html","id":"alt-text","chapter":"11 Data Visualisation","heading":"11.4.5.2 Alt-text","text":"Alt text alternative text used describe content image person can’t view . text helpful people visual impairment, particularly uses screen reader. Screen reading software reads digital text loud can’t interpret images. software replaces image provided alternative text. Alt text also valuable cases image can’t found loaded, example incorrect file path slow internet connection, ’ll displayed place image.purpose alt-text different caption. ’s designed replacement image, just shorthand provide additional information. important trend conclusion drawn visualisation (already mentioned main text) identified alt-text. sort interpretation key aspect alt-text shouldn’t included caption.","code":""},{"path":"edav-visualisation.html","id":"titles","chapter":"11 Data Visualisation","heading":"11.4.5.3 Titles","text":"Titles give additional context conveyed axis labels chart annotations. Alternatively title can used like newspaper headline deliver key findings visualisation. One example might looking visualisation composed many smaller plots, showing GDP US state last five years. Title smaller plot identify state describing, overall title might something like “US states increased GDP period 2017-2022”.including type interpretive title, make sure interpretation clear alt-text.","code":""},{"path":"edav-visualisation.html","id":"your-story","chapter":"11 Data Visualisation","heading":"11.5 Your Story 📖","text":"fourth aspect successful data visualisation must tell story. story doesn’t need multi-generational novel even captivating novella. picture speaks thousand words, really aiming engaging anecdote.visualisation something grabs viewers attention composition content alters knowledge view world way.Telling effective stories requires planning. construct narrative depends effect want audience. ’d encourage think like data journalist go work intended effect clear mind. purpose inform fact, persuade use different methodology entertain presenting dull commonplace data fresh engaging way?Three goals data visualisation: entertain, persuade performIn reality goal mixture , interior point triangle. Clearly identifying point help present visual story way works towards aims, rather .point presentation, important realise neutral way present information. creating visualisation ’re choosing aspects data emphasise, gets summarised presented . construct plot tells clear coherent story. However, one story tell single dataset.example , let’s consider time-series showing price two stocks particular choice scale y-axis. Suppose two stock values fluctuating around $100 per share. Choosing scale goes $90 $110 emphasise differences two stocks. Setting lower limit $0 instead emphasise variations small relative overall value stocks. valid approaches tell different stories. clear open telling chosen alternative.final cross-data journalism visualisations competing viewers attention. compete everything else going lives. Establish clear “hook” within visualisation attract viewer’s attention immediately deliver core message. might done contrasting trend-line intriguing title. Lead attention first key message supporting evidence.","code":""},{"path":"edav-visualisation.html","id":"your-guidelines","chapter":"11 Data Visualisation","heading":"11.6 Your Guidelines 📝","text":"","code":""},{"path":"edav-visualisation.html","id":"standardise-and-document","chapter":"11 Data Visualisation","heading":"11.6.1 Standardise and Document","text":"final consideration creating visualisations reduce number considerations make future. done thinking carefully decisions make writing guidelines make choices consistently.choices go making effective data visualisation important deserve careful consideration. However, consideration comes cost. employer literal, financial cost paying time. broadly opportunity cost things instead.efficient visualisation design, extend DRY coding principles design processes. Make choices carefully document decisions externalise cognitive work required future.Many companies aware financial opportunity costs provide style guides visualisations similar manner coding writing style guide. externalises formalises many decisions, also leads uniform style across visualisations data scientists producing . leads unified, house-style graphic design visual brand easily identifiable. beneficial large companies personal projects alike.","code":""},{"path":"edav-visualisation.html","id":"example-style-guides","chapter":"11 Data Visualisation","heading":"11.6.2 Example Style Guides","text":"’d highly recommend exploring visualisation guides get idea constructed might develop .Unsurprisingly best guides come media outlets government agencies. businesses used writing style guides text create maintain distinctive style across writers.BBC\nInfographics Guidelines\nR Cookbook\n{bbplot}\nBBCInfographics GuidelinesR Cookbook{bbplot}EconomistThe EconomistThe Office National StatisticsThe Office National StatisticsEurostatEurostatUrban InstituteUrban InstituteThe Pudding (learning resources)Pudding (learning resources)level detail technicality varies wildly examples. instance, BBC provide strong guidelines details final visualisation provide lot technical tools advice construct consistent way across corporation. ’ve even gone far write theme ggplot publish R package!","code":""},{"path":"edav-visualisation.html","id":"wrapping-up-4","chapter":"11 Data Visualisation","heading":"11.7 Wrapping Up","text":"🔨 Think tools.📽 Think medium.👥 Think audience.📖 Think story.📝 Think guidelines.Data visualisation might seem like soft skill comparison data acquisition, wrangling modelling. However, often effective visualisations greatest real world impact.regularly highly effective figures within reports presentations determine projects funded renewed. Similarly, visualisations press releases can determine whether result study trusted, correctly interpreted, remembered wider public.constructing visualisations important consider whether existing guidelines provide helpful constraints work. , determine story wish tell exactly telling story . decided can select medium tools use craft visualisation greatest chance achieving intended effect.","code":""},{"path":"edav-checklist.html","id":"edav-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-checklist.html","id":"videos-chapters-2","chapter":"Checklist","heading":"11.8 Videos / Chapters","text":"Data Wrangling (20 min) [slides]Data Wrangling (20 min) [slides]Data Exploration (25 min) [slides]Data Exploration (25 min) [slides]Data Visualisation (27 min) [slides]Data Visualisation (27 min) [slides]","code":""},{"path":"edav-checklist.html","id":"reading-2","chapter":"Checklist","heading":"11.9 Reading","text":"Use Data Exploration Visualisation section reading list support guide exploration week’s topics. Note texts divided core reading, reference materials materials interest.","code":""},{"path":"edav-checklist.html","id":"activities","chapter":"Checklist","heading":"11.10 Activities","text":"Core:NormConf conference dedicated unglamorous essential aspects working data sciences. December 2022 conference talks available Youtube Playlist. Find talk interests watch , post short summary EdStem, includes learned talk one thing still understand.NormConf conference dedicated unglamorous essential aspects working data sciences. December 2022 conference talks available Youtube Playlist. Find talk interests watch , post short summary EdStem, includes learned talk one thing still understand.Work ggplot2 tutorial beautiful plotting R Cédric Scherer, recreating examples .Work ggplot2 tutorial beautiful plotting R Cédric Scherer, recreating examples .Using rolling_mean() function inspiration, write rolling_sd() function calculates rolling standard deviation numeric vector.Extend rolling_sd() function optionally return approximate point-wise confidence bands rolling standard deviations. \\(\\pm2\\) standard errors default may computed using analytical re-sampling methods.\nCreate visualisation using extended rolling_sd() function assess whether variability daily change Dow Jones Index changing time. [data]\nUsing rolling_mean() function inspiration, write rolling_sd() function calculates rolling standard deviation numeric vector.Extend rolling_sd() function optionally return approximate point-wise confidence bands rolling standard deviations. \\(\\pm2\\) standard errors default may computed using analytical re-sampling methods.Extend rolling_sd() function optionally return approximate point-wise confidence bands rolling standard deviations. \\(\\pm2\\) standard errors default may computed using analytical re-sampling methods.Create visualisation using extended rolling_sd() function assess whether variability daily change Dow Jones Index changing time. [data]Create visualisation using extended rolling_sd() function assess whether variability daily change Dow Jones Index changing time. [data]Bonus:Add rolling_sd() function R package, adding documentation tests.Add rolling_sd() function R package, adding documentation tests.exploratory analysis, often need assess validity assumed distribution based sample data. Write versions qqnorm() qqplot(), add point-wise tolerance intervals assess whether deviation line \\(y=x\\) larger expected.exploratory analysis, often need assess validity assumed distribution based sample data. Write versions qqnorm() qqplot(), add point-wise tolerance intervals assess whether deviation line \\(y=x\\) larger expected.Add versions qqnorm() qqplot() R package, along documentation tests.Add versions qqnorm() qqplot() R package, along documentation tests.","code":""},{"path":"edav-checklist.html","id":"live-session-2","chapter":"Checklist","heading":"11.11 Live Session","text":"live session begin discussion week’s tasks. break small groups two data visualisation exercises.(Note: one exercises, helpful bring small selection coloured pens pencils, access . , please don’t worry - inventive use black, blue shading perfectly acceptable alternatives!)Please come live session prepared discuss following points:NormConf video watch learn ?NormConf video watch learn ?ggplot2, else used create data visualisations? relative strengths weaknesses?ggplot2, else used create data visualisations? relative strengths weaknesses?implement rolling_sd() function conclusions draw applying Dow Jones data?implement rolling_sd() function conclusions draw applying Dow Jones data?","code":""},{"path":"production-introduction.html","id":"production-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-reproducibility.html","id":"production-reproducibility","chapter":"12 Reproducibility","heading":"12 Reproducibility","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-reproducibility.html","id":"the-data-scientific-method","chapter":"12 Reproducibility","heading":"12.1 The Data Scientific Method","text":"covered far much focused first aspect data science: data. come consider whether work can reproduced results can replicated, shifts focus second , science.\nFigure 10.1: Cycle scientific enqiry.\ndata scientists, like think applying scientific method work.start question want answer problem want solve. followed search existing literature: well-known problem lots people solved ? , fantastic, can learn efforts. , proceed gather evidence combine whatever existing knowledge scrape together. Finally, draw conclusions synthesised information.acknowledge conclusions reach truth, just current best approximation . usefully simplified model messy reality can share world. happily update model become aware new evidence, whether new information supports contradicts current way thinking.sounds excellent , ideal world, science data science progress. However, just like models simplified (case idealised) description really happens.","code":""},{"path":"production-reproducibility.html","id":"issue-multiple-dependent-tests","chapter":"12 Reproducibility","heading":"12.2 Issue: Multiple, Dependent Tests","text":"Projects usually single hypothesis testProjects usually single hypothesis testSequence dependent decisionsSequence dependent decisionse.g. Model developmente.g. Model developmentCan fool looking lots times ignoring sequential dependent structure.Can fool looking lots times ignoring sequential dependent structure.aims data science project rarely framed clear unambiguous hypothesis, design study perform single statistical analysis. Apart special cases, like /B testing, much general aim data science projects.Gelmand Loken (2013) consider issue multiple comparisonsWe might want construct model given phenomenon ’ll try many variants model along way. taking relaxed approach data analysis, data scientists can run risk finding spurious relationships within data don’t hold generally. look relationship quantity trying predict enough samples random noise almost surely find significant relationship, even though true link.","code":""},{"path":"production-reproducibility.html","id":"issues-p-hacking-and-publiction-bias","chapter":"12 Reproducibility","heading":"12.3 Issues: \\(p\\)-hacking and Publiction Bias","text":"Distribution p-values medical publications. Perneger Combescure (2017)Okay, methods investigation data scientists might completely sound, balanced results studies exist, right? Well, second worrisome aspect process can’t always trust published literature fair representation people tried past.Studies don’t provide strong evidence null hypothesis rarely make publications, reports news. largely way scientific enquiry rewarded academy, business media. Funding attention tend go studies new findings, rather aim confirm strengthen findings existing studies.systemic problem incentives scientists ‘massage’ numbers obtain \\(p\\)-value less 0.05 result can reported statistically significant. process known \\(p\\)-hacking can occur deliberate malpractice often ’s result scientists generally receiving adequate training statistics. statistically trained data scientists know declaration significance indication meaningful effect size conventional significance level 5% entirely arbitrary. However, need aware case across science even aren’t immune societal systematic influences favour publication novel results confirmatory ones.influences also lead much subtle problem direct \\(p\\)-hacking. Consider model obvious unnecessary additional property add: example might adding unnecessary term regression. extension low hanging fruit, many scientists independently design experiments test . experiments provide insufficient evidence null hypothesis don’t get developed published papers technical reports, just support status-quo. However, enough people attempted scientist get “lucky” find significant (potentially relevant) benefit. experiments done, one makes onto public record.studies left proverbial desk drawer induces publication bias scientific literature. come assess state existing knowledge, unable properly assess importance findings, lack context null results went unreported.process means results many scientific studies expect recreated. known scientific replication crisis.","code":""},{"path":"production-reproducibility.html","id":"reproducibility-1","chapter":"12 Reproducibility","heading":"12.4 Reproducibility","text":"Reproducibility: given original raw data code, can get results ?Reproducible != CorrectReproducible != Correct“Code available request” new “Data available request”“Code available request” new “Data available request”Reproducible data analysis requires effort, time skill.Reproducible data analysis requires effort, time skill.idea reproducibility requires us able recover exact numerical summaries original investigator. particular means able reproduce exact point estimates measures uncertainty , ensures ’ll draw conclusions original investigator.putting work production several reasons might require reproducible. first logistical: production code needs robust efficient - often means code re-factored, rewritten translated another language. results reproducible way verify done correctly. Secondly, problem identified work (say customer raises complaint loan application incorrectly rejected) need able accurately recreate instance diagnose problem exactly caused .Note just findings reproducible, doesn’t means imply ’re correct. well documented flawed analysis entirely reproducible also completely unsuitable just plain wrong.data science projects, already taken several steps greatly improve reproducibility work. Although scripted, tested documented work improve management project, decisions improve scientific quality work . puts us strong position relative scientific literature whole.point now almost standard publish data along papers, long time case data data available , request. now similar situation comes code. ’s still far standard analysis code required put detailed scrutiny part peer-review process.little context isn’t unreasonable. Across many scientific disciplines, code-based approaches analysis standard; statistical software graphical user interface used instead. idea allow scientists analyse data providing tools trough combination menus buttons. However, interfaces often leave record data manipulated software can highly specialised proprietary. combination means even full datasets provided, often impossible others reproduce original analysis.None meant scold disparage scientists use type software allow perform statistical analyses. ’re well aware much time effort takes learn use implement statistical methods correctly. time scientists invest learning subject, can get point research first place. one wonders data science: ability work multi-disciplinary teams individual members specialised different areas.need pause check , fate can easily befall us data scientists. Yes, take time learn skills practices ensure reproducibility, also takes time implement time expert data practitioner doesn’t come cheap. wait end project make reproducible ’ll usually late - time money run .","code":""},{"path":"production-reproducibility.html","id":"replicability","chapter":"12 Reproducibility","heading":"12.5 Replicability","text":"Replicable: experiment repeated independent investigator, get slightly different data substative conclusions ?specific sense, core worry statistician!specific sense, core worry statistician!Also used generally: results stable perturbations population / study design / modelling / analysis?Also used generally: results stable perturbations population / study design / modelling / analysis?real test try . Control risk shadow parallel deployment.\nStatisticians well aware repeat experiment get slightly different data. lead slightly different estimates slightly different results.real test try . Control risk shadow parallel deployment.\nStatisticians well aware repeat experiment get slightly different data. lead slightly different estimates slightly different results.Ultimately, core problem statisticians get paid worry : changes small enough substantive conclusions impacted? Yes, point estimates vary slightly conclusions existence, direction magnitude effect still hold? Alternatively, estimating relationship two variables, functional form chosen suitable?general scientific context, replication takes broad meaning asks whether key properties results replicated another person. context getting work put production, acre concerned whether results findings also hold applied future instances might differ seen already.come data science statistical background well accustomed sorts considerations. Whenever perform hypothesis test compare two models, take steps make sure comparison valid particular sample, also true --sample. whole reason data scientists make training test sets first place, approximate test sort generalisation. things asking : good performance replicate different inputs?course train-test split, bootstrap resampling asymptotic arguments can ever approximate ways sample differs production population, models applied. way truly assess --sample performance models generalisability findings put work production.opens us risk: findings don’t generalise new model actually much worse current one? ’s possible deploy new model also avoid risk entirely. However, can take steps mitigate level exposure.","code":""},{"path":"production-reproducibility.html","id":"shadow-deployment","chapter":"12 Reproducibility","heading":"12.5.1 Shadow deployment","text":"risk-adverse setting might implement shadow deployment new model. case, current model still used decision making candidate model also run background can see might behave wild.\ngood can identify points catastrophic failure new model, also expensive run can give us limited information.Suppose, example, model recommender system retail website. shadow deployment let us check new system functions correctly can gather data products recommended customer investigate differ recommended current system. shadow deployment case tell us customer done shown products instead. means shadow deployment doesn’t allow investigate whether new system leads equivalent greater revenue current system.","code":""},{"path":"production-reproducibility.html","id":"parallel-deployment","chapter":"12 Reproducibility","heading":"12.5.2 Parallel deployment","text":"Parallel deployment /B tests current proposed new models running time. allows us truly test whether findings generalise production population controlling level risk exposure setting proportion times model used. instances assign new model faster learn performance also increases risk exposure.","code":""},{"path":"production-reproducibility.html","id":"reproduction-and-replication-in-statistical-data-science","chapter":"12 Reproducibility","heading":"12.6 Reproduction and Replication in Statistical Data Science","text":"","code":""},{"path":"production-reproducibility.html","id":"monte-carlo-methods","chapter":"12 Reproducibility","heading":"12.6.1 Monte Carlo Methods","text":"data science rely lot use stochastic methods. often used increase chance findings replicated another person production. However, also make difficult ensure exact results can reproduced, whether another person future selves.\nFigure 10.2: Monte Carlo approximation \\(\\pi\\)\nMonte Carlo methods modelling, estimation approximation technique leverages randomness way.seen examples improve probability successful replication. obvious example random allocation data Train/Test split model selection.Another example focused improving replication use bootstrap resampling approximate sampling distribution test statistic. might parametric bootstrap, alternative datasets generated sampling values fitted model observed data. Alternatively, non-parametric bootstrap generate alternative datasets sampling original data replacement.Monte Carlo methods can also used express uncertainties generally, approximate difficult integrals. common applications Monte Carlo methods Bayesian modelling, (unless models particularly simple) posterior posterior-predictive distribution approximated collection values sampled distribution.time run analyses ’ll get slightly different outcomes. work replicable need quantify level variation. example, different sample posterior distribution, much estimated posterior mean change ? Sometimes, case, can appeal law large numbers help us . take samples, variation realisations shrink. can collect enough samples estimated mean stable across realisations, desired number significant figures.make results reproducible ensure can reproduce remaining, unstable digits particular realisation. can setting seed random number generator, idea return shortly.","code":""},{"path":"production-reproducibility.html","id":"optimisation","chapter":"12 Reproducibility","heading":"12.6.2 Optimisation","text":"Optimisation second aspect data science can difficult ensure reproducible replicable.optimum find stable :runs procedure?starting points?step size / learning rate?realisations data?poorly drawn contour plot. Local modes make optimiation unstable choice starting point.optimisation routines used parameter estimation, ensure results find particular data set reproducible given configuration. might given data set, initial set starting parameters step size, learning rate (controls step size changes) maximum number iterations perform.additionally need concerned replication . chosen different starting point optimisation still converge, converge mode? original method may found local optimum can confirm global optimum?optimisation fails converge, can reproduce case diagnose problem? can particularly tricky optimisation routine uses Monte Carlo methods, stochastic gradient descent simulated annealing.","code":""},{"path":"production-reproducibility.html","id":"pseudo-random-numbers","chapter":"12 Reproducibility","heading":"12.6.3 (Pseudo-)Random Numbers","text":"Sometimes stochastic elements work can’t use brute force eliminate. Perhaps beyond computational abilities else number realisations important, fixed aspect study.Fortunately, computing ’s common truly random numbers. Instead, usually complex deterministic function generates sequence numbers statistically indistinguishable series independent random variates.next term sequence pseudo-random numbers generated generated based value current one. means , setting starting point, can always get sequence pesudo-random variates time run code. R set starting point using set.seed()especially useful simulations involve random variables, allows us recreate results exactly. makes possible others recreate results can also make much easier test debug code.","code":"\n# different values\nrnorm(n = 4)\n#> [1] -0.02262899  1.32967063  0.43357377  0.58909999\nrnorm(n = 4)\n#> [1] -0.7094037  0.9837464 -0.5024576  0.3385974\n# the same value\nset.seed(1234)\nrnorm(n = 4)\n#> [1] -1.2070657  0.2774292  1.0844412 -2.3456977\n\nset.seed(1234)\nrnorm(n = 4)\n#> [1] -1.2070657  0.2774292  1.0844412 -2.3456977"},{"path":"production-reproducibility.html","id":"beware","chapter":"12 Reproducibility","heading":"12.7 Beware","text":"running code sequentially interactively, setting seed need solve reproducibility problems. However, ’d advise take great care combining methods ’ll see speeding code. cases, strategies optimize code performance can interfere generation random numbers lead unintended results.writing code ’s executed parallel across multiple cores processors, carefully consider whether give seed value. correct decision context specific depends interpretation random variates generating. making comparison iterations might important random aspects kept similar possible, paralleling speed gains might important .Finally, ’s important wary quality pseudo-random number generation interfacing R programming languages. R developed statistical programming language languages statistically focused. Different languages may use different algorithms generating pseudo-random numbers, quality generated numbers can vary. ’s important make sure seeds appropriately passed languages ensure correct sequence random numbers generated.","code":""},{"path":"production-reproducibility.html","id":"wrapping-up-5","chapter":"12 Reproducibility","heading":"12.8 Wrapping Up","text":"get work put production reproducible replicable.Reproducible: can recreate results code dataReproducible: can recreate results code dataReplicable: core results remain valid using different dataReplicable: core results remain valid using different dataWhile randomness key part data science workflows can lead reproducibility nightmares. can manage appealing stability averages large samples explicitly setting sequence pseudo-random numbers generate using set.seed().Finally, take special care combine efficiency replicable workflows.ideas can now use good data good science.","code":""},{"path":"production-explainability.html","id":"production-explainability","chapter":"13 Explainability","heading":"13 Explainability","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-explainability.html","id":"what-are-we-explaining-and-to-whom","chapter":"13 Explainability","heading":"13.1 What are we explaining and to whom?","text":"many reasons might need explain behaviour model can put production. example, can consider credit scoring system determines whether customers given line credit.Regulatory legal requirements describe model works (e.g. ban “black-box” modelling).Understanding model works improve .Explaining individual load decisions customers.cases, exactly mean explanation? ’s likely thing example.Data scientists might interested know exactly types mapping covariates responses can represented neural network architecture underlying credit scoring system.Data scientists might interested know exactly types mapping covariates responses can represented neural network architecture underlying credit scoring system.Stakeholders within company regulators likely indifferent concerned understanding general behaviour model across large numbers loan applications.Stakeholders within company regulators likely indifferent concerned understanding general behaviour model across large numbers loan applications.Finally, individual customers might investment overall behaviour scoring model also like know actions can take increase chance securing loan.Finally, individual customers might investment overall behaviour scoring model also like know actions can take increase chance securing loan.examples, level technical detail differs importantly fundamental nature explanations different.","code":""},{"path":"production-explainability.html","id":"explaining-a-decision-tree","chapter":"13 Explainability","heading":"13.2 Explaining a Decision Tree","text":"\nFigure 10.1: example decision tree, optimised correctly identify category 1 ambulance calls questions possible.\nmodels giving explanation relatively straightforward. Decision trees perhaps easiest model explain mimic human decision making can represented like flow-charts make sequential, linear partitions predictor space.models use sort logic used medical triage call ambulance, determine urgency call. binary decisions used type triage optimised identify critical calls soon possible, just one form loss function use. might instead pick partitions get accurate overall classification calls urgency categories. might appropriate loss function ambulance calls might deciding loan applicants grant credit .issue decision trees limited relationships can represent (linear relationships approximated step function) sensitive small changes training data. overcome deficiencies can use bootstrap aggregation random forest model make predictions based collection trees. leads models stable flexible also removes chance simple human-friendly explanation.","code":""},{"path":"production-explainability.html","id":"explaining-regression-models","chapter":"13 Explainability","heading":"13.3 Explaining Regression Models","text":"Another model relatively straightforward interpret linear regression. can interpret model using estimated regression coefficients, describe predicted outcome changes unit change covariate values covariates held constant.\nFigure 10.2: Linear models global, conditional explanations, provided estimated regression coefficients.\nglobal conditional explanation.global effect increasing covariate one unit matter starting value covariate. explanation parts covariate space.explanation conditional assumes values held constant. can lead odd behaviour explanations, dependent terms included (left ) model.can contrasted non-linear regression, covariate effects still interpreted conditional value covariates size direction effect might vary depending value covariate.\nFigure 11.1: Non-linear models local, conditional explanations, provided estimated regression coefficients.\nexample unit increase covariate associated large change model response low values covariate, much smaller change large values covariate.","code":""},{"path":"production-explainability.html","id":"example-cherrywood-regression","chapter":"13 Explainability","heading":"13.4 Example: Cherrywood regression","text":"example can look height, girth volume cherry trees.wanting use lathe produce pretty, cherry wood ornaments might interested understanding girth trees varies height total volume. Using linear model, see positive linear association girth.\nFigure 10.3: Cherry tree girth can well modelled linear function either tree height harvestable volume\nHowever, include terms model, interpretation changes dramatically.Height longer positively associated girth. size, direction significance estimated effects conditional terms included model. fixed volume wood, taller tree necessarily smaller girth.Techniques SHAP try quantify importance predictor averaging combinations predictors might included within model. can read techniques Interpretable Machine Learning Christoph Molnar.","code":"\nlibrary(tidyverse)\nlibrary(ggtext)\n\nlm_height <- lm(Girth ~ 1 + Height, data = trees)\nlm_volume  <- lm(Girth ~ 1 + Volume, data = trees)\n\np1 <- ggplot(data = trees, aes(x = Height, y = Girth)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Height\", y = \"Girth\") +\n  ylim(c(8,22)) +\n  ggtitle(\"Cherry Tree Dimensions\")+\n  theme_minimal()\n\np2 <- ggplot(data = trees, aes(x = Volume, y = Girth)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Volume\", y = \"Girth\") +\n  ylim(c(8,22)) +\n  theme_minimal()\n\ncowplot::plot_grid(p1, p2, ncol = 2)\nlm(Girth ~ 1 + Height, data = trees)\n#> \n#> Call:\n#> lm(formula = Girth ~ 1 + Height, data = trees)\n#> \n#> Coefficients:\n#> (Intercept)       Height  \n#>     -6.1884       0.2557\nlm(Girth ~ 1 + Volume, data = trees)\n#> \n#> Call:\n#> lm(formula = Girth ~ 1 + Volume, data = trees)\n#> \n#> Coefficients:\n#> (Intercept)       Volume  \n#>      7.6779       0.1846\nlm(Girth ~ 1 + Height + Volume, data = trees)\n#> \n#> Call:\n#> lm(formula = Girth ~ 1 + Height + Volume, data = trees)\n#> \n#> Coefficients:\n#> (Intercept)       Height       Volume  \n#>    10.81637     -0.04548      0.19518"},{"path":"production-explainability.html","id":"simpsons-paradox","chapter":"13 Explainability","heading":"13.5 Simpson’s Paradox","text":"effect related Simpson’s Paradox, trend appears several groups data disappears reverses groups combined.regularly arises fields like epidemiology, population level trends assumed apply level individuals small groups, known ecological fallacy.Actually, Simpson’s parodox terrible name, isn’t actually paradox . ’s surprising two different answers two different questions, supposed contradiction arises fail distinguish questions.\nFigure 13.1: simulated example covariate negative trend population level positive trend within sub-group.\nhope highlighted simplest models might use data scietists, explanations much possible must made care attention detail - correctly interpreting models context can far straightforward.","code":"\ndata_1 <- mgcv::rmvn(n = 100, mu = c(0,4), V = matrix(c(2,0.8,0.8,2), nrow = 2))\ndata_2 <- mgcv::rmvn(n = 100, mu = c(2,2), V = matrix(c(2,0.8,0.8,2), nrow = 2))\ndata_3 <- mgcv::rmvn(n = 100, mu = c(4,0), V = matrix(c(2,0.8,0.8,2), nrow = 2))\n\nsimpson <- data.frame(\n  x_1 = c(data_1[,1], data_2[,1], data_3[,1]), \n  x_2 = c(data_1[,2], data_2[,2], data_3[,2]),\n  group = as.factor(rep(1:3, each = 100))\n)\n\np3 <- simpson %>% \n  ggplot(aes(x = x_1, y = x_2)) +\n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  ggtitle(\"Simpson's Paradox\", \"Groupings Unknown\") +\n  theme_minimal()\n\np4 <- simpson %>% \n  group_by(group) %>% \n  ggplot(aes(x = x_1, y = x_2, col = group)) +\n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  ggtitle(\"\", \"Groupings Unknown\") +\n  theme_minimal()\n\ncowplot::plot_grid(p3, p4, ncol = 2)\n#> `geom_smooth()` using formula = 'y ~ x'\n#> `geom_smooth()` using formula = 'y ~ x'"},{"path":"production-explainability.html","id":"what-hope-do-we-have","chapter":"13 Explainability","heading":"13.6 What hope do we have?","text":"stage, might asking hope explaining complex models like random forests neural networks, given difficuly explain even simple models might take benchmark. ’d right worry important remain humble can know complex systems building.hope isn’t lost though - still tricks sleeve!","code":""},{"path":"production-explainability.html","id":"permutation-testing","chapter":"13 Explainability","heading":"13.6.1 Permutation Testing","text":"Suppose ’re asked product manager determine predictors covariates usually import model determining credit scores loan outcomes.One way remove covariate model investigate changes predictions made model. However, ’ve seen already removing predictor can substantively change models.instead, answer question using permutation methods.take observed values covariate, say income, randomly allocate among training examples destroy association income loan outcome. allows us remove information provided income without altering overall structure model. can refit model modified data set investigate rearranging covariate alters predictions. large performance drop, covariate playing important role within model.many variations sort permutation test. can simple powerful tools understanding behaviour sorts model.","code":""},{"path":"production-explainability.html","id":"meta-modelling","chapter":"13 Explainability","heading":"13.6.2 Meta-modelling","text":"Meta-models , name suggests, models models can effective methods providing localised explanations complex models.idea look small region covariate space covered complex highly flexible model, neural network. can’t easily give global explanation complex model’s behaviour - just complicated.However, can interrogate model’s behaviour within small region construct simplified version model (meta-model) lets us explain model within small region. Often meta-model chosen linear model.\nFigure 13.2: local linear approximation two dimensions\npartly convenience familiarity also theoretical backing: complex model sufficiently smooth can appeal Taylor’s Theorem say small enough region mapping can well approximated linear function.sort local model explanation particularly useful want explain individual predictions decisions made model: example single loan applicant granted loan. inspecting coefficients local surrogate model can identify covariates influential decision suggest applicant increase chance success summarising covariates influential within ability applicant change. exactly approach taken LIME methodology developed Ribiero et al. ","code":""},{"path":"production-explainability.html","id":"aggregating-meta-models","chapter":"13 Explainability","heading":"13.6.3 Aggregating Meta-models","text":"Using local conditional explanations model’s behaviour can useful circumstances don’t give broader understanding going overall. want know covariate influences outcomes particular one? care covariate’s expected effect loan applicants, distribution effects applicants?local conditional explanations particularly nice. making explanations many points can aggregate explanations understand global marginal behaviour models.aggregate conditional effects marginal effect, local effect global effect must integrate joint distribution covariates. sounds bit scary , right. Integration hard enough best times without adding fact don’t know joint distribution covariates using predictors.Don’t worry though, can take easy way things approximately. can approximate joint distribution covariates empirical distribution observe sample, nasty integrals simplify averages measurement units data (loan applicants case).construct local, conditional model loan applicant data set, can approximate marginal effect covariate averaging conditional effects obtain loan applicant.\nFigure 13.3: Local approximations around observation can combined understand global model behaviour.\ngives us global understanding covariate influences response model. possible values covariates appropriately weights according frequency within sample (also within population, sample representative).","code":""},{"path":"production-explainability.html","id":"wrapping-up-6","chapter":"13 Explainability","heading":"13.7 Wrapping Up","text":"’ve seen models become flexible also become difficult explain, whether , subject expert user model.’s say simple models always easy explain interpret, simple models can deceptively tricky communicate accurately.Finally, looked couple techniques explaining complex models.can use permutation tests measure feature importance models: shuffling predictor values breaks relationship response, can measure much degrades model performance. big dip implies feature lot explaining.can also look local behaviour models making surrogate meta models, interpretable, aggregate understand model globally.Effective explanations essential want model used production feed real decisions decision making. requires level skill rhetoric tailor explanation clear person requested . isn’t soft skill, also requires surprising amount computational mathematical skill extract explanations complex modern models.\n","code":""},{"path":"production-scalability.html","id":"production-scalability","chapter":"14 Scalability","heading":"14 Scalability","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-scalability.html","id":"scalability-and-production","chapter":"14 Scalability","heading":"14.1 Scalability and Production","text":"put production code gets used data. likely consider scalability methods terms ofComputation timeComputation timeMemory requirementsMemory requirementsWhen balance trade-development costs usage costs.","code":""},{"path":"production-scalability.html","id":"example-bayesian-inference","chapter":"14 Scalability","heading":"14.1.1 Example: Bayesian Inference","text":"MCMC originally takes ~24 hoursMCMC originally takes ~24 hoursIdentifying amending bottlenecks code reduced ~24 minutes.Identifying amending bottlenecks code reduced ~24 minutes.actually better? depend number factors, including:human hours investedfrequency usesafe / stable / general / readabletrade scalability","code":""},{"path":"production-scalability.html","id":"knowing-when-to-worry","chapter":"14 Scalability","heading":"14.1.2 Knowing when to worry","text":"Sub-optimal optimisation can worse nothing… programmers spent far much time worrying efficiency wrong places wrong times; premature optimisation root evil (least ) programming. - Donald Knuth","code":""},{"path":"production-scalability.html","id":"our-focus","chapter":"14 Scalability","heading":"14.1.3 Our Focus","text":"Writing code scales well terms computaiton time memory used huge topic. section restrict aims :Basic profiling find bottlenecks.Strategies writing scalable (R) code.Signpost advanced methods & reading.","code":""},{"path":"production-scalability.html","id":"basics-of-code-profiling","chapter":"14 Scalability","heading":"14.2 Basics of Code Profiling","text":"","code":""},{"path":"production-scalability.html","id":"r-as-a-stopwatch","chapter":"14 Scalability","heading":"14.2.1 R as a stopwatch","text":"simplest way profile code time long takes run. three common ways .Firstly, record time code starts executing, time completes look difference .system.time function provides shorthand code runs sequentially extends functionality work parallel code .tictoc package similar features, also allows add intermediate timers understand parts code taking time run.tictoc can get fancyIf code already fast (run many times, efficiency gains required) methods may fail sample state code high enough frequency. cases might want explore {mircobenchmark} package.","code":"\nt_start <- Sys.time()\nSys.sleep(0.5) # YOUR CODE\nt_end <- Sys.time()\n\nt_end - t_start\n#> Time difference of 0.510226 secs\nsystem.time(\n  Sys.sleep(0.5)\n)\n#>    user  system elapsed \n#>   0.000   0.000   0.504\nlibrary(tictoc)\n\ntic() \nSys.sleep(0.5) # YOUR CODE \ntoc()\n#> 0.505 sec elapsed\ntic(\"total\")\ntic(\"first, easy part\")\nSys.sleep(0.5)\ntoc(log = TRUE)\n#> first, easy part: 0.51 sec elapsed\ntic(\"second, hard part\")\nSys.sleep(3)\ntoc(log = TRUE)\n#> second, hard part: 3.01 sec elapsed\ntoc()\n#> total: 3.527 sec elapsed"},{"path":"production-scalability.html","id":"profiling-your-code","chapter":"14 Scalability","heading":"14.3 Profiling Your Code","text":"diagnose scaling issues understand code .Stop code time \\(\\tau\\) examine call-stack.\ncurrent function evaluated, function called , function called , …, top level function.\nStop code time \\(\\tau\\) examine call-stack.current function evaluated, function called , function called , …, top level function.lot can measure (estimate) proportion working memory (RAM) uses time time spent evaluating function.lot can measure (estimate) proportion working memory (RAM) uses time time spent evaluating function.","code":"\nlibrary(profvis)\nlibrary(bench)"},{"path":"production-scalability.html","id":"profiling-toy-example","chapter":"14 Scalability","heading":"14.3.1 Profiling: Toy Example","text":"Suppose following code file called prof-vis-example.R.call stack f() look something like .can examine true call stack using profvis() function profvis package. saving code separate file sourcing session, function also give us line--line information time memory demands code.upper histogram lower flame plot can see majority time spent pause() h(). careful upper plot shows total amount time function call, h() appears take longer g(), called often code snippet profiling.","code":"\n\nh <- function() {\n  profvis::pause(1)\n}\n\ng <- function() {\n  profvis::pause(1)\n  h()\n}\n\nf <- function() {\n  profvis::pause(1)\n  g()\n  profvis::pause(1)\n  h()\n}\nsource(\"prof-vis-example.R\")\nprofvis::profvis(f())"},{"path":"production-scalability.html","id":"notes-on-time-profiling","chapter":"14 Scalability","heading":"14.4 Notes on Time Profiling","text":"get slightly different results time run functionChanges internal state computerUsually big deal, mainly effects fastest parts codeBe careful stochastic simulationsUse set.seed() make fair comparison many runs.","code":""},{"path":"production-scalability.html","id":"source-code-and-compiled-functions","chapter":"14 Scalability","heading":"14.4.1 Source code and compiled functions","text":"write function can see source function calling ’s nameThis equally true functions within packages.functions use compiled code written another language. case dplyr’s function, calls compiled C++ code final line.also true many functions base R, (obvious reason) R source code.compiled functions R source code, profiling methods used don’t extend compiled code. See {jointprof} really need profiling functionality.","code":"\npad_with_NAs <- function(x, n_left, n_right){\n  c(rep(NA, n_left), x, rep(NA, n_right))\n}\npad_with_NAs\n#> function(x, n_left, n_right){\n#>   c(rep(NA, n_left), x, rep(NA, n_right))\n#> }\neds::rnorm_rounded\n#> function (n, mu = 0, sigma = 1, digits = 0) \n#> {\n#>     raw_values <- stats::rnorm(n, mean = mu, sd = sigma)\n#>     rounded_values <- base::round(raw_values, digits)\n#>     return(rounded_values)\n#> }\n#> <bytecode: 0x7fe36bb6f820>\n#> <environment: namespace:eds>\ndplyr::between\n#> function (x, left, right) \n#> {\n#>     if (!is.null(attr(x, \"class\")) && !inherits(x, c(\"Date\", \n#>         \"POSIXct\"))) {\n#>         warn(\"between() called on numeric vector with S3 class\")\n#>     }\n#>     if (length(left) != 1) {\n#>         abort(\"`left` must be length 1\")\n#>     }\n#>     if (length(right) != 1) {\n#>         abort(\"`right` must be length 1\")\n#>     }\n#>     if (!is.double(x)) {\n#>         x <- as.numeric(x)\n#>     }\n#>     .Call(dplyr_between, x, as.numeric(left), as.numeric(right))\n#> }\n#> <bytecode: 0x7fe36c636130>\n#> <environment: namespace:dplyr>\nmean\n#> function (x, ...) \n#> UseMethod(\"mean\")\n#> <bytecode: 0x7fe3621cec88>\n#> <environment: namespace:base>"},{"path":"production-scalability.html","id":"memory-profiling","chapter":"14 Scalability","heading":"14.5 Memory Profiling","text":"profvis() can similarly measure memory usage code.Copy--modify behaviour makes growing objects slow.Pre-allocate storage possible.Strategies structures, see R inferno Effecient R.","code":"\nx <- integer()\nfor (i in 1:1e4) {\n  x <- c(x, i)\n}"},{"path":"production-scalability.html","id":"tips-to-work-at-scale","chapter":"14 Scalability","heading":"14.6 Tips to work at scale","text":"TL;DR: pick object types carefully, vectorise code last resort implement code faster language.","code":""},{"path":"production-scalability.html","id":"vectorise","chapter":"14 Scalability","heading":"14.6.1 Vectorise","text":"Two bits code task, second much faster, involves fewer function calls.possible write use functions take advantage vectorised inputs. E.g.careful recycling!","code":"\nx <- 1:10\ny <- 11:20 \nz <- rep(NA, length(x))\n\nfor (i in seq_along(x)) {\n  z[i] <- x[i] * y[i]\n}\nx <- 1:10\ny <- 11:20 \nz <- x * y\nrnorm(n = 100, mean = 1:10, sd = rep(1, 10))"},{"path":"production-scalability.html","id":"linear-algebra","chapter":"14 Scalability","heading":"14.6.2 Linear Algebra","text":"vectorising: Noam Ross Blog Post","code":"\nX <- diag(x = c(2, 0.5))\ny <- matrix(data = c(1, 1), ncol = 1)\n\nX %*% y\n#>      [,1]\n#> [1,]  2.0\n#> [2,]  0.5"},{"path":"production-scalability.html","id":"for-loops-in-disguise","chapter":"14 Scalability","heading":"14.7 For loops in disguise","text":"","code":""},{"path":"production-scalability.html","id":"the-apply-family","chapter":"14 Scalability","heading":"14.7.1 The apply family","text":"Functional programming equivalent loop. [apply(), mapply(), lapply(), …]Apply function element list-like object.generalises functions matrixStats","code":"\nA <- matrix(data = 1:12, nrow = 3, ncol = 4)\nA\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    4    7   10\n#> [2,]    2    5    8   11\n#> [3,]    3    6    9   12\n# MARGIN = 1 => rows,  MARGIN = 2 => columns\napply(X = A, MARGIN = 1, FUN = sum)\n#> [1] 22 26 30\nrowSums(A)\n#> [1] 22 26 30"},{"path":"production-scalability.html","id":"purrr","chapter":"14 Scalability","heading":"14.7.2 {purrr}","text":"Iterate single object map().Iterate multiple objects map2() pmap().details variants see Advanced R chapters 9-11 functional programming.","code":"\nmu <- c(-10, 0, 10)\npurrr::map(.x = mu, .f = rnorm, n = 5)\n#> [[1]]\n#> [1]  -8.764951 -11.768746 -10.280888  -9.920437 -10.114642\n#> \n#> [[2]]\n#> [1] -0.4973223 -0.3569849 -0.2116080 -0.2053137  0.2513292\n#> \n#> [[3]]\n#> [1]  9.189865 10.953393  9.811637 10.641173 10.568341\nmu <- c(-10, 0, 10)\nsigma <- c(0, 0.1, 0)\npurrr::map2(.x = mu, .y = sigma, .f = rnorm, n = 5)\n#> [[1]]\n#> [1] -10 -10 -10 -10 -10\n#> \n#> [[2]]\n#> [1] -0.17084728 -0.02850995  0.05385526  0.11920443  0.14579953\n#> \n#> [[3]]\n#> [1] 10 10 10 10 10\nmu <- c(-10, 0, 10)\nsigma <- c(0, 0.1, 0)\n\npurrr::pmap(\n  .f = rnorm, \n  n = 5,\n  .l = list(\n    mean = mu, \n    sd = sigma))\n#> [[1]]\n#> [1] -10 -10 -10 -10 -10\n#> \n#> [[2]]\n#> [1] -0.05578003  0.06207073 -0.14192287  0.11185182 -0.15145440\n#> \n#> [[3]]\n#> [1] 10 10 10 10 10"},{"path":"production-scalability.html","id":"easy-parallelisation-with-furrr","chapter":"14 Scalability","heading":"14.8 Easy parallelisation with furrr","text":"{parallel} {futures} allow parallel coding multiple cores.{parallel} {futures} allow parallel coding multiple cores.Powerful, steep learning curve.Powerful, steep learning curve.furrr makes easy, just add future_ purrr verbs.furrr makes easy, just add future_ purrr verbs., course excessive small example!One thing aware need careful handling random number generation relation parallelisation. many options might want set , see R-bloggers details.","code":"\nmu <- c(-10, 0, 10)\nfurrr::future_map(\n  .x = mu, \n  .f = rnorm,\n  .options = furrr::furrr_options(seed = TRUE),\n  n = 5) \n#> [[1]]\n#> [1] -10.199645 -10.812434  -8.763918 -10.270823  -7.901034\n#> \n#> [[2]]\n#> [1]  0.7616274 -1.0621565  0.2666566 -0.1408405 -1.6722262\n#> \n#> [[3]]\n#> [1] 11.63697 10.89879 10.56689 11.77466 11.07489"},{"path":"production-scalability.html","id":"sometimes-r-doesnt-cut-it","chapter":"14 Scalability","heading":"14.9 Sometimes R doesn’t cut it","text":"RCPP: API running C++ code R. Useful need:loops run orderlots function calls (e.g. deep recursion)optimised data structuresRewriting R code C++ low-level programming languages beyond scope, good know exists. Starting point: Advanced R Chapter 25.","code":""},{"path":"production-scalability.html","id":"wrapping-up-7","chapter":"14 Scalability","heading":"14.10 Wrapping up","text":"","code":""},{"path":"production-scalability.html","id":"summary-2","chapter":"14 Scalability","heading":"Summary","text":"Pick battles wiselyTarget energy profilingScale loops vectorsScale loops parallel processingScale another language","code":""},{"path":"production-scalability.html","id":"help","chapter":"14 Scalability","heading":"Help!","text":"Articles blog linksThe R inferno (Circles 2-4)Advanced R (Chapters 23-25),Efficient R (Chapter 7).","code":""},{"path":"production-checklist.html","id":"production-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-checklist.html","id":"videos-chapters-3","chapter":"Checklist","heading":"14.11 Videos / Chapters","text":"Reproducibility (26 min) [slides]Reproducibility (26 min) [slides]Explainability (16 min) [slides]Explainability (16 min) [slides]Scalability (30 min) [slides]Scalability (30 min) [slides]","code":""},{"path":"production-checklist.html","id":"reading-3","chapter":"Checklist","heading":"14.12 Reading","text":"Use Preparing Production section reading list support guide exploration week’s topics. Note texts divided core reading, reference materials materials interest.","code":""},{"path":"production-checklist.html","id":"activities-1","chapter":"Checklist","heading":"14.13 Activities","text":"week fewer activities, since working first assessment.CoreRead LIME paper, discuss live session.Read LIME paper, discuss live session.Work understanding LIME R tutorialWork understanding LIME R tutorialUse code profiling tools assess performance rolling_mean() rolling_sd() functions. Identify efficiencies can made.Use code profiling tools assess performance rolling_mean() rolling_sd() functions. Identify efficiencies can made.Bonus:Write two functions simulate homogeneous Poisson process intensity \\(\\lambda >0\\) interval \\((t_1, t_2) \\subset \\mathbb{R}\\). first use exponential distribution inter-event times simulate events sequence. second use Poisson distribution total event count first simulate number events randomly allocate locations interval. Evaluate compare reproducibility scalability implementation.","code":""},{"path":"production-checklist.html","id":"live-session-3","chapter":"Checklist","heading":"14.14 Live Session","text":"live session begin discussion week’s tasks. break small groups reading group style discussion LIME paper set reading week.","code":""},{"path":"ethics-introduction.html","id":"ethics-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-privacy.html","id":"ethics-privacy","chapter":"16 Privacy","heading":"16 Privacy","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-privacy.html","id":"privacy-and-data-science","chapter":"16 Privacy","heading":"16.1 Privacy and Data Science","text":"Data privacy keeping people’s personal information confidential secure. ’s idea people control personal data, companies organizations responsible protecting . Personal data can include things like names, addresses, phone numbers, email addresses, medical information, even online activity.personal data?Name, national insurance number, passport numberContact details: address, phone number, email addressMedical historyOnline activity, GPS data, finger print face-ID,collected, analysed distributed without consent.concept data privacy, individuals right know information collected , ’s used, ’s shared , ability control information. Companies organizations, hand, responsibility keep information secure use purpose intended.’s important remember growing use machine learning data science methods, personal information collected, stored, shared ever . makes data privacy critical issue work data scientists. ensuring personal information handled responsibly respect people’s privacy, can build trust confidence digital world.\nsection course hope introduce key ideas around data privacy use case studies demonstrate privacy easy thing ensure.","code":""},{"path":"ethics-privacy.html","id":"privacy-as-a-human-right","chapter":"16 Privacy","heading":"16.2 Privacy as a Human Right","text":"","code":""},{"path":"ethics-privacy.html","id":"article-12-of-the-universal-declaration-of-human-rights","chapter":"16 Privacy","heading":"16.2.1 Article 12 of the Universal Declaration of Human Rights","text":"one shall subjected arbitrary interference \nprivacy, family, home correspondence, attacks\nupon honour reputation. Everyone right\nprotection law interference \nattacks. - UN General Assembly, 1948.idea right privacy new one. Article 12 Universal Declaration Human Rights, written 1948, states everyone right privacy personal family life, home correspondence (includes communication via post, telephone, email).means everyone right keep personal information, private protected disclosed others without consent. right privacy essential protecting individual’s autonomy, dignity, freedom.Universal Declaration Human Rights often considered benchmark human rights many countries incorporated principles laws regulations. means many countries right privacy legally protected people right take action privacy violated.means take particular care work data scientists handling information personal information, whether individual level aggregate.","code":""},{"path":"ethics-privacy.html","id":"data-privacy-and-the-european-union","chapter":"16 Privacy","heading":"16.3 Data Privacy and the European Union","text":"","code":""},{"path":"ethics-privacy.html","id":"general-date-protection-regulation-2018","chapter":"16 Privacy","heading":"16.3.1 General Date Protection Regulation (2018)","text":"‘Consent’ data subject means freely given, specific, informed unambiguous indication data subject’s wishes , statement clear affirmative action, signifies agreement processing personal data relating ; - GDPR Article 4A recent set regulations relating use personal data General Data Protection Regulation (GDPR). comprehensive data privacy regulation went effect May 2018 within European Union. purpose GDPR give individuals control personal information ’s used, unify data protection laws across EU.GDPR extensive legal document lays strict rules companies organizations must handle personal data EU citizens, regardless data stored processed.key provisions GDPR include requirement obtain explicit, informed active consent individuals collection processing personal data. precisely banner notifications cookies websites became ubiquitous,GDPR also establishes right individuals request access , deletion , personal data. Furthermore, states event data breach (unauthorised access use personal data occurs) data holder must inform affected individuals relevant authorities within 72 hours.","code":""},{"path":"ethics-privacy.html","id":"privacy-key-terms","chapter":"16 Privacy","heading":"16.4 Privacy: Key Terms","text":"Measuring privacy within dataset complex task involves assessing degree personal information protected unauthorized access, use, disclosure. many ways measuring increasing degree privacy within data set. understand , literature data privacy, helps familiar key terms.Pseudonmisation: processing data relate identifiable person.Re-identification: relating pseudonymised data entry identifiable person.Anonymisation: pseudonmisation method precludes re-identification.data entry pseudonymised processed way relate identifiable person. key word identifiable. Replacing name CID considered form pseudonymisation, based information alone identified.Re-identification act relating pseudonymised data entry identifiable person. Re-identification makes something person known wasn’t known beforehand, perhaps using processing techniques cross-referencing external information. previous grading scenario, re-identifiaction occurs marking grades can returned .Anonymisation word casual usage often conflated previous definition pseudonymisation. technical sense, anonymisation form pseudonymisation precludes possibility re-identification.don’t wan anonymise test scores, able map back individual students. However, grades published online want ensure done anonymised format.","code":""},{"path":"ethics-privacy.html","id":"measuring-privacy","chapter":"16 Privacy","heading":"16.5 Measuring Privacy","text":"","code":""},{"path":"ethics-privacy.html","id":"pseudo-identifiers-and-k-anonymity","chapter":"16 Privacy","heading":"16.5.1 Pseudo-identifiers and \\(k\\)-anonymity","text":"Pseudo-identifiers: Attributes can also observed public data. \nexample, someone’s name, job title, zip code, email.set quasi-identifiers \\(A_1, \\ldots ,A_p\\), table \n\\(k\\)-anonymous possible value assignment \nvariables \\((a_1, . . . , a_n)\\) observed either 0 least \\(k\\) individuals.\\(k\\)-anonymity technique ensures combination attributes within table, shared least k records within dataset. ensures individual’s data can’t distinguished least \\(k-1\\) individuals.provides rudimentary level privacy, \\(k\\) corresponds size smallest equivalence class pseudo-identifiers within data. Therefore larger values \\(k\\) correspond greater levels privacy.see concretely, let’s take look example.","code":""},{"path":"ethics-privacy.html","id":"k-anonymity-example","chapter":"16 Privacy","heading":"16.5.2 \\(k\\)-anonymity example","text":"example dataset recording visits sexual health clinic. wish establish \\(k\\)-anonymity data set, diagnosed condition kept private. help , drug use status removed patients partial information available postcode age.grouping observations distinct combination pseudo-identifiers can establish equivalence classes within data.three distinct equivalence classes, four observations. Therefore smallest equivalence class also size four data set 4-anonymous.can easily identify equivalence classes small dataset, large datasets non-trivial task.","code":""},{"path":"ethics-privacy.html","id":"improving-privacy","chapter":"16 Privacy","heading":"16.6 Improving Privacy","text":"three main ways can improve level privacy within data, seen examples two already.Redaction may applied individual attribute, leading whole row column censored. quite extreme approach: can lead large amount information removed data set. However, sometimes redacting full row necessary; example row contains identifying information like person’s name national insurance number. additional concern redacting rows data artificially alter distribution sample, making unrepresentative population values.Aggregation coarsening second approach level anonymity can increased binning continuous variables discrete ranges combining categories within variable already takes discrete values. idea reduce number equivalence classes within quasi-identifiers level k-anonymity increased.similar approach corrupt obfuscate observed data adding noise observations, permuting portion . aim retain overall patterns ensure individual recorded values longer correspond individual raw data set. difficulty setting type amount noise added data grant sufficient privacy without removing information dataset.trade-information loss privacy common theme throughout methods.","code":""},{"path":"ethics-privacy.html","id":"breaking-k-anonymity","chapter":"16 Privacy","heading":"16.7 Breaking \\(k\\)-anonymity","text":"\\(k\\)-anonymity ensures least \\(k-1\\) people particular combination pseudo-identifiers. ensure variation within particular group. dataset sexual health just saw 4-anonymous, know person attended clinic Lancashire (LA) postcode (20s) know certain Chlamydia. alternative privacy measure called \\(l\\)-diversity tries address issue.second problem \\(k\\)-anonymity type privacy measure focused entirely data available within dataset. take account data might available elsewhere might become publicly available future. external data-linkage attack can cross-reference table information reduce size equivalence classes reveal personal information.","code":""},{"path":"ethics-privacy.html","id":"cautionary-tales","chapter":"16 Privacy","heading":"16.8 Cautionary tales","text":"","code":""},{"path":"ethics-privacy.html","id":"massachussets-medical-data","chapter":"16 Privacy","heading":"16.8.1 Massachussets Medical Data","text":"Medical research often slow difficult share medical records maintaining patients’ privacy. 1990s government agency Massachusetts wanted improve releasing dataset summarising hospital visits made state employees. understandably quite careful , making information available academic researchers redacted information like names, addresses security numbers. include patient’s date birth, zip code, sex - information deemed sufficiently general allowing difference healthcare provision investigated.Latanya Sweeney Speaking New York, 2017. Image CC-4.0 Parker Higgins.Latanya Sweeney now pre-eminent researcher field data privacy. 1990s studying PhD MIT wanted demonstrate potential risks de-anonymising sort data. demonstrate point chose focus public records Massachusetts’ governor, William Weld. small fee, Sweeney able obtain voter registration records area governor lived. cross-referencing two datasets Sweeney able uniquely identify governors medical history send post. particularly embarrassing Weld. since previously given statement reassuring public data release compromise privacy public servants.interaction Sweeney Governor Massachusetts significant highlighted potential privacy risks associated release publicly available information. demonstrated even data stripped names identifying information, can still possible re-identify individuals potentially access sensitive information. problem grows dimension dataset - characteristics measures greater chance one person unique combination .","code":""},{"path":"ethics-privacy.html","id":"neflix-competition","chapter":"16 Privacy","heading":"16.8.2 Neflix Competition","text":"Netflix logo 2001-2014. Public domain image.2006 Netflix announced public competition improve performance recommender system pros 1 million USD first person group improve performance least 10%. competition 100 million ratings 480,000 users made public. entry contained pseudonymised user ID, film id, rating 5 stars date rating given.Although Netflix team pseudonymised data (taken steps like adding noise observations), two researchers University Texas able successfully re-identify 96% individuals within data. cross reference competition dataset reviews openly available internet movie database (IMDb), working supposition users rate films services approximately time - 96% figure uses margin 3 days.researchers went , showing alter data achieve even modest 2-anonymity almost useful information removed competition data set.example show difficult can ensure individual privacy face unknown external data sources. might seem like trivial example compared medical records media consume, particular rate media, cam reveal religious beliefs, political stance sexual orientation. protected characteristics might want broadcast freely.might important , even average Netflix user, information becomes public. important whether user find privacy breach objectionable potentially come harm .","code":""},{"path":"ethics-privacy.html","id":"wrapping-up-8","chapter":"16 Privacy","heading":"16.9 Wrapping Up","text":"Privacy fundamental concern.Privacy hard measure hard ensure.Also model issue, since models trained data.universal answers, exciting area ongoing research.Although scratched surface privacy data science, wrap video .seen privacy fundamental concern working human-related data. chiefly aren’t position determine types privacy breach might significantly negatively impact lives people hold data .seen one example metric privacy can difficult measure even difficult preserve. issue releasing data world, also publishing models trained data. Approaches analogous used Latanya Sweeney’s can used used bad-actors identify high precision whether given individual included within data used train published model.universal answers question privacy data science, makes exciting area ongoing research. also reason lot Sweeney’s work published overcoming great resistance: exposed systematic vulnerabilities currently reliable solutions.","code":""},{"path":"ethics-fairness.html","id":"ethics-fairness","chapter":"17 Fairness","heading":"17 Fairness","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-fairness.html","id":"fairness-and-the-data-revolution","chapter":"17 Fairness","heading":"17.1 Fairness and the Data Revolution","text":"1990s, large datasets typically collected understand huge, complex systems :weatherinfrastructure\nhospitals\nroads\ntrain networks\nhospitalsroadstrain networksstock marketspopulations.Collecting high quality data systems immensely expensive paid dividends allowing us describe expected behaviour systems aggregate level. Using sort information, can’t make journeys healthcare better individual level can make changes try make experiences better average.Things changed widespread adoption internet mid-1990s subsequent surge data collection, sharing processing. Suddenly, individuals shifted just one part huge processes complex process worth modelling .point focus shifted toward making individual, personalised predictions specific people, based vast amounts data generate go daily lives.shift aggregate individual behaviour creates opportunity predictions systematically harm groups people, always , also acutely harm individuals.","code":""},{"path":"ethics-fairness.html","id":"you-are-your-data","chapter":"17 Fairness","heading":"17.2 You are Your Data","text":"blunt truth , far data science model concerned, nothing point high-dimensional predictor space.model might use location space group points sense “nearby”. Alternatively, model might estimate information currently doesn’t , based knows surrounding points. points also represent unique humans rich fascinating lives - model doesn’t care , just group points predict values.idea fairness comes data science begin ask predictors provide model carrying tasks. aren’t asking model selection stand-point. asking morally permissible predictors, leads significant improvement model fit.","code":""},{"path":"ethics-fairness.html","id":"forbidden-predictors","chapter":"17 Fairness","heading":"17.3 Forbidden Predictors","text":"argument features human can used make decisions started well 1990s. contentious arguments centre around inclusion characteristics either immutable easily changed. characteristics including race, gender, age religion receive legal protections. protected attributes often forbidden used important decisions, whether bank loan accepted.natural lead us ask classifies important decision?Protected Characteristics Equality Act (2010)ageagegender reassignmentgender reassignmentmarriage / civil partnershipmarriage / civil partnershippregnancy / parental leavepregnancy / parental leavedisabilitydisabilityrace including colour, nationality, ethnic national originrace including colour, nationality, ethnic national originreligion beliefreligion beliefsexsexsexual orientationsexual orientationWe also need careful protected attributes actually strong predictive power improve predictions (potentially benefit groups protected regulations). Just protected attribute isn’t used directly within model doesn’t mean model discriminate according attribute.multiple predictors within model, withholding protected attribute make model ignorant attribute. access someone’s browsing history, live recent purchases can probably make fairly accurate profile person, including many supposedly protected attributes. way, model can use combine attributes legally protected create new variable acts accurate proxy protected characteristic.wouldn’t model ? using standard loss function literally asked get best possible predictive performance. protected attribute predictive power model likely approximate using predictors available .see handle concern, let’s step back consider can quantify measure fairness model.","code":""},{"path":"ethics-fairness.html","id":"measuring-fairness","chapter":"17 Fairness","heading":"17.4 Measuring Fairness","text":"Converting concept fairness mathematical statement difficult task. partly moving natural language precise formalism hard, ’s also term fairness means different things different people different contexts. , many complementary definitions fairness try capture intuitive notion fair model. However, measures capture different facets complicated concept. Despite , measures vary extent can’t satisfied simultaneously.’ll introduce four measures shortly, focusing case binary outcomes “positive” response 1 corresponds event considered favourably taken context. example might loan successfully repaid person released bail re-offend.Binary outcome \\(Y \\\\{0,1\\}\\).’ll consider simple case binary prediction made instance, want predictions fair across \\(k\\) distinct levels protected attribute \\(\\).Binary Prediction \\(\\hat Y \\\\{0,1\\}\\).Protected attribute \\(\\) takes values \\(\\mathcal{} = \\{a_1, \\ldots, a_k\\}\\).","code":""},{"path":"ethics-fairness.html","id":"demographic-parity","chapter":"17 Fairness","heading":"17.4.1 Demographic Parity","text":"first, potentially obvious fairness definition demographic parity. model deemed fair , across subgroups protected attribute, probability predicting successful outcome equal.probability predicting ‘positive’ outcome groups.\\[\\mathbb{P}(\\hat Y = 1 | = a_i) = \\mathbb{P}( \\hat Y = 1 | = a_j), \\  \\text{ }\\  ,j \\\\mathcal{}.\\]obvious shortcoming demographic parity allow us account fact positive outcome might equally likely subgroups. way demographic parity analogous treating people equally, rather equitably.","code":""},{"path":"ethics-fairness.html","id":"equal-opportunity","chapter":"17 Fairness","heading":"17.4.2 Equal Opportunity","text":"Equality opportunity addresses shortcoming conditioning truly positive outcome. Equality opportunity states “worthy” loan (sense), subgroups protected characteristic treated equally.Among true ‘positive’ outcome, probability predicting ‘positive’ outcome groups.\\[\\mathbb{P}(\\hat Y = 1 | = a_i, Y =1) = \\mathbb{P}( \\hat Y = 1 | = a_j, Y=1), \\  \\text{ }\\  ,j \\\\mathcal{}.\\]","code":""},{"path":"ethics-fairness.html","id":"equal-odds","chapter":"17 Fairness","heading":"17.4.3 Equal Odds","text":"course, encountered two-way tables, type-type-II errors. Equally important granting loans people repay deny loans afford .model satisfying equal odds condition can identify true positives false negatives equally well across sub-groups protected characteristic.Among true ‘positive’ outcome, probability predicting ‘positive’ outcome groups.ANDAmong true ‘negative’ outcome, probability predicting ‘negative’ outcome groups.\\[\\mathbb{P}(\\hat Y = y | = a_i, Y =y) = \\mathbb{P}( \\hat Y = y | = a_j, Y=y), \\ \\text{ } \\ y \\\\{0,1\\} \\ \\text{ } \\  ,j \\\\mathcal{}.\\]","code":""},{"path":"ethics-fairness.html","id":"predictive-parity","chapter":"17 Fairness","heading":"17.4.4 Predictive Parity","text":"measures considered far consider probability prediction given true credit-worthiness applicant. Predictive Parity reverses order conditioning (compared equal opportunity).ensures among predicted successful outcome, probability truly successful outcome subgroups protected characteristic. ensures , financial example, bank spreading risk exposure equally across subgroups; subgroup approximately equal proportion approved loans successfully repaid.probability true ‘positive’ outcome people predicted ‘positive’ outcome equal across groups.\\[\\mathbb{P}(Y = 1 | \\hat Y = 1, = a_i) = \\mathbb{P}(Y_1 = 1 | \\hat Y = 1, = a_j) \\ \\text{ } \\  ,j \\\\mathcal{}.\\]can play devil’s advocate say might appropriate genuine difference probability successful repayment groups.","code":""},{"path":"ethics-fairness.html","id":"metric-madness","chapter":"17 Fairness","heading":"17.5 Metric Madness","text":"Even simple binary classification problem many ways can interpret term fairness. , , appropriate going highly context dependent.issue many metrics, including introduced, require knowledge true outcome. means metrics can evaluated retrospectively: knew information begin wouldn’t need model decide get loan. top , means ever get information loans granted - don’t access counter factual outcome whether loan granted repaid.additional problem evaluating fairness metrics requires us know protected sub-group individual belongs . clearly problem: evaluate fairness loan applications need know sensitive information applicants, - reasonably - unwilling provide information legally used inform decision making process. reason, independent third-party often required assess fairness collating data applicants bank.third complication definitions deal strict equalities. given sample, almost surely going satisfied even metric truly satisfied. formal statistical test used assess whether differences consistent truly fair model, however common approach regulators set acceptable tolerance discrepancy metric values sub-groups.Finally, worth noting problems arise simple binary classifier models far complicated . Even working conditional probability statements requires careful attention, things get much trickier response sensitive attribute continuous valued , non-sensitive predictors also included model.","code":""},{"path":"ethics-fairness.html","id":"modelling-fairly","chapter":"17 Fairness","heading":"17.6 Modelling Fairly","text":"","code":""},{"path":"ethics-fairness.html","id":"fairness-aware-loss-functions","chapter":"17 Fairness","heading":"17.6.1 Fairness Aware Loss Functions","text":"Now methods detect quantify fairness models, incorporate model fitting process?now multiple objectives: predict outcome accurately possible also treating people fairly possible (ever fairness metric combination metrics care consider). Unfortunately, things generally competition . one best model rather family best models, pick single model implement.Can resolve issue linearisation, create loss function linear weighted sum two component objectives. simplifies problem mathematically, actually just shift problem rather resolving . scaling constant, combination weights corresponds unique point Pareto frontier, just translated problem picking point frontier picking pair weights.actually resolve issue need define relative preference fairness predictive power. say “preference”, actually mean company organisation analysis - personal view. Eliciting preference communicating idea optimal frontier can tricky. One solution present small set possible models, represent range preferences two competing objectives, ask stakeholder choose .","code":""},{"path":"ethics-fairness.html","id":"other-approaches","chapter":"17 Fairness","heading":"17.6.2 Other Approaches","text":"Re-weighting resampling better represent minority groups.Forgetting factor reduce impact bias historical data.Meta-modelling intervene harmful feedback loops.Whenever treating predictions equally loss function optimises purely predictive performance, good predictions minority groups never prioritised. One strategy correct either re-weighting re-sample observation minority groups given greater importance within loss function.lot problems fairness see models replicating happens used happen reality. cases, better addressed now model can made fair -weighting training data ages. allows model quickly adapt changes system trying represent.cases use historically biased data train models put production lead feedback loop makes recent data even unjust.\nOne example , can consider racial disparity interest rates offered mortgages. Suppose one racial group applicants past slightly likely default loans, perhaps due historical pay inequity. means models likely suggest higher interest loans group, attempt offset bank’s exposure risk non-repayment.reduces number loans granted racial group also makes loans granted difficult repay likely defaulted . turn leads another increase offered interest rate, driving number loans approved pushing chance non-repayment even .decisions made using model impacting future training data creating harmful self-reinforcing feedback loop. Historical weighting nothing address sort issue, requires active intervention.meta-modelling approach possible type intervention. post-hoc methods used estimate biases within fitted model estimates used explicitly correct historical biases, model used make predictions decisions.","code":""},{"path":"ethics-fairness.html","id":"wrapping-up-9","chapter":"17 Fairness","heading":"17.7 Wrapping Up","text":"’s good point us wrap introduction fairness data science.seen optimising predictive accuracy alone can lead unjust models. also raised concerns protected characteristics included models, whether directly predictor via collection predictors well approximate .seen multitude measures assess fairness models. can combine standard metrics goodness--fit create custom loss functions represent preference fairness predictive performance.privacy, universal answers comes measuring implementing fair data science methodology. still relatively new rapidly evolving field data science.","code":""},{"path":"ethics-conduct.html","id":"ethics-conduct","chapter":"18 Codes of Conduct","heading":"18 Codes of Conduct","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-conduct.html","id":"data-science-miracle-cure-and-sexiest-job","chapter":"18 Codes of Conduct","heading":"18.1 Data Science: Miracle Cure and Sexiest Job","text":"10 years since proclaimed data science sexiest job century. Current turbulence technology sector may questioning veracity claim, still rings true take less myopic view data science used.Title Harvard Business Review article: data scientist still sexiest job 21st Century?Data science machine learning used extensively ever across fields including medicine, healthcare sociology. Data science also applied understand environment, study ecological systems inform strategies preservation. addition , data science used widely across private sector, informs business strategies financial services, alongside public sector influences governance policy development.","code":""},{"path":"ethics-conduct.html","id":"what-could-go-wrong","chapter":"18 Codes of Conduct","heading":"18.2 What could go wrong?","text":"data science methods can wonderful, infallible. number use cases increases, also expect number misuses high-profile failures increase .two news articles highlight just badly can go.BBC news article: facial recognition fails race, government study says.first article shows combination non-representative training data lack consideration part developers led facial recognition system much less accurate people darker skin. absolutely unacceptable, particularly given widespread use similar systems important security applications biometric ID online banking automatic passport gates.second example shows risk catastrophic failures self-driving cars. know misclassification missed edge cases inevitable case self-driving cars errors can cause serious even fatal accidents. might ask errors acceptable lead fewer accidents fatalities caused human drivers.BBC news article: Uber’s self-driving operator charged fatal crash.Another important point establish liability falls cases: operator self-driving vehicle, vendor solid data scientist wrote script controlling car’s actions? behaviour determined training data test-drives, driver share liability.important questions haven’t necessarily thought ask deploying data science solutions world.","code":""},{"path":"ethics-conduct.html","id":"thats-not-my-type-of-data-science","chapter":"18 Codes of Conduct","heading":"18.3 That’s not my type of data science …","text":"might thinking doesn’t effect don’t work image analysis self-driving cars, related issues come standard applications data science.Consider retailer implementing targeted promotions. might combine previous shopping habits recent product searches target offers.\nFigure 10.1: Predicting pregnancy search purchase history.\nAround 7 months ago, one customer stopped regularly buying contraceptives alcohol started buying supplements vitamin D folic acid. Based similar behaviour lots customers, recommender model might now expect good way increase sales send customer offers nappies, baby food talcum powder.seriously upsetting customer also happened experiencing fertility issues pregnant abortion miscarriage. important human-overrides included systems like customer contact retailer prevent sort advertising continuing happen months years come.","code":""},{"path":"ethics-conduct.html","id":"technological-adoption-relies-on-public-trust","chapter":"18 Codes of Conduct","heading":"18.4 Technological Adoption Relies on Public Trust","text":"\nFigure 10.2: Thirteen people killed 145 injured Interstate 35W bridge collapse 2007.\nData science new discipline, means issues mentioned new happening first time. However, many issues unique data science means can learn disciplines already invested lot time energy problems.Across many professions ’s standard follow professional code practice code ethics applies everyone working space. Usually, professions place also legally liable way outcomes consequences work.obvious example might doctors medical professionals, holds equally true many jobs law engineering, practitioner’s work can impact freedom safety people. thinking critically practical ethical implications work data scientists, shouldn’t start scratch learn much possible fields.One profession particular can learn medicine. Around world, doctors agree uphold privacy patients maintaining medical confidentiality. also agree act believe best interests patient harm. admirable principles can tracked directly onto work data science.Google’s original internal code encapsulated extreme version non-maleficence principle, starting phrase ‘Don’t evil’ (thought later rephrased ‘can make money without evil’).","code":""},{"path":"ethics-conduct.html","id":"a-hippocratic-oath-for-data-scientists","chapter":"18 Codes of Conduct","heading":"18.5 A Hippocratic Oath for Data Scientists","text":"\nFigure 11.1: Weapons Math Destruction (2016), Cathy O’Neil among first authors make call Hippocratic oath data scientists\n’ve seen examples broad acute harms can befall individuals data science. mind, seems reasonable expect data scientists aware negative impacts work required mitigate wherever possible.argument made Cathy O’Neil book Weapons Math Destruction: data scientists, working mathematical models generally, need equivalent Hippocratic oath ’s used medicine. understand necessary, understand case already.right thing, data science life,neither obvious easy.naive sense, negative consequences can come result lack understanding unanticipated consequences. lack understanding might relate model (might explainable) setting model applied (might require domain specific expertise data scientist lacks).Alternatively, ignorance can groups people risk harm data science models. Data science methods tend model expected behaviour echo biases within training data. means minority groups groups historically disadvantaged risk harm. minority status historical disadvantage means groups low representation within data science teams. increases chance data science teams ignorant ignore breadth depth damage might causing.second way data science can lead harm incentives properly aligned. example consider credit scoring application data scientist trying include fairness measures addition optimising predictive accuracy loan repayments. business incentives purely based around immediate increases profit, likely difficult get put production. misalignment priorities business ethics data scientist.seen,errors inevitable sometimes best can balance inherent trade-conflicting priorities different sources harm. cases vitally important highlight trade-explicitly offer decision makers range solutions may choose according priorities.right thing neither obvious easy:Lack understanding,Lack understanding,Unanticipated consequencesUnanticipated consequencesIncentive structures,Incentive structures,Inherent trade-offs.Inherent trade-offs.","code":""},{"path":"ethics-conduct.html","id":"codes-of-conduct","chapter":"18 Codes of Conduct","heading":"18.6 Codes of Conduct","text":"Membership professional body can offer ethical benefits Hippocratic oath, memberships often require agreement abide code conduct.provides individual data scientists accountability actions broader sense direct accountability employer. can helpful motivator high quality line ethical guidelines.can also useful form external support, convince organisations employ data scientists ethical considerations important aspect work. also opportunity peer--peer support, best practices can established improved unified way across industries, rather isolation within individual organisations. might informal networking knowledge sharing formal training organised professional body.Data science still developing field currently professional body specifically dedicated data science. Instead, data scientists might join one related professional bodies, Royal Statistical Society, Operational Research Society,) Society Industrial Applied Mathematics.","code":""},{"path":"ethics-conduct.html","id":"wrapping-up-10","chapter":"18 Codes of Conduct","heading":"18.7 Wrapping Up","text":"Data Science still maturing, field study career path. combination widespread adoption data science recent years, means discipline ’re now navigating many new modes failure ethical issues. However, issues entirely novel. awful lot can learn, borrow adapt established fields medicine, engineering physics.important remain alert dangers work: liability opens us harm might causing others, potentially without aware either things. joining professional bodies upholding codes conduct can push back bad practices reward good ones.","code":""},{"path":"ethics-checklist.html","id":"ethics-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-checklist.html","id":"videos-chapters-4","chapter":"Checklist","heading":"18.8 Videos / Chapters","text":"Privacy (19 min) [slides]Privacy (19 min) [slides]Fairness (20 min) [slides]Fairness (20 min) [slides]Codes Conduct (14 min) [slides]Codes Conduct (14 min) [slides]","code":""},{"path":"ethics-checklist.html","id":"reading-4","chapter":"Checklist","heading":"18.9 Reading","text":"Use Data Science Ethics section reading list support guide exploration week’s topics. Note texts divided core reading, reference materials materials interest.","code":""},{"path":"ethics-checklist.html","id":"activities-2","chapter":"Checklist","heading":"18.10 Activities","text":"week fewer activities, may look second assessment end course.Core:Read Gender Shades: Intersectional Accuracy Disparities Commercial Gender Classification Joy Buolamwini Timnit Gebru (2018). Proceedings 1st Conference Fairness, Accountability Transparency.Read Gender Shades: Intersectional Accuracy Disparities Commercial Gender Classification Joy Buolamwini Timnit Gebru (2018). Proceedings 1st Conference Fairness, Accountability Transparency.Find example case study method relating ethical data science covered lectures. Share find writing short summary case study method discussion forum.Find example case study method relating ethical data science covered lectures. Share find writing short summary case study method discussion forum.Skim Professional Guidelines listed reference materials week, preparation live session.Skim Professional Guidelines listed reference materials week, preparation live session.BonusAnswer multiple choice questions Buolamwini Timnit Gebru (2018).","code":""},{"path":"ethics-checklist.html","id":"live-session-4","chapter":"Checklist","heading":"18.11 Live Session","text":"live session week begin minutes Q&assessments. break groups discuss compare professional guidelines ethical data science.Finally, time allows round session activity randomised response survey designs.","code":""},{"path":"reading-list.html","id":"reading-list","chapter":"19 Reading List","heading":"19 Reading List","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.reading list organised topic, according week course. split several categories.Core Materials: form core part course activities.Core Materials: form core part course activities.Reference Materials: used extensively course, seen helpful guides, rather required reading cover cover.Reference Materials: used extensively course, seen helpful guides, rather required reading cover cover.Materials Interest: form core part course, give deeper understanding interesting perspective weekly topic. might fun stuff .Materials Interest: form core part course, give deeper understanding interesting perspective weekly topic. might fun stuff .","code":""},{"path":"reading-list.html","id":"workflows-reading","chapter":"19 Reading List","heading":"19.1 Effective Data Science Workflows","text":"","code":""},{"path":"reading-list.html","id":"core-materials","chapter":"19 Reading List","heading":"Core Materials","text":"Tidyverse R Style Guide Hadley Wickham.Wilson, et al (2017). Good Enough Practices Scientific Computing. PLOS Computational Biology.","code":""},{"path":"reading-list.html","id":"reference-materials","chapter":"19 Reading List","heading":"Reference Materials","text":"R Data Science Chapters 2, 6 8 Hadley Wickham Garrett Grolemund. Chapters covering R workflow basics, scripting project based workflow.R Data Science Chapters 2, 6 8 Hadley Wickham Garrett Grolemund. Chapters covering R workflow basics, scripting project based workflow.Documentation {} packageDocumentation {} packageR Packages Book (Second Edition) Hadley Wickham Jenny Bryan.R Packages Book (Second Edition) Hadley Wickham Jenny Bryan.","code":""},{"path":"reading-list.html","id":"materials-of-interest","chapter":"19 Reading List","heading":"Materials of Interest","text":"STAT545, Part 1 Jennifer Bryan STAT 545 TAsWhat forgot teach R, Chapters 2-4 Jennifer Bryan Jim Hester.Broman et al (2017). Recommendations Funding Agencies Supporting Reproducible Research. American Statistical Association.Advanced R Hadley Wickham Section introductions functional object oriented approaches programming.Advanced R Hadley Wickham Section introductions functional object oriented approaches programming.Atlassian Article Agile Project ManagementAtlassian Article Agile Project ManagementThe Pragmatic Programmer, 20th Anniversary Edition Edition David Thomas Andrew Hunt. section DRY coding others freely available.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week. Chapter 4 Efficient Workflows links nicely last week’s topics.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week. Chapter 4 Efficient Workflows links nicely last week’s topics.Towards Principled Bayesian Workflow Michael Betancourt.Towards Principled Bayesian Workflow Michael Betancourt.Happy Git GitHub useR Jennifer Bryan","code":""},{"path":"reading-list.html","id":"data-reading","chapter":"19 Reading List","heading":"19.2 Aquiring and Sharing Data","text":"","code":""},{"path":"reading-list.html","id":"core-materials-1","chapter":"19 Reading List","heading":"Core Materials","text":"R Data Science Chapters 9 - 12 Hadley Wickham. chapters introduce tibbles data structure, import data R wrangle data tidy format.R Data Science Chapters 9 - 12 Hadley Wickham. chapters introduce tibbles data structure, import data R wrangle data tidy format.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week.Wickham (2014). Tidy Data. Journal Statistical Software. paper brought tidy data mainstream.Wickham (2014). Tidy Data. Journal Statistical Software. paper brought tidy data mainstream.","code":""},{"path":"reading-list.html","id":"reference-materials-1","chapter":"19 Reading List","heading":"Reference Materials","text":"{readr} documentationThe {readr} documentationThe {data.table} documentation vignetteThe {data.table} documentation vignetteThe {rvest} documentationThe {rvest} documentationThe {tidyr} documentationThe {tidyr} documentationMDN Web Docs HTML CSSMDN Web Docs HTML CSS","code":""},{"path":"reading-list.html","id":"materials-of-interest-1","chapter":"19 Reading List","heading":"Materials of Interest","text":"Introduction APIs Brian CookseyR Data Science (Second Edition) Chapters within Import section.covers importing data spreadsheets, databases, using Apache Arrow importing hierarchical data well web scraping.","code":""},{"path":"reading-list.html","id":"edav-reading","chapter":"19 Reading List","heading":"19.3 Data Exploration and Visualisation","text":"","code":""},{"path":"reading-list.html","id":"core-materials-2","chapter":"19 Reading List","heading":"Core Materials","text":"Exploratory Data Analysis R Roger Peng.Chapters 3 4 core reading, respectively introducing data frame manipulation {dplyr} example workflow exploratory data analysis. chapters may useful references.Flexible Imputation Missing Data Stef van Buuren. Sections 1.1-1.4 give thorough introduction missing data problems.","code":""},{"path":"reading-list.html","id":"referene-materials","chapter":"19 Reading List","heading":"Referene Materials","text":"ggplot2 Tutorial Beautiful Plotting R https://www.cedricscherer.com/2019/08/05/-ggplot2-tutorial--beautiful-plotting--r/) Cédric Scherer.ggplot2 Tutorial Beautiful Plotting R https://www.cedricscherer.com/2019/08/05/-ggplot2-tutorial--beautiful-plotting--r/) Cédric Scherer.{dplyr} documentationThe {dplyr} documentationRStudio Data Transformation Cheat SheetRStudio Data Transformation Cheat SheetR Data Science (First Edition) Chapters Data Transformations, Exploratory Data Analysis Relational Data.R Data Science (First Edition) Chapters Data Transformations, Exploratory Data Analysis Relational Data.Equivalent sections R Data Science Second EditionEquivalent sections R Data Science Second Edition","code":""},{"path":"reading-list.html","id":"materials-of-interest-2","chapter":"19 Reading List","heading":"Materials of Interest","text":"Wickham, H. (2010). Layered Grammar Graphics. Journal Computational Graphical Statistics.Wickham, H. (2010). Layered Grammar Graphics. Journal Computational Graphical Statistics.Better Data Visualisations Jonathan SchwabishBetter Data Visualisations Jonathan SchwabishData Visualization: Practical Introduction Kieran Healy","code":""},{"path":"reading-list.html","id":"production-reading","chapter":"19 Reading List","heading":"19.4 Preparing for Production","text":"","code":""},{"path":"reading-list.html","id":"core-materials-3","chapter":"19 Reading List","heading":"Core Materials","text":"Ethical Algorithm M Kearns Roth (Chapter 4)Ethical Algorithm M Kearns Roth (Chapter 4)Ribeiro et al (2016). “Trust ?”: Explaining Predictions Classifier.Ribeiro et al (2016). “Trust ?”: Explaining Predictions Classifier.","code":""},{"path":"reading-list.html","id":"reference-materials-2","chapter":"19 Reading List","heading":"Reference Materials","text":"Docker Curriculum Prakhar Srivastav.Docker Curriculum Prakhar Srivastav.LIME package documentation CRAN.LIME package documentation CRAN.Interpretable Machine Learning: Guide Making Black Box Models Explainable Christoph Molnar.Interpretable Machine Learning: Guide Making Black Box Models Explainable Christoph Molnar.Documentation apply(), map() pmap()Documentation apply(), map() pmap()Advanced R (Second Edition) Hadley Wickham. Chapter 23 measuring performance Chapter 24 improving performance.Advanced R (Second Edition) Hadley Wickham. Chapter 23 measuring performance Chapter 24 improving performance.","code":""},{"path":"reading-list.html","id":"materials-of-interest-3","chapter":"19 Reading List","heading":"Materials of Interest","text":"ASA Statement \\(p\\)-values: Context, Process PurposeThe ASA Statement \\(p\\)-values: Context, Process PurposeThe Garden Forking Paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013)Garden Forking Paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013)Understanding LIME tutorial T Pedersen M Benesty.Understanding LIME tutorial T Pedersen M Benesty.Advanced R (Second Edition) Hadley Wickham. Chapter 25 writing R code C++.Advanced R (Second Edition) Hadley Wickham. Chapter 25 writing R code C++.","code":""},{"path":"reading-list.html","id":"data-science-ethics","chapter":"19 Reading List","heading":"19.5 Data Science Ethics","text":"","code":""},{"path":"reading-list.html","id":"core-materials-4","chapter":"19 Reading List","heading":"Core Materials","text":"Ethical Algorithm M Kearns Roth. Chapters 1 2 Algorithmic Privacy Algortihmic Fairness.Ethical Algorithm M Kearns Roth. Chapters 1 2 Algorithmic Privacy Algortihmic Fairness.Gender Shades: Intersectional Accuracy Disparities Commercial Gender Classification Joy Buolamwini Timnit Gebru (2018). Proceedings 1st Conference Fairness, Accountability Transparency.Gender Shades: Intersectional Accuracy Disparities Commercial Gender Classification Joy Buolamwini Timnit Gebru (2018). Proceedings 1st Conference Fairness, Accountability Transparency.Robust De-anonymization Large Sparse Datasets Arvind Narayanan Vitaly Shmatikov (2008). IEEE Symposium Security Privacy.Robust De-anonymization Large Sparse Datasets Arvind Narayanan Vitaly Shmatikov (2008). IEEE Symposium Security Privacy.","code":""},{"path":"reading-list.html","id":"reference-materials-3","chapter":"19 Reading List","heading":"Reference Materials","text":"Fairness machine learning\nLimitations Opportunities Solon Barocas, Moritz Hardt Arvind Narayanan.Fairness machine learning\nLimitations Opportunities Solon Barocas, Moritz Hardt Arvind Narayanan.Professional Guidleines Data Ethics :\nAmerican Mathematical Society\nEuropean Union\nUK Government\nRoyal Statistical Society\nDutch Government\nProfessional Guidleines Data Ethics :American Mathematical SocietyThe European UnionUK GovernmentRoyal Statistical SocietyDutch Government","code":""},{"path":"reading-list.html","id":"materials-of-interest-4","chapter":"19 Reading List","heading":"Materials of Interest","text":"Algorithmic Fairness (2020). Pre-print review paper Dana Pessach Erez Shmueli.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
