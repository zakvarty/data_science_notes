[{"path":"index.html","id":"about-this-course","chapter":"About this Course","heading":"About this Course","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"index.html","id":"course-description","chapter":"About this Course","heading":"Course Description","text":"Model building evaluation necessary sufficient skills effective practice data science. module develop technical personal skills required work successfully data scientist within organisation.module critically explore :effectively scope manage data science project;work openly reproducibly;efficiently acquire, manipulate, present data;interpret explain work variety stakeholders;ensure work can put production;assess ethical implications work data scientist.interdisciplinary course draw fields including statistics, computing, management science data ethics. topic investigated selection lecture videos, conference presentations academic papers, hands-lab exercises, readings industry best-practices recognised professional bodies.","code":""},{"path":"index.html","id":"schedule","chapter":"About this Course","heading":"Schedule","text":"notes intended students course MATH70076: Data Science academic year 2022/23.course scheduled take place five weeks, suggested schedule :1st week: effective data science workflows;2nd week: aquiring sharing data;3rd week: exploratory data analysis visualisation;4th week: preparing production;5th week: ethics context data science.pdf version notes may downloaded .","code":""},{"path":"index.html","id":"learning-outcomes","chapter":"About this Course","heading":"Learning outcomes","text":"successful completion module students able :Independently scope manage data science project;Source data internet web scraping APIs;Clean, explore visualise data, justifying documenting decisions made;Evaluate need (implement) approaches explainable, reproducible scalable;Appraise ethical implications data science projects, particularly risks compromising privacy fairness potential cause harm.","code":""},{"path":"index.html","id":"allocation-of-study-hours","chapter":"About this Course","heading":"Allocation of Study Hours","text":"Lectures: 10 Hours (2 hours per week)Group Teaching: 5 Hours (1 hour per week)Lab / Practical: 5 hours (1 hour per week)Independent Study: 105 hours (17 hours per week + 20 hours coursework)","code":""},{"path":"index.html","id":"assessment-structure","chapter":"About this Course","heading":"Assessment Structure","text":"course assessed entirely coursework, reflecting practical pragmatic nature course material.Coursework 1 (30%): completed fourth week course.Coursework 2 (70%): released last week course submitted following examination period Summer term.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"About this Course","heading":"Acknowledgements","text":"notes created Dr Zak Varty. inspired previous lecture series Dr Purvasha Chakravarti Imperial College London draw many resource made available R community.","code":""},{"path":"workflows-introduction.html","id":"workflows-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.data scientist never work alone.Within single project data scientist likely interact range people, including limited : one project managers, stakeholders subject matter experts. experts might come single specialism form multidisciplinary team, depending type work .get project put use working scale likely collaborate data engineers. also work closely data scientists, review one another’s work collaborate larger projects.Familiarity skills, processes practices make collaboration instrumental successful data scientist. aim part course provide structure organise perform work, can good collaborator current colleges future self.going require bit effort upfront, benefits compound time. get done wasting less time staring quizzically messy folders indecipherable code. also gain reputation someone good work . promotes better professional relationships greater levels trust, can turn lead working exciting impactful projects.","code":""},{"path":"workflows-organising-your-work.html","id":"workflows-organising-your-work","chapter":"2 Organising your work","heading":"2 Organising your work","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.Welcome course effective data science. week ’ll considering effective data science workflows. workflows ways progressing project help produce high quality work help make good collaborator.Chapter, ’ll kick things looking can structure data\nscience projects organize work. Familiarity skills, processes\npractices collaborative working going instrumental \nbecome successful data scientist.","code":""},{"path":"workflows-organising-your-work.html","id":"what-are-we-trying-to-do","chapter":"2 Organising your work","heading":"2.1 What are we trying to do?","text":"First, let’s consider want provide data science projects sense structure organization.data scientist ’ll never work alone. Within single project ’ll interact whole range people. might project manager, one business stakeholders variety subject matter experts.experts might trained sociologists, chemists, civil servants depending exact type data science work ’re .get project put use working scale ’ll \ncollaborate data engineers. ’ll also likely work closely data scientists. smaller projects might act reviewers one another’s work. larger projects working collaboratively allow tackle larger challenges. sorts project wouldn’t feasible alone, inherent limitations time skill one individual person.Even work small organization, ’re data scientist, adopting way working ’s focused collaborating pay dividends time. inevitably return project ’re working several weeks months years future ’ll forgotten almost everything first time around. ’ll also forgotten made decisions potential options didn’t take.exactly like working current colleague shoddy poor working practices. Nobody wants colleague somebody else, let alone future self. Even working alone, treating future self current collaborator (one want get along well ) makes kind colleague pleasure work .aim week provide guiding structure organize perform work. None going particularly difficult onerous. However require bit effort front daily discipline. Like flossing, daily effort required large benefits compound time.’ll get done wasting less time staring quizzically mess \nfolders indecipherable code. ’ll also get reputation someone ’s well organized good work . promotes better professional relationships greater levels trust within team. can , turn, tead working exciting impactful projects future.","code":""},{"path":"workflows-organising-your-work.html","id":"an-r-focused-approach","chapter":"2 Organising your work","heading":"2.2 An R Focused Approach","text":"structures workflows recommend throughout rest module focused strongly workflow predominantly uses R, markdown LaTeX.Similar techniques, code software can achieve results show coding Python C, writing projects Quarto markup language. Similarly, different organizations variations best practices ’ll go together. Often organisations extensive guidance topics.important thing understand good habits build one programming language business, transferring skills new setting largely matter learning new vocabulary slightly different syntax.said, let’s get going!","code":""},{"path":"workflows-organising-your-work.html","id":"one-project-one-directory","chapter":"2 Organising your work","heading":"2.3 One Project = One Directory","text":"’s one thing take away chapter, ’s one\nGolden Rule:Every individual project work data scientist single, self-contained directory folder.worth repeating. Every single project work self-contained live single directory. analogy might separate ring-binder folder modules degree program.one golden rule deceptively simple.first issue requires predetermined scope \nisn’t going covered particular project. seems straightforward outset project often know exactly project go, link pieces work within organization.second issue second law Thermodynamics, applies equally well project management heatdeath universe. takes continual external effort prevent contents one folder becoming chaotic disordered time.said, single directory several benefits justify additional work.","code":""},{"path":"workflows-organising-your-work.html","id":"properties-of-a-well-orgainsed-project","chapter":"2 Organising your work","heading":"2.4 Properties of a Well-Orgainsed Project","text":"properties like single, well-organized project ? Ideally, ’d like organize projects following\nproperties:PortableVersion Control FriendlyReproducibleIDE friendly.Don’t worry haven’t heard terms already. ’re going look little bit detail.","code":""},{"path":"workflows-organising-your-work.html","id":"portability","chapter":"2 Organising your work","heading":"2.4.1 Portability","text":"project said portable can easily moved without breaking.might small move, like relocating directory different location computer. might also mean moderate move, say another machine dies just big deadline. Alternatively, might large shift - uses another person using different operating system.thought experiment can see ’s full spectrum portable project may may need .","code":""},{"path":"workflows-organising-your-work.html","id":"version-control-friendly","chapter":"2 Organising your work","heading":"2.4.2 Version Control Friendly","text":"project Version Control changes tracked either manually automatically. means snapshots project taken regularly gradually develops evolves time. snapshots many, incremental changes made project allow rolled back specific previous state something goes wrong.version controlled pattern working helps avoid horrendous state found - renaming final_version.doc final_final_version.doc .organising workflow around incremental changes helps acknowledge work ever finally complete. always small changes need done future.","code":""},{"path":"workflows-organising-your-work.html","id":"reproducibility","chapter":"2 Organising your work","heading":"2.4.3 Reproducibility","text":"paper, Broman et al define reproducibility project can take original data code used perform analysis using create numerical findings study.definition leads naturally several follow-questions.exactly definition? specifically mean future someone else access data code able recreate findings ? Also, reproducibility limited just numerical results? also able create associated figures, reports press releases?Another important question project needs reproduced. weeks time 10 years time? need protect project changes dependencies, like new versions packages modules? different versions R Python? Taking time scale even , different operating systems hardware?’s unlikely ’d consider someone handing floppy disk code runs Windows XP acceptably reproducible. Sure, probably find way get work, awful lot effort end.’s perhaps bit extreme example, emphasizes importance clearly defining level reproducibility ’re aiming within every project work . example also highlights amount work can required reproduce analysis, especially quite time. important explicitly think dividing effort original developer person trying reproduce analysis future.","code":""},{"path":"workflows-organising-your-work.html","id":"ide-friendly","chapter":"2 Organising your work","heading":"2.4.4 IDE Friendly","text":"final desirable property ’d like projects play nicely\nintegrated development environments.’re coding document writing data science projects ’d possible work entirely either plain text editor typing code directly command line. approaches data science workflow benefit simplicity, also expect great deal data scientist.workflows expect type everything perfectly accurately every time, recall names argument orders every function use, constantly aware current state objects within working environment.Integrated Development Environments (IDEs) applications help reduce burden, helping make effective programmer data scientist. IDEs offer tools like code completion highlighting make code easier read write. offer tools debugging, fix things going wrong, also offer environment panes don’t hold everything head . Many IDEs also often templating facilities. let save reuse snippets code can avoid typing repetitive, boilerplate code introducing errors process.Even haven’t heard IDEs , ’ve likely already used one. common examples might RStudio R-users, PyCharm python users, Visual Studio language agnostic coding environment.Whichever use, ’d like project play nicely . lets us reap benefits keeping project portable, version controlled, reproducible someone working different set-.","code":""},{"path":"workflows-organising-your-work.html","id":"project-structure","chapter":"2 Organising your work","heading":"2.5 Project Structure","text":"’ve given pretty exhaustive argument single directory project good idea. Let’s now take look inside directory define common starting layout content projects.sort project directory template mean ’ll always know find ’re looking members team . , start ’ll reiterate ’re taking opinionated approach providing sensible starting point organizing many projects.Every project going slightly different might require slight alterations suggest . Indeed, even start suggest might adapt project structure develops grows. think ’s helpful consider tailor making changes. ’m providing one size fits design, ’s great lots projects perfect none . ’s job alter refine design individual case.One final caveat get started: companies businesses many times house style write organize code projects. ’s case, follow style guide business company uses. important thing consistent individual level across entire data science team. ’s consistency reaps benefits.Okay, imagine now ’ve assigned shiny new project created single directory house project. ’ve, quite imaginatively, called directory exciting-new-project. populate folder ?rest video, ’ll define house-style organizing root\ndirectory data science projects module.Within project directory subdirectories, can tell folders file structure forward slash following names. also files directly root directory. One called readme.md another called either makefile make.r. ’re going explore files directories turn.","code":""},{"path":"workflows-organising-your-work.html","id":"readme.md","chapter":"2 Organising your work","heading":"2.5.1 README.md","text":"Let’s begin readme file. gives brief introduction project gives information project aims . readme file describe get started using project contribute development.readme written either plain text format readme.txt markdown\nformat readme.md. benefit using markdown allows light\nformatting sections headers lists using plain text characters.\ncan see using hashes mark first second level headers using bullet points unnumbered list. Whichever format use, readme file project always stored root directory typically named uppercase letters.readme file first thing someone ’s new project reads. placing readme root directory capitalising file name increase visibility file increae chances actually happening.additional benefit keeping readme root directory project code hosting services like GitHub, GitLab BitBucket display contents readme file next contents project. services also nicely format markdown use readme file.writing readme, can useful imaginge writing new, junior team member. readme file let get started project make simple contributions reading file. might also link detailed project documentation wll help new team member toward advanced understanding complex contribution.","code":""},{"path":"workflows-organising-your-work.html","id":"inside-the-readme","chapter":"2 Organising your work","heading":"2.5.2 Inside the README","text":"let’s take quick aside see detial covered within readme file.readme include name project, self-explanatory (nothing like generic chouce exciting-new-project). readme also give project status, just couple sentences say whether project still development, version oft current release , end project life-cylce, project deprecated \nclosed.Following , also include description project. state purpose work provide, link , additional context references visitors aren’t assumed familiar .project involves code depends packages give instruction install dependencies run code. might just text also include thing sline screenshots, code snippets, gifs video whole process.Tt’s also good practice include simple examples use code within project expected results, new users can confirm everything’s working local instance. Keep examples simple minimal can new usersFor longer complicated examples aren’t necessary short introductory document can add links readme explain \ndetail elsewhere.ideally short description people can report issues project also people can get started resolving issues extend \nproject way.leads one point ’ve forgotten list . section listing authors work license ’s\ndistributed. give credit people ’ve contributed \nproject license file says people may use work. license declates may use project whether give direct\nattribution work modifications use.","code":""},{"path":"workflows-organising-your-work.html","id":"data","chapter":"2 Organising your work","heading":"2.5.3 data","text":"Moving back project structure, next data directory.data directory two subdirectories one called raw one\ncalled derived. data generate part project stored raw subdirectory. ensure project reproducible, data Raw folder never edited modified.example ’ve got two different data types: Excel spreadsheet XLS file JSON file. files exacty received project stakeholder.text file metadata.txt plain text file explaining contents interpretation raw data sets. metadata include descriptions measured variables, units recorded , date file created acquired, source obtained.raw data likely isn’t going form ’s amenable analyzing straight away. get data pleasant form work, require data manipulation cleaning. manipulation cleaning applied well documented resulting cleaned files saved within derived data directory.exciting new project, can see clean versions previous data sets ready modelling. ’s also third file folder. \ndata ’ve aquired web scraping, using script within project.","code":""},{"path":"workflows-organising-your-work.html","id":"src","chapter":"2 Organising your work","heading":"2.5.4 src","text":"src source directory contains source code project. typically functions ’ve written make analysis modelling code accessible.’ve saved function R script , project, ’ve used subdirectories organise use case. ’ve got two functions used data cleaning: first replaces NA values given value, second replaces mean non-missing values.also three helper functions: first two calculate rolling mean \ngeometric mean given vector, third function scrapes \nweb data saw derived data subdirectory.","code":""},{"path":"workflows-organising-your-work.html","id":"tests","chapter":"2 Organising your work","heading":"2.5.5 tests","text":"Moving tests directory. structure directory mirrors source directory. function file counterpart file tests.test files provide example sets inputs expected outputs \nfunction. test files used check edge cases function assure\nhaven’t broken anything fixing small bug adding new capabilities function.","code":""},{"path":"workflows-organising-your-work.html","id":"analyses","chapter":"2 Organising your work","heading":"2.5.6 analyses","text":"analyses directory contains probably think bulk \ndata science work. ’s going one subdirectory major analysis ’s performed within project within might series steps collect separate scripts.activity performed step made clear name script, order ’re going perform steps. can see scripts used 2021 annual report. First script used take raw monthly receipts produce cleaned version data set saw earlier.\nfollowed trend analysis cleaned data set.Similarly spending review data cleaning step, followed forecast modelling finally production diagnostic plots compare \nforecasts.","code":""},{"path":"workflows-organising-your-work.html","id":"outputs","chapter":"2 Organising your work","heading":"2.5.7 outputs","text":"outputs directory one subdirectory meta-analysis within project. organized output type whether data, figure, table.Depending nature project, might want use modified subdirectory structure . example, ’re several numerical experiments might want arrange outputs experiment, rather output type.","code":""},{"path":"workflows-organising-your-work.html","id":"reports","chapter":"2 Organising your work","heading":"2.5.8 reports","text":"reports directory everything comes together. written documents form final deliverables project created. final documents written LaTeX markdown, source \ncompiled documents can found within directory.including content report, example figures, ’d recommend \nmaking copies figure files within reports directory. , ’ll manually update files every time modify . Instead can use relative file paths include figures. Relative file paths specify get image, starting TeX document moving levels project directory.’re using markdown LaTeX write reports, instead use online platform like overleaf latex editor Google docs write collaboratively links reports directory using additional readme files. Make sure set read write permissions links appropriately, .using online writing systems, ’ll manually upload update plots whenever modify earlier analysis. ’s one drawbacks online tools traded ease use.exciting new project, can see annual report written markdown format, compiled HTML PDF. spendiing review written LaTeX source , don’t compiled pdf version document.","code":""},{"path":"workflows-organising-your-work.html","id":"make-file","chapter":"2 Organising your work","heading":"2.5.9 make file","text":"final element template project structure make file. aren’t going cover read write make files course. Instead, ’ll give brief description supposed .high level, make file just text file. makes special contains. Similar shell bash script, make file contains code run command line. code create update element project.make file defines shorthand commands full lines code create element project. make file also records order operations happen, steps dependent one another. means one step part project updated changes propagated entire project. done quite clever way part projects re-run need updated.’re omitting make files course ’re fiendishly difficult write read, rather require reasonable foundation working command line understood. suggest instead throughout course create R markdown file called make. file define intended running order dependencies project R file, might also automate parts analysis.","code":""},{"path":"workflows-organising-your-work.html","id":"wrapping-up","chapter":"2 Organising your work","heading":"2.6 Wrapping up","text":"Wrapping , ’s everything video.’ve introduced project structure serve well baseline vast majority projects data science.work, remember key standardisation. Working consistently across projects, company group important sticking rigidly particular structure defined .two notable exceptions probably don’t want use project structure. ’s ’re building app ’re building package. require specific organisation files within project directory. ’ll\nexplore project structure used package development live\nsession week.","code":""},{"path":"workflows-naming.html","id":"workflows-naming","chapter":"3 Naming Files","heading":"3 Naming Files","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-naming.html","id":"naming-things","chapter":"3 Naming Files","heading":"3.1 Naming things","text":"Naming things\nJenny Bryan slide summary (https://speakerdeck.com/jennybc/--name-files)\nlike file names :\nMachine Readable\nHuman Readable\nOrder friendly\n\nMachine readable:\nregex globbing friendly: avoid spaces, punctuation, accents, cases\neasy compute : deliberate use delimiters (spaces aren’t spaces)\nhyphens separate words, underscores separate metadata\n\nuseful : searching files later, narrow file list based names, extract information file names, new regex (sadist)\n\nHuman Readable:\nName contains information content. ( untitled31.R, finalreportV8.docx, temp.txt)\nconnects concept slug URLs\nset filenames want 3am deadline?\n\nDefault order friendly\nput something numeric first\nuse ISO 8601 standard dates: YYYY-MM-DD\nleft pad zeros achieve chronological logical order within directory\n\nSummary:\nMachine readable, human readable, default order friendly\nBrushing teeth analogy: tedious get habit. Huge long-term rewards\n\nJenny Bryan slide summary (https://speakerdeck.com/jennybc/--name-files)Jenny Bryan slide summary (https://speakerdeck.com/jennybc/--name-files)like file names :\nMachine Readable\nHuman Readable\nOrder friendly\nlike file names :Machine ReadableHuman ReadableOrder friendlyMachine readable:\nregex globbing friendly: avoid spaces, punctuation, accents, cases\neasy compute : deliberate use delimiters (spaces aren’t spaces)\nhyphens separate words, underscores separate metadata\n\nuseful : searching files later, narrow file list based names, extract information file names, new regex (sadist)\nMachine readable:regex globbing friendly: avoid spaces, punctuation, accents, caseseasy compute : deliberate use delimiters (spaces aren’t spaces)\nhyphens separate words, underscores separate metadata\nhyphens separate words, underscores separate metadatauseful : searching files later, narrow file list based names, extract information file names, new regex (sadist)Human Readable:\nName contains information content. ( untitled31.R, finalreportV8.docx, temp.txt)\nconnects concept slug URLs\nset filenames want 3am deadline?\nHuman Readable:Name contains information content. ( untitled31.R, finalreportV8.docx, temp.txt)connects concept slug URLsWhich set filenames want 3am deadline?Default order friendly\nput something numeric first\nuse ISO 8601 standard dates: YYYY-MM-DD\nleft pad zeros achieve chronological logical order within directory\nDefault order friendlyput something numeric firstuse ISO 8601 standard dates: YYYY-MM-DDleft pad zeros achieve chronological logical order within directorySummary:\nMachine readable, human readable, default order friendly\nBrushing teeth analogy: tedious get habit. Huge long-term rewards\nSummary:Machine readable, human readable, default order friendlyBrushing teeth analogy: tedious get habit. Huge long-term rewards","code":""},{"path":"workflows-code.html","id":"workflows-code","chapter":"4 Code","heading":"4 Code","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-code.html","id":"code","chapter":"4 Code","heading":"4.1 Code","text":"Code\nthing twice write function\nwrite function, document \nwrite function, test \nmight ever want use function , add package\nnaming things revisited:\nfunctions=verbs,\nobjects=nouns,\nreadable code,\nCamelCase snakecase pointless.points\ntidyverse google style guides R\n\nfilepaths relative root directory (top level project)\nadvanced: ::\n\nthing twice write functionIf write function, document itIf write function, test itIf might ever want use function , add packagenaming things revisited:\nfunctions=verbs,\nobjects=nouns,\nreadable code,\nCamelCase snakecase pointless.points\ntidyverse google style guides R\nfunctions=verbs,objects=nouns,readable code,CamelCase snakecase pointless.pointstidyverse google style guides Rall filepaths relative root directory (top level project)\nadvanced: ::\nadvanced: ::","code":""},{"path":"workflows-project-management.html","id":"workflows-project-management","chapter":"5 Project Management","heading":"5 Project Management","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-project-management.html","id":"bigger-picture-project-management","chapter":"5 Project Management","heading":"5.1 Bigger Picture: Project management","text":"Project management\nDefining outcomes\nscoping projects, (rememberer said code single project live single directory?)\ncontinuous development, agile + jira ?\nLinking github (extension)\nDefining outcomesscoping projects, (rememberer said code single project live single directory?)continuous development, agile + jira ?Linking github (extension)","code":""},{"path":"workflows-checklist.html","id":"workflows-checklist","chapter":"Workflows: Checklist","heading":"Workflows: Checklist","text":"Effective Data Science still work--progress. chapter undergoing heavy restructuring may confusing incomplete.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-checklist.html","id":"videos-chapters","chapter":"Workflows: Checklist","heading":"5.2 Videos / Chapters","text":"Organising workNaming FilesOrganising codeProject Management (AY 2023/24 onward)","code":""},{"path":"workflows-checklist.html","id":"reading","chapter":"Workflows: Checklist","heading":"5.3 Reading","text":"Good enough practices scientific computingGood enough practices scientific computingBayesian workflows: Michael BetancourtBayesian workflows: Michael Betancourthttps://www.atlassian.com/agile/project-managementhttps://www.atlassian.com/agile/project-managementR4DS project workflowR4DS project workflowhere::herehere::hereR packagesR packageshappy git Rhappy git R","code":""},{"path":"workflows-checklist.html","id":"tasks","chapter":"Workflows: Checklist","heading":"5.4 Tasks","text":"go github find 3 different data science projects, explore organise work.create projects course assignments.bonus: put Github (make sure assignments private repos!)","code":""},{"path":"workflows-checklist.html","id":"live-session","chapter":"Workflows: Checklist","heading":"5.5 Live Session","text":"Discussion point live session:make assignment projects subdirectories stand alone projects? ?make assignment projects subdirectories stand alone projects? ?terms met readings?terms met readings?Activity live session:making minimal R package course","code":""},{"path":"data-introduction.html","id":"data-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.Data can difficult acquire gnarly get .","code":""},{"path":"data-tabular.html","id":"data-tabular","chapter":"6 Tabular Data","heading":"6 Tabular Data","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-tabular.html","id":"reading-tabular-data","chapter":"6 Tabular Data","heading":"6.1 Reading Tabular Data","text":"","code":""},{"path":"data-tabular.html","id":"excel-is-not-the-only-file-type","chapter":"6 Tabular Data","heading":"6.1.1 Excel is not the only file type","text":"(Img: Oranges fruit book cover)data seen already:\ntabular data: csv, tsv\nlist-like data: JSON, XML","code":""},{"path":"data-tabular.html","id":"reading-in-tabular-data","chapter":"6 Tabular Data","heading":"6.1.2 Reading in tabular data","text":"read.csv() base R function load csv file memory.Whirlwind tour reading file formats..r readr::read_csv(file = \"path//file.csv\")\n.r readr::read_tsv(file = \"path//file.tsv\")\n.r readr::read_delim(file = \"path//file.txt\", sep =\" \")csvs read using read.csv() tabular data created data.frame R.ideal several reasons:Pretty printing: print method tibbles highlighting defaults first 10 rows, columns fit screen. addition name, column reports type.limited printing designed don’t accidentally overwhelm console print large data frames. can override default printing behaviour specifying number rows n number columns width like print part function call.Type stability: subset data.frame, type object get back depends ask . can lead forgotten edge cases writing functions work create data frames.extracted row still data.frame:\nprint(example_data_frame[1,])\nclass(example_data_frame[1,])extracted column vector:\nprint(example_data_frame[,1])\nclass(example_data_frame[,1])Similar bugs can introduced data frames allowing partial matching: column names used subsetting partially matched strings quietly continue function add another column breaks partial match.Reproducibility: Base R functions inherit behaviour operating system environment variables, import code works computer might work someone else’sstringsAsFactors = FALSE","code":"set.seed(1234)\nexample_data_frame <- data.frame(\n  gaussian = rnorm(10), \n  uniform = runif(10), \n  gamma = rgamma(10,1,1))\n\nset.seed(1234)\nexample_tibble <- tibble::tibble(\n  gaussian = rnorm(10),\n  uniform = runif(10),\n  gamma = rgamma(10,1,1))example_data_frame$uniform\nexample_data_frame$u\nexample_data_frame[[\"u\"]]\n\nexample_data_frame$gaussian\nexample_data_frame$gaus\nexample_data_frame$g          # returns NULL b/c of name conflict\n\nexample_tibble$g              # returns NULL and a warning messagenycflights13::flights %>% \n  print(n = 10, width = Inf)"},{"path":"data-webscraping.html","id":"data-webscraping","chapter":"7 Web Scraping","heading":"7 Web Scraping","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-webscraping.html","id":"scraping-webpage-data-using-rvest","chapter":"7 Web Scraping","heading":"7.1 Scraping webpage data using {rvest}","text":"can’t always rely tidy, tabular data land desk. Sometimes going go gather data .’m suggesting need manually, likely need get data internet ’s made publicly privately available .might information webpage gather , data shared collaborator using API.chapter cover basics scraping webpages, following vignette {rvest} package.","code":""},{"path":"data-webscraping.html","id":"what-is-a-webpage","chapter":"7 Web Scraping","heading":"7.2 What is a webpage?","text":"can even hope get data webpage, first need understand webpage .Webpages written similar way LaTeX: content styling webpages handled separately coded using plain text files.fact, websites go one step LaTeX. content styling websites written different files different languages. HTML (HyperText Markup Language) used write content CSS (Cascading Style Sheets) used control appearance content ’s displayed user.","code":""},{"path":"data-webscraping.html","id":"html","chapter":"7 Web Scraping","heading":"7.3 HTML","text":"basic HTML page styling applied might look something like :","code":"<html>\n<head>\n  <title>Page title<\/title>\n<\/head>\n<body>\n  <h1 id='first'>A level 1 heading<\/h1>\n  <p>Hello World!<\/p>\n  <p>Here is some plain text &amp; <b>some bold text.<\/b><\/p>\n  <img src='myimg.png' width='100' height='100'>\n<\/body>"},{"path":"data-webscraping.html","id":"html-elements","chapter":"7 Web Scraping","heading":"7.3.1 HTML elements","text":"Just like XML data files, HTML hierarchical structure. structure crafted using HTML elements. HTML element made start tag, optional attributes, end tag.can see first level header, <h1> opening tag, id='first' additional attribute <\/h1> closing tag. Everything opening closing tag contents element. also special elements consist single tag optional attributes. example <img> tag.Since < > used start end tags, can’t write directly HTML document. Instead, use escape characters. sounds fancy, ’s just alternative way write characters serve special function within language.can write greater &gt; less &lt;. might notice escapes use ampersand (&). means want literal ampersand webpage, escape using &amp;.wide range possible HTML tags escapes. ’ll cover common tags lecture don’t need worry escapes much rvest automatically handle .","code":""},{"path":"data-webscraping.html","id":"important-html-elements","chapter":"7 Web Scraping","heading":"7.3.2 Important HTML Elements","text":", excess 100 HTML elements. important ones know :<html> element, must enclose every HTML page. <html> element must two child elements within . <head> element contains metadata document, like page title shown browser tab CSS style sheet applied. <body> element contains content see browser.<html> element, must enclose every HTML page. <html> element must two child elements within . <head> element contains metadata document, like page title shown browser tab CSS style sheet applied. <body> element contains content see browser.Block elements used give structure page. elements like headings, sub-headings <h1> way <h6>. category also contains paragraph elements <p>, ordered lists <ol> unordered lists <ul>.Block elements used give structure page. elements like headings, sub-headings <h1> way <h6>. category also contains paragraph elements <p>, ordered lists <ol> unordered lists <ul>.Finally, inline tags like <b> bold, <> italics, <> hyperlinks used format text inside block elements.Finally, inline tags like <b> bold, <> italics, <> hyperlinks used format text inside block elements.come across tag ’ve never seen , can find just little bit googling. good resource MDN Web Docs produced Mozilla, company makes Firefox web browser. W3schools website another great resource web development coding resources generally.","code":""},{"path":"data-webscraping.html","id":"html-attributes","chapter":"7 Web Scraping","heading":"7.4 HTML Attributes","text":"’ve seen one example header additional attribute. generally, tags can named attributes. attributes contained within opening tag look something like:Two important attributes id class. attributes used conjunction CSS file control visual appearance page. often useful identify elements interested scraping data page.","code":"<tag attribute1='value1' attribute2='value2'>element contents<\/tag>"},{"path":"data-webscraping.html","id":"css-selectors","chapter":"7 Web Scraping","heading":"7.5 CSS Selectors","text":"Cascading Style Sheet used describe HTML content displayed. , CSS ’s system selecting elements webpage, called CSS selectors.CSS selectors define patterns locating HTML elements particular style applied . happy side-effect can sometimes useful scraping, provide concise way describing elements want extract.CSS Selectors can work level element type, class, tag can used nested (cascading) way.p selector select paragraph <p> elements.p selector select paragraph <p> elements..title selector select elements class “title”..title selector select elements class “title”.p.special selector select <p> elements class “special”.p.special selector select <p> elements class “special”.#title selector select element id attribute “title”.#title selector select element id attribute “title”.want select single element id attributes particularly useful must unique within html document. Unfortunately, helpful developer added id attribute element(s) want scrape!want learn CSS selectors recommend starting fun CSS dinner tutorial build base knowledge using W3schools resources reference explore webpages wild.","code":""},{"path":"data-webscraping.html","id":"which-attributes-and-selectors-do-you-need","chapter":"7 Web Scraping","heading":"7.6 Which Attributes and Selectors Do You Need?","text":"scrape data webpage, first identify tag attribute combinations interested gathering.find elements interest, three options. go hardest easiest also least robust.right click + “inspect page source” (F12)right click + “inspect”Rvest Selector Gadget (useful fallible)Inspecting source familiar websites can useful way get head around concepts. Beware though sophisticated webpages can quite intimidating. good place start simpler, static websites personal websites, rather dynamic webpages online retailers social media platforms.","code":""},{"path":"data-webscraping.html","id":"reading-html-with-rvest","chapter":"7 Web Scraping","heading":"7.7 Reading HTML with {rvest}","text":"rvest, reading html page can simple loading tabular data.class resulting object xml_document. type object low-level package xml2, allows read xml files R.can see object split several components: first metadata type document scraped, followed head body html document.several possible approaches extracting information document.","code":"\nhtml <- rvest::read_html(\"https://www.zakvarty.com/professional/teaching.html\")\nclass(html)\n#> [1] \"xml_document\" \"xml_node\"\nhtml\n#> {html_document}\n#> <html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#> [2] <body class=\"nav-fixed\">\\n\\n<div id=\"quarto-search-results\"><\/div>\\n   ..."},{"path":"data-webscraping.html","id":"extracting-html-elements","chapter":"7 Web Scraping","heading":"7.8 Extracting HTML elements","text":"rvest can extract single element html_element(), matching elements html_elements(). functions take document object one CSS selectors inputs.can also combine nest selectors. example might want extract links within paragraphs second level headers.","code":"\nlibrary(rvest)\n\nhtml %>% html_elements(\"h1\")\n#> {xml_nodeset (1)}\n#> [1] <h1>Teaching<\/h1>\nhtml %>% html_elements(\"h2\")\n#> {xml_nodeset (2)}\n#> [1] <h2 id=\"toc-title\">On this page<\/h2>\n#> [2] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History<\/h2>\nhtml %>% html_elements(\"p\")\n#> {xml_nodeset (2)}\n#> [1] <p>I am fortunate to have had the opportunity to teach in a variety of ...\n#> [2] <p>I am an associate fellow of the Higher Education Academy, which you ...\nhtml %>% html_elements(\"p a,h2\")\n#> {xml_nodeset (3)}\n#> [1] <h2 id=\"toc-title\">On this page<\/h2>\n#> [2] <a href=\"https://www.advance-he.ac.uk/fellowship/associate-fellowship\" ...\n#> [3] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History<\/h2>"},{"path":"data-webscraping.html","id":"extracting-data-from-html-elements","chapter":"7 Web Scraping","heading":"7.9 Extracting Data From HTML Elements","text":"Now ’ve got elements care extracted complete document. get data need elements?’ll usually get data either contents HTML element else one ’s attributes. ’re really lucky, data need already formatted HTML table list.","code":""},{"path":"data-webscraping.html","id":"extracting-text","chapter":"7 Web Scraping","heading":"7.9.1 Extracting text","text":"functions rvest::html_text() rvest::html_text2() can used extract plain text contents HTML element.difference html_text() html_text2() handle whitespace. HTML whitespace line breaks little influence code interpreted computer (similar R different Python). html_text() extract text raw html, html_text2() best extract text way gives something similar ’d see browser.","code":"\nhtml %>% \n  html_elements(\"#teaching li\") %>% \n  html_text2()\n#> [1] \"one-to-one tuition for high school students;\"                                   \n#> [2] \"running workshops and computer labs for undergraduate and postgraduate modules;\"\n#> [3] \"delivering short courses on scientific communication and LaTeX;\"                \n#> [4] \"supervising an undergraduate research project;\"                                 \n#> [5] \"developing and lecturing postgraduate modules in statistics and data science.\""},{"path":"data-webscraping.html","id":"extracting-attributes","chapter":"7 Web Scraping","heading":"7.9.2 Extracting Attributes","text":"Attributes also used record information might like collect. example, destination links stored href attribute source images stored src attribute.example , consider trying extract twitter link icon page footer. quite tricky locate html source, used Selector Gadget help find correct combination elements.extract href attribute scraped element, use rvest::html_attr() function.Note: rvest::html_attr() always return character string (list character strings). extracting attribute describes quantity, width image, ’ll need convert string required data type. example, width measures pixels might use .integer().","code":"\nhtml %>% html_element(\".compact:nth-child(1) .nav-link\")\n#> {html_node}\n#> <a class=\"nav-link\" href=\"https://www.twitter.com/zakvarty\">\n#> [1] <i class=\"bi bi-twitter\" role=\"img\">\\n<\/i>\nhtml %>% \n  html_elements(\".compact:nth-child(1) .nav-link\") %>% \n  html_attr(\"href\")\n#> [1] \"https://www.twitter.com/zakvarty\""},{"path":"data-webscraping.html","id":"extracting-tables","chapter":"7 Web Scraping","heading":"7.9.3 Extracting tables","text":"HTML tables composed similar, nested manner LaTeX tables.four main elements know make HTML table:<table>,<tr> (table row),<th> (table heading),<td> (table data).’s simple example data, formatted HTML table:Since tables common way store data, rvest includes useful function html_table() converts directly HTML table tibble.Applying real scraped data can easily extract table taught courses.","code":"\nhtml_2 <- minimal_html(\"\n  <table>\n    <tr>\n      <th>Name<\/th>\n      <th>Number<\/th>\n    <\/tr>\n    <tr>\n      <td>A<\/td>\n      <td>1<\/td>\n    <\/tr>\n    <tr>\n      <td>B<\/td>\n      <td>2<\/td>\n    <\/tr>\n    <tr>\n      <td>C<\/td>\n      <td>3<\/td>\n    <\/tr>\n  <\/table>\n  \")\nhtml_2 %>% \n  html_element(\"table\") %>% \n  html_table()\n#> # A tibble: 3 × 2\n#>   Name  Number\n#>   <chr>  <int>\n#> 1 A          1\n#> 2 B          2\n#> 3 C          3\nhtml %>% \n  html_element(\"table\") %>% \n  html_table()\n#> # A tibble: 25 × 3\n#>   Year      Course                            Role                       \n#>   <chr>     <chr>                             <chr>                      \n#> 1 \"2021-22\" Supervised Learning               Lecturer                   \n#> 2 \"\"        Ethics in Data Science I          Lecturer                   \n#> 3 \"\"        Ethics in Data Science II         Lecturer                   \n#> 4 \"—\"       —                                 —                          \n#> 5 \"2020-21\" MATH562/582: Extreme Value Theory Lecturer                   \n#> 6 \"\"        MATH331: Bayesian Inference       Graduate teaching assistant\n#> # … with 19 more rows"},{"path":"data-webscraping.html","id":"tip-for-building-tibbles","chapter":"7 Web Scraping","heading":"7.10 Tip for Building Tibbles","text":"scraping data webpage, end-goal typically going constructing data.frame tibble.following description tidy data, ’ll want row correspond repeated unit HTML page. case, shouldUse html_elements() select elements contain observation unit;Use html_element() extract variables observations.Taking approach guarantees ’ll get number values variable, html_element() always returns number outputs inputs. vital missing data - every observation unit value every variable interest.example, consider extract text starwars dataset.unordered list list item corresponds one observational unit (one character starwars universe). name character given bold, character species specified italics weight character denoted .weight class. However, characters subset variables defined: example Yoda species entry.try extract element directly, vectors variable values different lengths. don’t know missing values , can’t line back make tibble.instead start extracting list item elements using html_elements(). done , can use html_element() extract variable characters. pad NAs, can collate tibble.","code":"\nstarwars_html <- minimal_html(\"\n  <ul>\n    <li><b>C-3PO<\/b> is a <i>droid<\/i> that weighs <span class='weight'>167 kg<\/span><\/li>\n    <li><b>R2-D2<\/b> is a <i>droid<\/i> that weighs <span class='weight'>96 kg<\/span><\/li>\n    <li><b>Yoda<\/b> weighs <span class='weight'>66 kg<\/span><\/li>\n    <li><b>R4-P17<\/b> is a <i>droid<\/i><\/li>\n  <\/ul>\n  \")\nstarwars_html %>% html_elements(\"b\") %>% html_text2()\n#> [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_html %>% html_elements(\"i\") %>% html_text2()\n#> [1] \"droid\" \"droid\" \"droid\"\nstarwars_html %>% html_elements(\".weight\") %>% html_text2()\n#> [1] \"167 kg\" \"96 kg\"  \"66 kg\"\nstarwars_characters <- starwars_html %>% html_elements(\"li\")\n\nstarwars_characters %>% html_element(\"b\") %>% html_text2()\n#> [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_characters %>% html_element(\"i\") %>% html_text2()\n#> [1] \"droid\" \"droid\" NA      \"droid\"\nstarwars_characters %>% html_element(\".weight\") %>% html_text2()\n#> [1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA\ntibble::tibble(\n  name = starwars_characters %>% html_element(\"b\") %>% html_text2(),\n  species = starwars_characters %>% html_element(\"i\") %>% html_text2(),\n  weight = starwars_characters %>% html_element(\".weight\") %>% html_text2()\n)\n#> # A tibble: 4 × 3\n#>   name   species weight\n#>   <chr>  <chr>   <chr> \n#> 1 C-3PO  droid   167 kg\n#> 2 R2-D2  droid   96 kg \n#> 3 Yoda   <NA>    66 kg \n#> 4 R4-P17 droid   <NA>"},{"path":"data-apis.html","id":"data-apis","chapter":"8 APIs","heading":"8 APIs","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-apis.html","id":"aquiring-data-via-an-api","chapter":"8 APIs","heading":"8.1 Aquiring Data Via an API","text":"’ve already established can’t always rely tidy, tabular data land desk.Sometimes going go gather data . already seen scrape information directly HTML source webpage. surely easer way. Thankfully, often !chapter cover basics obtaining data via API. material draws together Introduction APIs book Brian Cooksey DIY web data section STAT545 University British Columbia.","code":""},{"path":"data-apis.html","id":"why-do-i-need-to-know-about-apis","chapter":"8 APIs","heading":"8.2 Why do I need to know about APIs?","text":"API, application programming interface, set rules allows different software applications communicate .data scientist, often need access data stored remote servers cloud-based services. APIs provide convenient way data scientists programmatically retrieve data, without manually download data sets process locally computer.multiple benefits including automation standardisation data sharing.Automation: much faster machine process data request human. machine handling data requests also scales much better either number complexity data requests grows. Additionally, lower risk introducing human error. example, human might accidentally share wrong data, can serious legal repercussions.Automation: much faster machine process data request human. machine handling data requests also scales much better either number complexity data requests grows. Additionally, lower risk introducing human error. example, human might accidentally share wrong data, can serious legal repercussions.Standardisation: machine process data requests requires format requests associated responses standardised. allows data sharing retrieval become reproducible programmatic aspect work.Standardisation: machine process data requests requires format requests associated responses standardised. allows data sharing retrieval become reproducible programmatic aspect work.","code":""},{"path":"data-apis.html","id":"what-is-an-api","chapter":"8 APIs","heading":"8.3 What is an API?","text":", APIs great, exactly ?human--human communication, set rules governing acceptable behaviour known etiquette. Depending live, social etiquette can rather strict. rules computer--computer communication take whole new level, machines can room left interpretation.set rules governing interactions computers programmes known protocol.APIs provide standard protocol different programs interact one another. makes easier developers build complex systems leveraging functionality existing services platforms. benefits working standardised modular way apply equally well sharing data writing code organising files.two sides communication machines communicate known server client.Servers can seem intimidating, unlike laptop mobile phone don’t input output devices; keyboard, monitor, mouse. Despite , servers just regular computers designed store data run programmes. Servers don’t input output devices intended used remotely, via another computer. need screen mouse user miles away. Nothing scary going !People often find clients much less intimidating - simply computer application might contact sever.","code":""},{"path":"data-apis.html","id":"http","chapter":"8 APIs","heading":"8.4 HTTP","text":"leads us one step rabbit-hole. API protocol defines rules applications communicate one another. communication happen?HTTP (Hypertext Transfer Protocol) dominant mode communication World Wide Web. can see secure version HTTP, HTTPS, start web addresses top browser. example:HTTP foundation data communication web used transfer files (text, images, videos) web servers clients.understand HTTP communications, find helpful imagine client server customer waiter restaurant. client makes request server, tries comply giving response. server might respond confirm request completed successfully. Alternatively, server might respond error message, (hopefully) informative request completed.request-response model basis HTTP, communication system used majority APIs.","code":"https://www.zakvarty.com/blog"},{"path":"data-apis.html","id":"http-requests","chapter":"8 APIs","heading":"8.5 HTTP Requests","text":"HTML request consists :Uniform Resource Locator (URL) [unique identifier thing]Method [tells server type action requested client]Headers [meta-information request, e.g. device type]Body [Data client wants send server]","code":""},{"path":"data-apis.html","id":"url","chapter":"8 APIs","heading":"8.5.1 URL","text":"URL HTTP request specifies request going made, example http://example.com.","code":""},{"path":"data-apis.html","id":"method","chapter":"8 APIs","heading":"8.5.2 Method","text":"action client wants take indicated set well-defined methods HTTP verbs. common HTTP verbs GET, POST, PUT, PATCH, DELETE.GET verb used retrieve resource server, web page image. POST verb used send data server, submitting form uploading file. PUT verb used replace resource server new one, PATCH verb used update resource server without replacing entirely. Finally, DELETE verb used delete resource server.addition common HTTP verbs, also several less frequently used verbs. used specialized purposes, requesting headers resource, testing connectivity client server.","code":""},{"path":"data-apis.html","id":"header","chapter":"8 APIs","heading":"8.5.3 Header","text":"request headers contain meta-information request. information device type included within request.","code":""},{"path":"data-apis.html","id":"body","chapter":"8 APIs","heading":"8.5.4 Body","text":"Finally, body request contains data client providing server.","code":""},{"path":"data-apis.html","id":"http-responses","chapter":"8 APIs","heading":"8.6 HTTP Responses","text":"server receives request attempt fulfil send response back client.response similar structure request apart :responses URL,responses method,responses status code.","code":""},{"path":"data-apis.html","id":"status-codes","chapter":"8 APIs","heading":"8.6.1 Status Codes","text":"status code 3 digit number, specific meaning. common error codes might (already ) come across :200: Success,404: Page found (400s errors),503: Page .data science context, successful response return requested data within data field. likely given JSON XML format.","code":""},{"path":"data-apis.html","id":"authentication","chapter":"8 APIs","heading":"8.7 Authentication","text":"Now know applications communicate, might ask can control access API types request can make. can done server setting appropriate permissions client. server verify client really claims ?Authentication way ensure authorized clients able access API. typically done server requiring client provide secret information uniquely identifies , whenever make requests API. information allows API server validate authenticity user authorises request.","code":""},{"path":"data-apis.html","id":"basic-authentication","chapter":"8 APIs","heading":"8.7.1 Basic Authentication","text":"various ways implement API authentication.Basic authentication involves legitimate client username password. encrypted version included Authorization header HTTP request. hear matches server’s records request processed. , special status code (401) returned client.Basic authentication dangerous put restrictions client can authorised. Additional, individualised restrictions can added using alternative authentication scheme.","code":""},{"path":"data-apis.html","id":"api-key-authentication","chapter":"8 APIs","heading":"8.7.2 API Key Authentication","text":"API key long, random string letters numbers assigned authorised user. API key distinct user’s password keys typically issued service provides API. Using keys rather basic authentication allows API provider track limit usage API.example, provider may issue unique API key developer organization wants use API. provider can limit access certain data. also limit number requests key can make given time period prevent access certain administrative functions, like changing passwords deleting accounts.Unlike Basic Authentication, standard way client sharing key server. Depending API might Authorization field header, end URL (http://example.com?api_key=my_secret_key), within body data.","code":""},{"path":"data-apis.html","id":"api-wrappers","chapter":"8 APIs","heading":"8.8 API wrappers","text":"’ve learned lot internet works. Fortunately, lot time won’t worry new information debugging purposes.best case scenario, kind developer written “wrapper” function API. wrappers functions R construct HTML request . particularly lucky, API wrapper also format response , converting XML JSON back R object ready immediate use.","code":""},{"path":"data-apis.html","id":"geonames-wrapper","chapter":"8 APIs","heading":"8.9 {geonames} wrapper","text":"rOpenSci curated list many wrappers accessing scientific data using R. focus GeoNames API, gives open access geographical database. access data, use wrapper functions provided {geonames} package.aim illustrate important steps getting started new API.","code":""},{"path":"data-apis.html","id":"set-up","chapter":"8 APIs","heading":"8.9.1 Set up","text":"can get data GeoNames API, first need little bit set .Install load {geonames} CRANCreate user account GeoNames APIActivate account (see activation email)Enable free web services GeoNames account logging link.Enable free web services GeoNames account logging link.Tell R credentials GeoNames.Tell R credentials GeoNames.use following code tell R credentials, absolutely .save username environment variable, also puts API credentials directly script. share script others (internally, externally publicly) sharing credentials . good!","code":"\n#install.packages(\"geonames\")\nlibrary(geonames)\noptions(geonamesUsername=\"example_username\")"},{"path":"data-apis.html","id":"keep-it-secret-keep-it-safe","chapter":"8 APIs","heading":"8.9.2 Keep it Secret, Keep it Safe","text":"solution problem add credentials environment variables .Rprofile rather script. .Rprofile R script run start every session. can created edited directly, can also created edited within R.make/open .Rprofile use edit_r_profile() function usethis package.Within file, add options(geonamesUsername=\"example_username\") new line, remembering replace example_username GeoNames username.final step check file ends blank line, save restart R. set start using {geonames}.set procedure indicative API wrappers, course details vary API. good documentation important!","code":"\nlibrary(usethis)\nusethis::edit_r_profile()"},{"path":"data-apis.html","id":"using-geonames","chapter":"8 APIs","heading":"8.9.3 Using {geonames}","text":"GeoNames whole host different geo-datasets can explore.\nfirst example, let’s get geo-tagged wikipedia articles within 1km Imperial College London.Looking structure imperial_neighbours can see data frame one row per geo-tagged wikipedia article.confirm correct location can inspect title first five neighbours.Nothing surprising , mainly departments college Exhibition Road, runs along one side campus. sorts check important - initially forgot minus longitude getting results East London!","code":"\nimperial_coords <- list(lat = 51.49876, lon = -0.1749)\nsearch_radius_km <- 1\n\nimperial_neighbours <- geonames::GNfindNearbyWikipedia(\n  lat = imperial_coords$lat,\n  lng = imperial_coords$lon, \n  radius = search_radius_km,\n  lang = \"en\",                # english language articles\n  maxRows = 500              # maximum number of results to return \n)\nstr(imperial_neighbours)\n#> 'data.frame':    204 obs. of  13 variables:\n#>  $ summary     : chr  \"The Department of Mechanical Engineering is responsible for teaching and research in mechanical engineering at \"| __truncated__ \"Imperial College Business School is a global business school located in London. The business school was opened \"| __truncated__ \"Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est\"| __truncated__ \"Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one\"| __truncated__ ...\n#>  $ elevation   : chr  \"20\" \"18\" \"19\" \"24\" ...\n#>  $ feature     : chr  \"edu\" \"edu\" \"landmark\" \"edu\" ...\n#>  $ lng         : chr  \"-0.1746\" \"-0.1748\" \"-0.17425\" \"-0.1757\" ...\n#>  $ distance    : chr  \"0.0335\" \"0.0494\" \"0.0508\" \"0.0558\" ...\n#>  $ rank        : chr  \"81\" \"91\" \"90\" \"96\" ...\n#>  $ lang        : chr  \"en\" \"en\" \"en\" \"en\" ...\n#>  $ title       : chr  \"Department of Mechanical Engineering, Imperial College London\" \"Imperial College Business School\" \"Exhibition Road\" \"Imperial College School of Medicine\" ...\n#>  $ lat         : chr  \"51.498524\" \"51.4992\" \"51.4989722222222\" \"51.4987\" ...\n#>  $ wikipediaUrl: chr  \"en.wikipedia.org/wiki/Department_of_Mechanical_Engineering%2C_Imperial_College_London\" \"en.wikipedia.org/wiki/Imperial_College_Business_School\" \"en.wikipedia.org/wiki/Exhibition_Road\" \"en.wikipedia.org/wiki/Imperial_College_School_of_Medicine\" ...\n#>  $ countryCode : chr  NA \"AE\" NA \"GB\" ...\n#>  $ thumbnailImg: chr  NA NA NA NA ...\n#>  $ geoNameId   : chr  NA NA NA NA ...\nimperial_neighbours$title[1:5]\n#> [1] \"Department of Mechanical Engineering, Imperial College London\"             \n#> [2] \"Imperial College Business School\"                                          \n#> [3] \"Exhibition Road\"                                                           \n#> [4] \"Imperial College School of Medicine\"                                       \n#> [5] \"Department of Civil and Environmental Engineering, Imperial College London\""},{"path":"data-apis.html","id":"what-if-there-is-no-wrapper","chapter":"8 APIs","heading":"8.10 What if there is no wrapper?","text":"wrapper function, can still access APIs fairly easilty using httr package.look example using OMDb, open source version IMDb, get information movie Mean Girls.use OMDB API need request free API key, follow verification link add API key .Rprofile.can restart R safely access API key within R session.Using documentation API, requests URLs following form, terms angular brackets replaced .little bit effort, can write function composes type request URL us. using glue package help us join strings together.Running example get:can use httr package construct request store response get.Thankfully success! get 401 error code , check clicked activation link API key.full structure response quite complicated, can easily extract requested data using content()","code":"\n# Add this to .Rprofile, pasting in your own API key\noptions(OMDB_API_Key = \"PASTE YOUR KEY HERE\")\n# Load your API key into the current R session,\nombd_api_key <- getOption(\"OMDB_API_Key\")http://www.omdbapi.com/?t=<TITLE>&y=<YEAR>&plot=<LENGTH>&r=<FORMAT>&apikey=<API_KEY>\n\n#' Compose search requests for the OMBD API\n#'\n#' @param title String defining title to search for. Words are separated by \"+\".\n#' @param year String defining release year to search for\n#' @param plot String defining whether \"short\" or \"full\" plot is returned\n#' @param format String defining return format. One of \"json\" or \"xml\"\n#' @param api_key String defining your OMDb API key.\n#'\n#' @return String giving a OMBD search request URL\n#'\n#' @examples \n#' omdb_url(\"mean+girls\", \"2004\", \"short\", \"json\", getOption(OMBD_API_Key))\n#' \nomdb_url <- function(title, year, plot, format, api_key) {\n  glue::glue(\"http://www.omdbapi.com/?t={title}&y={year}&plot={plot}&r={format}&apikey={api_key}\")\n}\nmean_girls_request <- omdb_url(\n  title = \"mean+girls\",\n  year =  \"2004\",\n  plot = \"short\",\n  format =  \"json\",\n  api_key =  getOption(\"OMDB_API_Key\"))\nresponse <- httr::GET(url = mean_girls_request)\nhttr::status_code(response)\n#> [1] 200\nhttr::content(response)\n#> $Title\n#> [1] \"Mean Girls\"\n#> \n#> $Year\n#> [1] \"2004\"\n#> \n#> $Rated\n#> [1] \"PG-13\"\n#> \n#> $Released\n#> [1] \"30 Apr 2004\"\n#> \n#> $Runtime\n#> [1] \"97 min\"\n#> \n#> $Genre\n#> [1] \"Comedy\"\n#> \n#> $Director\n#> [1] \"Mark Waters\"\n#> \n#> $Writer\n#> [1] \"Rosalind Wiseman, Tina Fey\"\n#> \n#> $Actors\n#> [1] \"Lindsay Lohan, Jonathan Bennett, Rachel McAdams\"\n#> \n#> $Plot\n#> [1] \"Cady Heron is a hit with The Plastics, the A-list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex-boyfriend of alpha Plastic Regina George.\"\n#> \n#> $Language\n#> [1] \"English, German, Vietnamese, Swahili\"\n#> \n#> $Country\n#> [1] \"United States, Canada\"\n#> \n#> $Awards\n#> [1] \"7 wins & 25 nominations\"\n#> \n#> $Poster\n#> [1] \"https://m.media-amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@._V1_SX300.jpg\"\n#> \n#> $Ratings\n#> $Ratings[[1]]\n#> $Ratings[[1]]$Source\n#> [1] \"Internet Movie Database\"\n#> \n#> $Ratings[[1]]$Value\n#> [1] \"7.1/10\"\n#> \n#> \n#> $Ratings[[2]]\n#> $Ratings[[2]]$Source\n#> [1] \"Rotten Tomatoes\"\n#> \n#> $Ratings[[2]]$Value\n#> [1] \"84%\"\n#> \n#> \n#> $Ratings[[3]]\n#> $Ratings[[3]]$Source\n#> [1] \"Metacritic\"\n#> \n#> $Ratings[[3]]$Value\n#> [1] \"66/100\"\n#> \n#> \n#> \n#> $Metascore\n#> [1] \"66\"\n#> \n#> $imdbRating\n#> [1] \"7.1\"\n#> \n#> $imdbVotes\n#> [1] \"385,107\"\n#> \n#> $imdbID\n#> [1] \"tt0377092\"\n#> \n#> $Type\n#> [1] \"movie\"\n#> \n#> $DVD\n#> [1] \"21 Sep 2004\"\n#> \n#> $BoxOffice\n#> [1] \"$86,058,055\"\n#> \n#> $Production\n#> [1] \"N/A\"\n#> \n#> $Website\n#> [1] \"N/A\"\n#> \n#> $Response\n#> [1] \"True\""},{"path":"data-apis.html","id":"wrapping-up-1","chapter":"8 APIs","heading":"8.11 Wrapping up","text":"learned bit internet works, benefits using API share data request data Open APIs.obtaining data internet ’s vital keep credentials safe, don’t work needed.Keep API keys code. Store .Rprofile (make sure version control!)Keep API keys code. Store .Rprofile (make sure version control!)Scraping always last resort. API already?Scraping always last resort. API already?Writing code access API can painful necessary.Writing code access API can painful necessary.Don’t repeat people, suitable wrapper exists use .Don’t repeat people, suitable wrapper exists use .","code":""},{"path":"data-sql.html","id":"data-sql","chapter":"9 SQL","heading":"9 SQL","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.(Content added AY 2023-24)","code":""},{"path":"data-sql.html","id":"databases-and-sql","chapter":"9 SQL","heading":"9.1 Databases and SQL","text":"Structure webpage. Web Scraping source dataStructure webpage. Web Scraping source dataAPIs source data, data files beyond csv.APIs source data, data files beyond csv.Data bases SQLData bases SQLData always nicely formatted csvData always nicely formatted csvBeginner: reading clipboard googlesheetsBeginner: reading clipboard googlesheetsIntermediate: reading messy csvs making workflow robust new versionsIntermediate: reading messy csvs making workflow robust new versionsAdvanced:\nWebscraping APIs data sources, data files beyond CSV\ndata files beyond csv, benefits drawbacks: JSON, xml, parquet, .Rdata, .pkl\nData bases data sources: Basic SQL verbs\nLearning SQL theory small scale: dplyr verbs\nAdvanced:Webscraping APIs data sources, data files beyond CSVdata files beyond csv, benefits drawbacks: JSON, xml, parquet, .Rdata, .pklData bases data sources: Basic SQL verbsLearning SQL theory small scale: dplyr verbs","code":""},{"path":"data-checklist.html","id":"data-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-checklist.html","id":"videos-chapters-1","chapter":"Checklist","heading":"9.2 Videos / Chapters","text":"[ ][ ][ ][ ][ ][ ]","code":""},{"path":"data-checklist.html","id":"reading-1","chapter":"Checklist","heading":"9.3 Reading","text":"","code":""},{"path":"data-checklist.html","id":"activities","chapter":"Checklist","heading":"9.4 Activities","text":"","code":""},{"path":"data-checklist.html","id":"live-session-1","chapter":"Checklist","heading":"9.5 Live Session","text":"","code":""},{"path":"edav-introduction.html","id":"edav-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-cleaning.html","id":"edav-cleaning","chapter":"11 Data Cleaning","heading":"11 Data Cleaning","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-analysis.html","id":"edav-analysis","chapter":"12 Exploratory Data Analysis","heading":"12 Exploratory Data Analysis","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-visualisation.html","id":"edav-visualisation","chapter":"13 Data Visualisation","heading":"13 Data Visualisation","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-checklist.html","id":"edav-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-checklist.html","id":"videos-chapters-2","chapter":"Checklist","heading":"13.1 Videos / Chapters","text":"[ ][ ][ ][ ][ ][ ]","code":""},{"path":"edav-checklist.html","id":"reading-2","chapter":"Checklist","heading":"13.2 Reading","text":"","code":""},{"path":"edav-checklist.html","id":"activities-1","chapter":"Checklist","heading":"13.3 Activities","text":"","code":""},{"path":"edav-checklist.html","id":"live-session-2","chapter":"Checklist","heading":"13.4 Live Session","text":"","code":""},{"path":"production-introduction.html","id":"production-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-reproducibility.html","id":"production-reproducibility","chapter":"14 Reproducibility","heading":"14 Reproducibility","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-reproducibility.html","id":"reproducibility-and-replicability","chapter":"14 Reproducibility","heading":"14.1 Reproducibility and Replicability —————————–","text":"Write chapter following structure articlehttps://www.kdnuggets.com/2019/11/reproducibility-replicability-data-science.htmlhttps://www.kdnuggets.com/2019/11/reproducibility-replicability-data-science.htmlThe Ethical Algorithm M Kearns Roth (Chapter 4) https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591&context=L&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Local%20Search%20Engine&tab=Everything&query=,contains,kearns%20and%20roth&mode=BasicThe Ethical Algorithm M Kearns Roth (Chapter 4) https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591&context=L&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Local%20Search%20Engine&tab=Everything&query=,contains,kearns%20and%20roth&mode=BasicThe ASA Statement \\(p\\)-values: Context, Process Purpose https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1080_00031305_2016_1154108&context=PC&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Primo%20Central&tab=Everything&query=,contains,ASA%20p-value&offset=0The ASA Statement \\(p\\)-values: Context, Process Purpose https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1080_00031305_2016_1154108&context=PC&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Primo%20Central&tab=Everything&query=,contains,ASA%20p-value&offset=0The garden forking paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013) http://stat.columbia.edu/~gelman/research/unpublished/forking.pdfThe garden forking paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013) http://stat.columbia.edu/~gelman/research/unpublished/forking.pdfhttps://docker-curriculum.com/ Prakhar Srivastavhttps://docker-curriculum.com/ Prakhar Srivastav","code":""},{"path":"production-explainability.html","id":"production-explainability","chapter":"15 Explainability","heading":"15 Explainability","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-explainability.html","id":"explainability--","chapter":"15 Explainability","heading":"15.1 Explainability ————————————————-","text":"LIME paper ArXiV: https://arxiv.org/abs/1602.04938. Ribeiro et al (2016) “Trust ?”: Explaining Predictions Classifier.LIME paper ArXiV: https://arxiv.org/abs/1602.04938. Ribeiro et al (2016) “Trust ?”: Explaining Predictions Classifier.LIME pacakge documentation CRAN https://cran.r-project.org/web/packages/lime/index.htmlLIME pacakge documentation CRAN https://cran.r-project.org/web/packages/lime/index.htmlUnderstanding LIME tutorial - T Pedersen M BenestyUnderstanding LIME tutorial - T Pedersen M Benestyhttps://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.htmlReference: Interpretable Machine Learning: Guide Making Black Box Models Explainable Christoph Molnar","code":""},{"path":"production-scalability.html","id":"production-scalability","chapter":"16 Scalability","heading":"16 Scalability","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-scalability.html","id":"scalability","chapter":"16 Scalability","heading":"16.1 Scalability ————————————————–","text":"Structure talk around Alex’s parallelisation slidesCode profilingCode profilingvectorisevectoriseparalleliseparalleliseUse better language: C++, PythonUse better language: C++, PythonDocumentation forDocumentation forapplyapplypurr::mappurr::mapfurr::pmapfurr::pmapAdvanced R (Second Edition) Hadley Wickham. Chapters 23 24, measuring improving performanceAdvanced R (Second Edition) Hadley Wickham. Chapters 23 24, measuring improving performanceAdvanced R (Second Edition) Hadley Wickham. Chapter 25, measuring improving performanceAdvanced R (Second Edition) Hadley Wickham. Chapter 25, measuring improving performance","code":""},{"path":"production-checklist.html","id":"production-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-checklist.html","id":"videos-chapters-3","chapter":"Checklist","heading":"16.2 Videos / Chapters","text":"[ ][ ][ ][ ][ ][ ]","code":""},{"path":"production-checklist.html","id":"reading-3","chapter":"Checklist","heading":"16.3 Reading","text":"","code":""},{"path":"production-checklist.html","id":"activities-2","chapter":"Checklist","heading":"16.4 Activities","text":"","code":""},{"path":"production-checklist.html","id":"live-session-3","chapter":"Checklist","heading":"16.5 Live Session","text":"","code":""},{"path":"ethics-introduction.html","id":"ethics-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-privacy.html","id":"ethics-privacy","chapter":"18 Privacy","heading":"18 Privacy","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-fairness.html","id":"ethics-fairness","chapter":"19 Fairness","heading":"19 Fairness","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-conduct.html","id":"ethics-conduct","chapter":"20 Codes of Conduct","heading":"20 Codes of Conduct","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-checklist.html","id":"ethics-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-checklist.html","id":"videos-chapters-4","chapter":"Checklist","heading":"20.1 Videos / Chapters","text":"[ ][ ][ ][ ][ ][ ]","code":""},{"path":"ethics-checklist.html","id":"reading-4","chapter":"Checklist","heading":"20.2 Reading","text":"","code":""},{"path":"ethics-checklist.html","id":"activities-3","chapter":"Checklist","heading":"20.3 Activities","text":"","code":""},{"path":"ethics-checklist.html","id":"live-session-4","chapter":"Checklist","heading":"20.4 Live Session","text":"","code":""},{"path":"reading-list.html","id":"reading-list","chapter":"21 Reading List","heading":"21 Reading List","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.reading list organised topic, according week course. split several categories.Core Materials: form core part course activities.Core Materials: form core part course activities.Reference Materials: used extensively course, seen helpful guides, rather required reading cover cover.Reference Materials: used extensively course, seen helpful guides, rather required reading cover cover.Materials Interest: form core part course, give deeper understanding interesting perspective weekly topic. might fun stuff .Materials Interest: form core part course, give deeper understanding interesting perspective weekly topic. might fun stuff .","code":""},{"path":"reading-list.html","id":"effective-data-science-workflows","chapter":"21 Reading List","heading":"21.1 Effective Data Science Workflows","text":"","code":""},{"path":"reading-list.html","id":"core-materials","chapter":"21 Reading List","heading":"Core Materials","text":"Tidyverse R Style Guide Hadley Wickham.Wilson, et al (2017). Good Enough Practices Scientific Computing. PLOS Computational Biology.","code":""},{"path":"reading-list.html","id":"reference-materials","chapter":"21 Reading List","heading":"Reference Materials","text":"R Data Science Chapters 2, 6 8 Hadley Wickham Garrett Grolemund. Chapters covering R workflow basics, scripting project based workflow.R Data Science Chapters 2, 6 8 Hadley Wickham Garrett Grolemund. Chapters covering R workflow basics, scripting project based workflow.Documentation {} packageDocumentation {} packageR Packages Book (Second Edition) Hadley Wickham Jenny Bryan.R Packages Book (Second Edition) Hadley Wickham Jenny Bryan.","code":""},{"path":"reading-list.html","id":"materials-of-interest","chapter":"21 Reading List","heading":"Materials of Interest","text":"STAT545, Part 1 Jennifer Bryan STAT 545 TAsWhat forgot teach R, Chapters 2-4 Jennifer Bryan Jim Hester.Broman et al (2017). Recommendations Funding Agencies Supporting Reproducible Research. American Statistical Association.Advanced R Hadley Wickham Section introductions functional object oriented approaches programming.Advanced R Hadley Wickham Section introductions functional object oriented approaches programming.Atlassian Article Agile Project ManagementAtlassian Article Agile Project ManagementThe Pragmatic Programmer, 20th Anniversary Edition Edition David Thomas Andrew Hunt. section DRY coding others freely available.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week. Chapter 4 Efficient Workflows links nicely last week’s topics.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week. Chapter 4 Efficient Workflows links nicely last week’s topics.Towards Principled Bayesian Workflow Michael Betancourt.Towards Principled Bayesian Workflow Michael Betancourt.Happy Git GitHub useR Jennifer Bryan","code":""},{"path":"reading-list.html","id":"aquiring-and-sharing-data","chapter":"21 Reading List","heading":"21.2 Aquiring and Sharing Data","text":"","code":""},{"path":"reading-list.html","id":"core-materials-1","chapter":"21 Reading List","heading":"Core Materials","text":"R Data Science Chapters 9 - 12 Hadley Wickham. chapters introduce tibbles data structure, import data R wrangle data tidy format.R Data Science Chapters 9 - 12 Hadley Wickham. chapters introduce tibbles data structure, import data R wrangle data tidy format.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week.Wickham (2014). Tidy Data. Journal Statistical Software. paper brought tidy data mainstream.Wickham (2014). Tidy Data. Journal Statistical Software. paper brought tidy data mainstream.","code":""},{"path":"reading-list.html","id":"reference-materials-1","chapter":"21 Reading List","heading":"21.2.1 Reference Materials","text":"{readr} documentationThe {readr} documentationThe {data.table} documentation vignetteThe {data.table} documentation vignetteThe {rvest} documentationThe {rvest} documentationThe {tidyr} documentationThe {tidyr} documentationMDN Web Docs HTML CSSMDN Web Docs HTML CSS","code":""},{"path":"reading-list.html","id":"data---materials-of-interest","chapter":"21 Reading List","heading":"21.2.2 Data - Materials of Interest","text":"Introduction APIs Brian CookseyR Data Science (Second Edition) Chapters within Import section.covers importing data spreadsheets, databases, using Apache Arrow importing hierarchical data well web scraping.","code":""},{"path":"reading-list.html","id":"week-3-data-exploration-and-visualisation","chapter":"21 Reading List","heading":"21.3 Week 3: Data Exploration and Visualisation","text":"","code":""},{"path":"reading-list.html","id":"core-materials-2","chapter":"21 Reading List","heading":"21.3.1 Core Materials","text":"Exploratory Data Analysis R Roger Peng.Chapters 3 4 core reading, respectively introducing data frame manipulation {dplyr} example workflow exploratory data analysis. chapters may useful references.Flexible Imputation Missing Data Stef van Buuren. Sections 1.1-1.4 give thorough introduction missing data problems.","code":""},{"path":"reading-list.html","id":"referene-materials","chapter":"21 Reading List","heading":"Referene Materials","text":"ggplot2 Tutorial Beautiful Plotting R https://www.cedricscherer.com/2019/08/05/-ggplot2-tutorial--beautiful-plotting--r/) Cédric Scherer.ggplot2 Tutorial Beautiful Plotting R https://www.cedricscherer.com/2019/08/05/-ggplot2-tutorial--beautiful-plotting--r/) Cédric Scherer.{dplyr} documentationThe {dplyr} documentationRStudio Data Transformation Cheat SheetRStudio Data Transformation Cheat SheetR Data Science (First Edition) Chapters Data Transformations, Exploratory Data Analysis Relational Data.R Data Science (First Edition) Chapters Data Transformations, Exploratory Data Analysis Relational Data.Equivalent sections R Data Science Second EditionEquivalent sections R Data Science Second Edition","code":""},{"path":"reading-list.html","id":"materials-of-interest-1","chapter":"21 Reading List","heading":"Materials of Interest","text":"Wickham, H. (2010). Layered Grammar Graphics. Journal Computational Graphical Statistics.Wickham, H. (2010). Layered Grammar Graphics. Journal Computational Graphical Statistics.Better Data Visualisations Jonathan SchwabishBetter Data Visualisations Jonathan SchwabishData Visualization: Practical Introduction Kieran Healy","code":""},{"path":"reading-list.html","id":"preparing-for-production","chapter":"21 Reading List","heading":"21.4 Preparing for Production","text":"","code":""},{"path":"reading-list.html","id":"core-materials-3","chapter":"21 Reading List","heading":"Core Materials","text":"Ethical Algorithm M Kearns Roth (Chapter 4)Ethical Algorithm M Kearns Roth (Chapter 4)Ribeiro et al (2016). “Trust ?”: Explaining Predictions Classifier.Ribeiro et al (2016). “Trust ?”: Explaining Predictions Classifier.","code":""},{"path":"reading-list.html","id":"reference-materials-2","chapter":"21 Reading List","heading":"Reference Materials","text":"Docker Curriculum Prakhar Srivastav.Docker Curriculum Prakhar Srivastav.LIME package documentation CRAN.LIME package documentation CRAN.Interpretable Machine Learning: Guide Making Black Box Models Explainable Christoph Molnar.Interpretable Machine Learning: Guide Making Black Box Models Explainable Christoph Molnar.Documentation apply(), map() pmap()Documentation apply(), map() pmap()Advanced R (Second Edition) Hadley Wickham. Chapter 23 measuring performance Chapter 24 improving performance.Advanced R (Second Edition) Hadley Wickham. Chapter 23 measuring performance Chapter 24 improving performance.","code":""},{"path":"reading-list.html","id":"materials-of-interest-2","chapter":"21 Reading List","heading":"Materials of Interest","text":"ASA Statement \\(p\\)-values: Context, Process PurposeThe ASA Statement \\(p\\)-values: Context, Process PurposeThe Garden Forking Paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013)Garden Forking Paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013)Understanding LIME tutorial T Pedersen M Benesty.Understanding LIME tutorial T Pedersen M Benesty.Advanced R (Second Edition) Hadley Wickham. Chapter 25 writing R code C++.Advanced R (Second Edition) Hadley Wickham. Chapter 25 writing R code C++.","code":""},{"path":"reading-list.html","id":"data-science-ethics","chapter":"21 Reading List","heading":"21.5 Data Science Ethics","text":"","code":""},{"path":"reading-list.html","id":"core-materials-4","chapter":"21 Reading List","heading":"Core Materials","text":"Ethical Algorithm M Kearns Roth. Chapters 1 2 Algorithmic Privacy Algortihmic Fairness.Ethical Algorithm M Kearns Roth. Chapters 1 2 Algorithmic Privacy Algortihmic Fairness.Gender Shades: Intersectional Accuracy Disparities Commercial Gender Classification Joy Buolamwini Timnit Gebru (2018). Proceedings 1st Conference Fairness, Accountability Transparency.Gender Shades: Intersectional Accuracy Disparities Commercial Gender Classification Joy Buolamwini Timnit Gebru (2018). Proceedings 1st Conference Fairness, Accountability Transparency.Robust De-anonymization Large Sparse Datasets Arvind Narayanan Vitaly Shmatikov (2008). IEEE Symposium Security Privacy.Robust De-anonymization Large Sparse Datasets Arvind Narayanan Vitaly Shmatikov (2008). IEEE Symposium Security Privacy.","code":""},{"path":"reading-list.html","id":"reference-materials-3","chapter":"21 Reading List","heading":"Reference Materials","text":"Fairness machine learning\nLimitations Opportunities Solon Barocas, Moritz Hardt Arvind Narayanan.Fairness machine learning\nLimitations Opportunities Solon Barocas, Moritz Hardt Arvind Narayanan.Professional Guidleines Data Ethics :\nAmerican Mathematical Society\nEuropean Union\nUK Government\nRoyal Statistical Society\nDutch Government\nProfessional Guidleines Data Ethics :American Mathematical SocietyThe European UnionUK GovernmentRoyal Statistical SocietyDutch Government","code":""},{"path":"reading-list.html","id":"materials-of-interest-3","chapter":"21 Reading List","heading":"Materials of Interest","text":"Algorithmic Fairness (2020). Pre-print review paper Dana Pessach Erez Shmueli.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
