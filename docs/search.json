[{"path":"index.html","id":"about-this-course","chapter":"About this Course","heading":"About this Course","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"index.html","id":"course-description","chapter":"About this Course","heading":"Course Description","text":"Model building evaluation necessary sufficient skills effective practice data science. module develop technical personal skills required work successfully data scientist within organisation.module critically explore :effectively scope manage data science project;work openly reproducibly;efficiently acquire, manipulate, present data;interpret explain work variety stakeholders;ensure work can put production;assess ethical implications work data scientist.interdisciplinary course draw fields including statistics, computing, management science data ethics. topic investigated selection lecture videos, conference presentations academic papers, hands-lab exercises, readings industry best-practices recognised professional bodies.","code":""},{"path":"index.html","id":"schedule","chapter":"About this Course","heading":"Schedule","text":"notes intended students course MATH70076: Data Science academic year 2022/23.course scheduled take place five weeks, suggested schedule :1st week: effective data science workflows;2nd week: aquiring sharing data;3rd week: exploratory data analysis visualisation;4th week: preparing production;5th week: ethics context data science.pdf version notes may downloaded .","code":""},{"path":"index.html","id":"learning-outcomes","chapter":"About this Course","heading":"Learning outcomes","text":"successful completion module students able :Independently scope manage data science project;Source data internet web scraping APIs;Clean, explore visualise data, justifying documenting decisions made;Evaluate need (implement) approaches explainable, reproducible scalable;Appraise ethical implications data science projects, particularly risks compromising privacy fairness potential cause harm.","code":""},{"path":"index.html","id":"allocation-of-study-hours","chapter":"About this Course","heading":"Allocation of Study Hours","text":"Lectures: 10 Hours (2 hours per week)Group Teaching: 5 Hours (1 hour per week)Lab / Practical: 5 hours (1 hour per week)Independent Study: 105 hours (17 hours per week + 20 hours coursework)","code":""},{"path":"index.html","id":"assessment-structure","chapter":"About this Course","heading":"Assessment Structure","text":"course assessed entirely coursework, reflecting practical pragmatic nature course material.Coursework 1 (30%): completed fourth week course.Coursework 2 (70%): released last week course submitted following examination period Summer term.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"About this Course","heading":"Acknowledgements","text":"notes created Dr Zak Varty. inspired previous lecture series Dr Purvasha Chakravarti Imperial College London draw many resource made available R community.","code":""},{"path":"workflows-introduction.html","id":"workflows-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.data scientist never work alone.Within single project data scientist likely interact range people, including limited : one project managers, stakeholders subject matter experts. experts might come single specialism form multidisciplinary team, depending type work .get project put use working scale likely collaborate data engineers. also work closely data scientists, review one another’s work collaborate larger projects.Familiarity skills, processes practices make collaboration instrumental successful data scientist. aim part course provide structure organise perform work, can good collaborator current colleges future self.going require bit effort upfront, benefits compound time. get done wasting less time staring quizzically messy folders indecipherable code. also gain reputation someone good work . promotes better professional relationships greater levels trust, can turn lead working exciting impactful projects.","code":""},{"path":"workflows-organising-your-work.html","id":"workflows-organising-your-work","chapter":"2 Organising your work","heading":"2 Organising your work","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.Welcome course effective data science. week ’ll considering effective data science workflows. workflows ways progressing project help produce high quality work help make good collaborator.Chapter, ’ll kick things looking can structure data\nscience projects organize work. Familiarity skills, processes\npractices collaborative working going instrumental \nbecome successful data scientist.","code":""},{"path":"workflows-organising-your-work.html","id":"what-are-we-trying-to-do","chapter":"2 Organising your work","heading":"2.1 What are we trying to do?","text":"First, let’s consider want provide data science projects sense structure organization.data scientist ’ll never work alone. Within single project ’ll interact whole range people. might project manager, one business stakeholders variety subject matter experts.experts might trained sociologists, chemists, civil servants depending exact type data science work ’re .get project put use working scale ’ll \ncollaborate data engineers. ’ll also likely work closely data scientists. smaller projects might act reviewers one another’s work. larger projects working collaboratively allow tackle larger challenges. sorts project wouldn’t feasible alone, inherent limitations time skill one individual person.Even work small organization, ’re data scientist, adopting way working ’s focused collaborating pay dividends time. inevitably return project ’re working several weeks months years future ’ll forgotten almost everything first time around. ’ll also forgotten made decisions potential options didn’t take.exactly like working current colleague shoddy poor working practices. Nobody wants colleague somebody else, let alone future self. Even working alone, treating future self current collaborator (one want get along well ) makes kind colleague pleasure work .aim week provide guiding structure organize perform work. None going particularly difficult onerous. However require bit effort front daily discipline. Like flossing, daily effort required large benefits compound time.’ll get done wasting less time staring quizzically mess \nfolders indecipherable code. ’ll also get reputation someone ’s well organized good work . promotes better professional relationships greater levels trust within team. can , turn, tead working exciting impactful projects future.","code":""},{"path":"workflows-organising-your-work.html","id":"an-r-focused-approach","chapter":"2 Organising your work","heading":"2.2 An R Focused Approach","text":"structures workflows recommend throughout rest module focused strongly workflow predominantly uses R, markdown LaTeX.Similar techniques, code software can achieve results show coding Python C, writing projects Quarto markup language. Similarly, different organizations variations best practices ’ll go together. Often organisations extensive guidance topics.important thing understand good habits build one programming language business, transferring skills new setting largely matter learning new vocabulary slightly different syntax.said, let’s get going!","code":""},{"path":"workflows-organising-your-work.html","id":"one-project-one-directory","chapter":"2 Organising your work","heading":"2.3 One Project = One Directory","text":"’s one thing take away chapter, ’s one\nGolden Rule:Every individual project work data scientist single, self-contained directory folder.worth repeating. Every single project work self-contained live single directory. analogy might separate ring-binder folder modules degree program.one golden rule deceptively simple.first issue requires predetermined scope \nisn’t going covered particular project. seems straightforward outset project often know exactly project go, link pieces work within organization.second issue second law Thermodynamics, applies equally well project management heatdeath universe. takes continual external effort prevent contents one folder becoming chaotic disordered time.said, single directory several benefits justify additional work.","code":""},{"path":"workflows-organising-your-work.html","id":"properties-of-a-well-orgainsed-project","chapter":"2 Organising your work","heading":"2.4 Properties of a Well-Orgainsed Project","text":"properties like single, well-organized project ? Ideally, ’d like organize projects following\nproperties:PortableVersion Control FriendlyReproducibleIDE friendly.Don’t worry haven’t heard terms already. ’re going look little bit detail.","code":""},{"path":"workflows-organising-your-work.html","id":"portability","chapter":"2 Organising your work","heading":"2.4.1 Portability","text":"project said portable can easily moved without breaking.might small move, like relocating directory different location computer. might also mean moderate move, say another machine dies just big deadline. Alternatively, might large shift - uses another person using different operating system.thought experiment can see ’s full spectrum portable project may may need .","code":""},{"path":"workflows-organising-your-work.html","id":"version-control-friendly","chapter":"2 Organising your work","heading":"2.4.2 Version Control Friendly","text":"project Version Control changes tracked either manually automatically. means snapshots project taken regularly gradually develops evolves time. snapshots many, incremental changes made project allow rolled back specific previous state something goes wrong.version controlled pattern working helps avoid horrendous state found - renaming final_version.doc final_final_version.doc .organising workflow around incremental changes helps acknowledge work ever finally complete. always small changes need done future.","code":""},{"path":"workflows-organising-your-work.html","id":"reproducibility","chapter":"2 Organising your work","heading":"2.4.3 Reproducibility","text":"study reproducible can take original data computer code used analyze data recreate numerical findings study.\nBroman et al (2017). “Recommendations Funding Agencies Supporting Reproducible Research”paper, Broman et al define reproducibility project can take original data code used perform analysis using create numerical findings study.definition leads naturally several follow-questions.exactly definition? specifically mean future someone else access data code able recreate findings ? Also, reproducibility limited just numerical results? also able create associated figures, reports press releases?Another important question project needs reproduced. weeks time 10 years time? need protect project changes dependencies, like new versions packages modules? different versions R Python? Taking time scale even , different operating systems hardware?’s unlikely ’d consider someone handing floppy disk code runs Windows XP acceptably reproducible. Sure, probably find way get work, awful lot effort end.’s perhaps bit extreme example, emphasizes importance clearly defining level reproducibility ’re aiming within every project work . example also highlights amount work can required reproduce analysis, especially quite time. important explicitly think dividing effort original developer person trying reproduce analysis future.","code":""},{"path":"workflows-organising-your-work.html","id":"ide-friendly","chapter":"2 Organising your work","heading":"2.4.4 IDE Friendly","text":"final desirable property ’d like projects play nicely\nintegrated development environments.’re coding document writing data science projects ’d possible work entirely either plain text editor typing code directly command line. approaches data science workflow benefit simplicity, also expect great deal data scientist.workflows expect type everything perfectly accurately every time, recall names argument orders every function use, constantly aware current state objects within working environment.Integrated Development Environments (IDEs) applications help reduce burden, helping make effective programmer data scientist. IDEs offer tools like code completion highlighting make code easier read write. offer tools debugging, fix things going wrong, also offer environment panes don’t hold everything head . Many IDEs also often templating facilities. let save reuse snippets code can avoid typing repetitive, boilerplate code introducing errors process.Even haven’t heard IDEs , ’ve likely already used one. common examples might RStudio R-users, PyCharm python users, Visual Studio language agnostic coding environment.Whichever use, ’d like project play nicely . lets us reap benefits keeping project portable, version controlled, reproducible someone working different set-.","code":""},{"path":"workflows-organising-your-work.html","id":"project-structure","chapter":"2 Organising your work","heading":"2.5 Project Structure","text":"’ve given pretty exhaustive argument single directory project good idea. Let’s now take look inside directory define common starting layout content projects.sort project directory template mean ’ll always know find ’re looking members team . , start ’ll reiterate ’re taking opinionated approach providing sensible starting point organizing many projects.Every project going slightly different might require slight alterations suggest . Indeed, even start suggest might adapt project structure develops grows. think ’s helpful consider tailor making changes. ’m providing one size fits design, ’s great lots projects perfect none . ’s job alter refine design individual case.One final caveat get started: companies businesses many times house style write organize code projects. ’s case, follow style guide business company uses. important thing consistent individual level across entire data science team. ’s consistency reaps benefits.Okay, imagine now ’ve assigned shiny new project created single directory house project. ’ve, quite imaginatively, called directory exciting-new-project. populate folder ?rest video, ’ll define house-style organizing root directory data science projects module.Within project directory subdirectories, can tell folders file structure forward slash following names. also files directly root directory. One called readme.md another called either makefile make.r. ’re going explore files directories turn.","code":""},{"path":"workflows-organising-your-work.html","id":"readme.md","chapter":"2 Organising your work","heading":"2.5.1 README.md","text":"Let’s begin readme file. gives brief introduction project gives information project aims . readme file describe get started using project contribute development.readme written either plain text format readme.txt markdown\nformat readme.md. benefit using markdown allows light\nformatting sections headers lists using plain text characters.\ncan see using hashes mark first second level headers using bullet points unnumbered list. Whichever format use, readme file project always stored root directory typically named uppercase letters.readme file first thing someone ’s new project reads. placing readme root directory capitalising file name increase visibility file increae chances actually happening.additional benefit keeping readme root directory project code hosting services like GitHub, GitLab BitBucket display contents readme file next contents project. services also nicely format markdown use readme file.writing readme, can useful imaginge writing new, junior team member. readme file let get started project make simple contributions reading file. might also link detailed project documentation help new team member toward advanced understanding complex contribution.","code":""},{"path":"workflows-organising-your-work.html","id":"inside-the-readme","chapter":"2 Organising your work","heading":"2.5.2 Inside the README","text":"let’s take quick aside see detail covered within readme file.readme include name project, self-explanatory (nothing like generic choice exciting-new-project). readme also give project status, just couple sentences say whether project still development, version oft current release , end project life-cycle, project deprecated \nclosed.Following , also include description project. state purpose work provide, link , additional context references visitors aren’t assumed familiar .project involves code depends packages give instruction install dependencies run code. might just text also include things like screenshots, code snippets, gifs video whole process.’s also good practice include simple examples use code within project expected results, new users can confirm everything working local instance. Keep examples simple minimal can new usersFor longer complicated examples aren’t necessary short introductory document can add links readme explain \ndetail elsewhere.ideally short description people can report issues project also people can get started resolving issues extend \nproject way.leads one point ’ve forgotten list . section listing authors work license ’s\ndistributed. give credit people ’ve contributed \nproject license file says people may use work. license declates may use project whether give direct\nattribution work modifications use.","code":""},{"path":"workflows-organising-your-work.html","id":"data","chapter":"2 Organising your work","heading":"2.5.3 data","text":"Moving back project structure, next data directory.data directory two subdirectories one called raw one called derived. data generate part project stored raw subdirectory. ensure project reproducible, data Raw folder never edited modified.example ’ve got two different data types: Excel spreadsheet XLS file JSON file. files exacty received project stakeholder.text file metadata.txt plain text file explaining contents interpretation raw data sets. metadata include descriptions measured variables, units recorded , date file created acquired, source obtained.raw data likely isn’t going form ’s amenable analyzing straight away. get data pleasant form work, require data manipulation cleaning. manipulation cleaning applied well documented resulting cleaned files saved within derived data directory.exciting new project, can see clean versions previous data sets ready modelling. ’s also third file folder. data ’ve aquired web scraping, using script within project.","code":""},{"path":"workflows-organising-your-work.html","id":"src","chapter":"2 Organising your work","heading":"2.5.4 src","text":"src source directory contains source code project. typically functions ’ve written make analysis modelling code accessible.’ve saved function R script , project, ’ve used subdirectories organise use case. ’ve got two functions used data cleaning: first replaces NA values given value, second replaces mean non-missing values.also three helper functions: first two calculate rolling mean \ngeometric mean given vector, third function scrapes \nweb data saw derived data subdirectory.","code":""},{"path":"workflows-organising-your-work.html","id":"tests","chapter":"2 Organising your work","heading":"2.5.5 tests","text":"Moving tests directory. structure directory mirrors source directory. function file counterpart file tests.test files provide example sets inputs expected outputs \nfunction. test files used check edge cases function assure\nhaven’t broken anything fixing small bug adding new capabilities function.","code":""},{"path":"workflows-organising-your-work.html","id":"analyses","chapter":"2 Organising your work","heading":"2.5.6 analyses","text":"analyses directory contains probably think bulk \ndata science work. ’s going one subdirectory major analysis ’s performed within project within might series steps collect separate scripts.activity performed step made clear name script, order ’re going perform steps. can see scripts used 2021 annual report. First script used take raw monthly receipts produce cleaned version data set saw earlier.\nfollowed trend analysis cleaned data set.Similarly spending review data cleaning step, followed forecast modelling finally production diagnostic plots compare \nforecasts.","code":""},{"path":"workflows-organising-your-work.html","id":"outputs","chapter":"2 Organising your work","heading":"2.5.7 outputs","text":"outputs directory one subdirectory meta-analysis within project. organized output type whether data, figure, table.Depending nature project, might want use modified subdirectory structure . example, ’re several numerical experiments might want arrange outputs experiment, rather output type.","code":""},{"path":"workflows-organising-your-work.html","id":"reports","chapter":"2 Organising your work","heading":"2.5.8 reports","text":"reports directory everything comes together. written documents form final deliverables project created. final documents written LaTeX markdown, source \ncompiled documents can found within directory.including content report, example figures, ’d recommend \nmaking copies figure files within reports directory. , ’ll manually update files every time modify . Instead can use relative file paths include figures. Relative file paths specify get image, starting TeX document moving levels project directory.’re using markdown LaTeX write reports, instead use online platform like overleaf latex editor Google docs write collaboratively links reports directory using additional readme files. Make sure set read write permissions links appropriately, .using online writing systems, ’ll manually upload update plots whenever modify earlier analysis. ’s one drawbacks online tools traded ease use.exciting new project, can see annual report written markdown format, compiled HTML PDF. spendiing review written LaTeX source , don’t compiled pdf version document.","code":""},{"path":"workflows-organising-your-work.html","id":"make-file","chapter":"2 Organising your work","heading":"2.5.9 make file","text":"final element template project structure make file. aren’t going cover read write make files course. Instead, ’ll give brief description supposed .high level, make file just text file. makes special contains. Similar shell bash script, make file contains code run command line. code create update element project.make file defines shorthand commands full lines code create element project. make file also records order operations happen, steps dependent one another. means one step part project updated changes propagated entire project. done quite clever way part projects re-run need updated.’re omitting make files course ’re fiendishly difficult write read, rather require reasonable foundation working command line understood. suggest instead throughout course create R markdown file called make. file define intended running order dependencies project R file, might also automate parts analysis.","code":""},{"path":"workflows-organising-your-work.html","id":"wrapping-up","chapter":"2 Organising your work","heading":"2.6 Wrapping up","text":"Wrapping , ’s everything chapter.’ve introduced project structure serve well baseline vast majority projects data science.work, remember key standardisation. Working consistently across projects, company group important sticking rigidly particular structure defined .two notable exceptions probably don’t want use project structure. ’s ’re building app ’re building package. require specific organisation files within project directory. ’ll\nexplore project structure used package development live\nsession week.","code":""},{"path":"workflows-naming.html","id":"workflows-naming","chapter":"3 Naming Files","heading":"3 Naming Files","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-naming.html","id":"introduction","chapter":"3 Naming Files","heading":"3.1 Introduction","text":"“two hard things Computer Science: cache invalidation naming things.”Phil Karlton, Netscape DeveloperWhen working data science project can principle name directories, files, functions objects whatever like. reality though, using ad-hoc system naming likely cause confusion, headaches mistakes. obviously want avoid things, spirit kind current colleges also future selves.Coming good names art form. Like art, naming things activity get better practice. Another similarity best naming systems don’t come giving data scientists free reign naming system. Like art, best approaches naming things give strong guidelines boundaries within express creativity skill.lecture ’ll explore boundaries want achieve us. content lecture based largely around talk name given Jennifer Bryan tidyverse style guide, forms basis Google’s style guide R programming.","code":""},{"path":"workflows-naming.html","id":"naming-files","chapter":"3 Naming Files","heading":"3.2 Naming Files","text":"’ll begin focusing call files. , ’ll first focus part file name comes dot. second part video, ’ll cycle back around discuss file extensions.","code":""},{"path":"workflows-naming.html","id":"what-do-we-want-from-our-file-names","chapter":"3 Naming Files","heading":"3.2.1 What do we want from our file names?","text":"dive naming files, first consider want file names choose. three key properties like satisfy.Machine ReadableHuman ReadableOrder FriendlyThee first desirable property file names easily readable computers, second file names easily readable humans finally file names take advantage default ordering imposed files.set current file names sorely lacking across properties:want provide naming conventions move us toward better file names listed .Let’s take minutes examine exactly mean properties.","code":"abstract.docx\nEffective Data Science's module guide 2022.docx \nfig 12.png\nRplot7.png\n1711.05189.pdf\nHR Protocols 2015 FINAL (Nov 2015).pdf2015-10-22_human-resources-protocols.pdf\n2022_effective-data-science-module-guide.docx\n2022_RSS-conference-abstract.docx \nfig12_earthquake-timeseries.png \nfig07_earthquake-location-map.png\nogata_1984_spacetime-clustering.pdf"},{"path":"workflows-naming.html","id":"machine-readable","chapter":"3 Naming Files","heading":"3.2.2 Machine Readable","text":"mean machine readable file names?Easy compute deliberate use delimiters:\nunderscores_separate_metadata, hyphens-separate-words.\nunderscores_separate_metadata, hyphens-separate-words.Play nicely regular expressions globbing:\navoid spaces, punctuation, accents, cases;\nrm Rplot*.png\navoid spaces, punctuation, accents, cases;rm Rplot*.pngMachine readable names useful :managing files: ordering, finding, moving, deleting:managing files: ordering, finding, moving, deleting:extracting information directly file names,extracting information directly file names,working programmatically file names regex.working programmatically file names regex.operating large number files useful able work programmatically.One example might useful downloading assessments marking. might require unzip large number zip files, copying pdf report unzipped folder single directory R scripts unzipped folder another directory. marked scripts code need paired back folders named student, re-zipped ready returned.monotonously dull might work ~50 students ~5000. Working programmatically files way get job done efficiently. requires file names play nicely way computers interpret file names, regard string characters.often helpful metadata included file name, example student’s id number assessment title. use underscore separate elements metadata within file name hyphen separate sub-elements meta-data, example words within assessment title.Regular expressions globbing two ideas string manipulation may met, inform naming conventions. Regular expressions allow search strings (case file names) match particular pattern. Regular expressions can really complicated searches become gnarly worry special characters like spaces, punctuation, accents cases, avoided file names.special type regular expression called globbing star used replace number subsequent characters file name, can delete png images begin Rplot using single line code. Globbing becomes particular powerful use consistent structure create file names.assessment marking example, machine readable file names particularly useful managing files, ordering, finding, moving deleting . Another example analysis requires load large number individual data files.Machine readable file names also useful extracting meta-information files without open memory. particularly useful files might large load memory, want load data certain year.final benefit list scalability, reduction drudgery lowered risk human error operating large number files.","code":""},{"path":"workflows-naming.html","id":"order-friendly","chapter":"3 Naming Files","heading":"3.2.3 Order Friendly","text":"next property focus also links computers operate. ’d like file names exploit default orderings used computers. means starting file names character strings metadata allow us order files meaningful way.","code":""},{"path":"workflows-naming.html","id":"running-order","chapter":"3 Naming Files","heading":"3.2.3.1 Running Order","text":"One example ’s logical order code executed, example analysis .Prepreding numbers file names can make intended ordering immediately obvious.Starting single digit numbers leading 0 good idea prevent script 1 sorted tens, script 2 twenties . might 100 files, example saving output many simulations, use two zeros maintain nice ordering.","code":"diagnositc-plots.R\ndownload.R\nruntime-comparison.R\n...\nmodel-evaluation.R\nwrangle.R00_download.R\n01_wrangle.R\n02_model.R\n...\n09_model-evaluation.R\n10_model-comparison-plots.R"},{"path":"workflows-naming.html","id":"date-order","chapter":"3 Naming Files","heading":"3.2.3.2 Date Order","text":"second example orderable file names file date associated . might version report date data recorded, cleaned updated.using dates, file names elsewhere, conform ISO standard date format.ISO 8601 sets international standard format dates: YYYY-MM-DD.format uses four numbers year, followed two numbers month two numbers day month. structure mirrors nested file structure moving least specific. also avoids confusion ordering date elements. Without using ISO standard date like 04-05-22 might interpreted fourth May 2022, fifth April 2022, 22nd May 2004.","code":"2015-10-22_human-resources-protocols.pdf\n...\n2022-effective-data-science-module-guide.docx"},{"path":"workflows-naming.html","id":"human-readable","chapter":"3 Naming Files","heading":"3.2.4 Human Readable","text":"final property like file names human readability. requires names files meaningful, informative easily read real people.first two handled including appropriate metadata file name. ease read real people determined length file name name formatted.lots formatting options fun names like camelCase, PascalCase, snake_case.’s weak evidence suggests snake skewer cases readable. ’ll use mixture , using snake case metadata items skewer case within . slight cost legibility, trade-making computing file names easier.final aspect control length name. short, evocative useful file names easy skill . hints tips might want look tips writing URL slugs. last part web address intended improve accessibility immediately intuitively meaningful user.","code":"   easilyReadByRealPeople (camelCase)\n   EasilyReadByRealPeople (PascalCase)\n   easily_read_by_real_people (snake_case)\n   easily-read-by-real-people (skewer-case)"},{"path":"workflows-naming.html","id":"naming-files---summary","chapter":"3 Naming Files","heading":"3.2.5 Naming Files - Summary","text":"File names meaningful, informative scripts end .rFile names meaningful, informative scripts end .rStick letters, numbers underscores (_) hyphens (-).Stick letters, numbers underscores (_) hyphens (-).Pay attention capitalisation file.r \\(\\neq\\) File.r operating systems.Pay attention capitalisation file.r \\(\\neq\\) File.r operating systems.Show order left-padded numbers ISO dates.Show order left-padded numbers ISO dates.","code":""},{"path":"workflows-naming.html","id":"file-extensions-and-where-you-work","chapter":"3 Naming Files","heading":"3.3 File Extensions and Where You Work","text":"far focused entirely comes dot, file name.Equally, , important comes dot, file extension.file extension describes information stored file determines software can use, view run file.likely already use file extensions distinguish code scripts, written documents, images, notebook files. ’ll now explore benefits drawbacks various file types respect several important features.","code":"example-script.r\nexample-script.py\n\nproject-writeup.doc\nproject-writeup.tex"},{"path":"workflows-naming.html","id":"open-source-vs-proprietary-file-types","chapter":"3 Naming Files","heading":"3.3.1 Open Source vs Proprietary File Types","text":"first feature ’ll consider whether file type open source, can used anyone without charge, specialist software must paid order interact files.figure , column represents different class file, moving left right example file types tabular data, list-like data text documents. File types closer top open source lower rely proprietary software, may may require payment.make sure work accessible many people possible favour open source options like csv files Google sheets excel, JSON files Matlab data files, tex markdown word Google doc.usually benefit terms project longevity scalability. open source file types often somewhat simpler structure, making robust changes time less memory intensive.see , let’s take look inside data files.","code":""},{"path":"workflows-naming.html","id":"inside-data-files","chapter":"3 Naming Files","heading":"3.3.2 Inside Data Files","text":"","code":""},{"path":"workflows-naming.html","id":"inside-a-csv-file","chapter":"3 Naming Files","heading":"3.3.2.1 Inside a CSV file","text":"CSV comma separated value files used store tabular data.tabular data, row data represents one record column represents data value.csv encodes record separate line using commas separate values record. can see opening csv file text editor notepad.raw data stores line breaks using \\n indicates new rows \\r. backslashed indicae escape characters special meanings, literally interpreted letters n r.viewed text editor, example file look something like .","code":"\nlibrary(readr)\nread_file(file = \"images/102-workflows-naming-files/example.csv\")\n#> [1] \"Name,Number\\r\\nA,1\\r\\nB,2\\r\\nC,3\"Name,Number \nA,1\nB,2\nC,3"},{"path":"workflows-naming.html","id":"inside-a-tsv-file","chapter":"3 Naming Files","heading":"3.3.2.2 Inside a TSV file","text":"TSV tab separated value files also used store tabular data.Like csv record given new line tsv tabs rather commas used separate values record. can also seen opening tsv file text editor notepad.One thing note tabs separate character just multiple spaces. plain text can impossible tell apart, text editors option display tabs differently repeated spaces, though usually enabled default.","code":"\nlibrary(readr)\nread_file(file = \"images/102-workflows-naming-files/example.tsv\")\n#> [1] \"Name\\tNumber\\r\\nA\\t1\\r\\nB\\t2\\r\\nC\\t3\"Name    Number \nA   1\nB   2\nC   3"},{"path":"workflows-naming.html","id":"inside-an-excel-file","chapter":"3 Naming Files","heading":"3.3.2.3 Inside an Excel file","text":"open excel file text editor, immediately see human interpretable file format.entry four digit hexadecimal number lot entries small table.excel files can carry lot additional information csv tsv able , example cell formatting multiple tables (called sheets excel) stored within single file.means excel files take much memory carrying lot information strictly contained within data .","code":"504b 0304 1400 0600 0800 0000 2100 62ee\n9d68 5e01 0000 9004 0000 1300 0802 5b43\n6f6e 7465 6e74 5f54 7970 6573 5d2e 786d\n6c20 a204 0228 a000 0200 0000 0000 0000\n0000 0000 0000 0000 0000 0000 0000 0000\n.... .... .... .... .... .... .... ....\n0000 0000 0000 0000 ac92 4d4f c330 0c86\nef48 fc87 c8f7 d5dd 9010 424b 7741 48bb\n2154 7e80 49dc 0fb5 8da3 241b ddbf 271c\n1054 1a83 0347 7fbd 7efc cadb dd3c 8dea\n.... .... .... .... .... .... .... ...."},{"path":"workflows-naming.html","id":"indise-a-json-file","chapter":"3 Naming Files","heading":"3.3.2.4 Indise a JSON file","text":"JSON, Java Script Object Notation, files open source format list-like data. record represented collection key:value pairs. example table entry two fields, one corresponding Name key one corresponding Number key.list-like structure allows non-tabular data stored using property called nesting: value taken key can single value, vector values another list-like object.ability create nested data structures lead data format used widely range applications require data transfer.","code":"[{\n    \"Name\": \"A\",\n    \"Number\": \"1\"\n}, {\n    \"Name\": \"B\",\n    \"Number\": \"2\"\n}, {\n    \"Name\": \"C\",\n    \"Number\": \"3\"\n}]"},{"path":"workflows-naming.html","id":"inside-an-xml-file","chapter":"3 Naming Files","heading":"3.3.2.5 Inside an XML file","text":"XML files another open source format list-like data, record represented collection key:value pairs.difference JSON file mainly records formatted within file. JSON file designed look like objects Java Script programming language XML formatting done look like html, markup language used write websites.","code":"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<root>\n  <row>\n    <Name>A<\/Name>\n    <Number>1<\/Number>\n  <\/row>\n  <row>\n    <Name>B<\/Name>\n    <Number>2<\/Number>\n  <\/row>\n  <row>\n    <Name>C<\/Name>\n    <Number>3<\/Number>\n  <\/row>\n<\/root>"},{"path":"workflows-naming.html","id":"a-note-on-notebooks","chapter":"3 Naming Files","heading":"3.3.3 A Note on Notebooks","text":"two half notebook formats likely use : .rmd (alternatively .qmd) .ipynb.two half notebook formats likely use : .rmd (alternatively .qmd) .ipynb.R markdown documents .rmd plain text files, human friendly.R markdown documents .rmd plain text files, human friendly.JuPyteR notebooks multi-language support human friendly (JSON disguise).JuPyteR notebooks multi-language support human friendly (JSON disguise).Quarto documents offer best worlds extensive language support. yet established format.Quarto documents offer best worlds extensive language support. yet established format.addition files read write, files code largely determine workflow.three main options way code: first typing directly command line, second using text editor IDE write scripts third writing notebook mixes code. text output together single document.’ll compare methods working next slide, first let’s quick review notebooks available might want use .data scientist, two half notebook formats ’re likely met . first two Rmarkdown files working predominantly R interactive python jupyter notebooks working predominantly python. final half format quarto markdown documents, relatively new extend functionality Rmarkdown files.main benefit R markdown documents ’re plain text files, ’re human friendly. JuPyteR notebooks benefit supporting code written Julia, Python R, human friendly - hood documents JSON files edited directly misplaced bracket break .Quarto documents offer best worlds, plain text formatting even extensive language support jupyter notebooks. Quarto recent extension Rmarkdown, rapidly becoming popular data science community.format benefits drawbacks depending context used shared benefits limitations nature notebook documents.","code":""},{"path":"workflows-naming.html","id":"file-extensions-and-where-you-code","chapter":"3 Naming Files","heading":"3.3.4 File Extensions and Where You Code","text":"main benefit notebook documents self-documenting, can mix documentation, code report single document. Notebooks also provide level interactivity coding possible working directly command line using text editor write scripts. second factor easily overcome using integrated development environment scripting.Writing code .r files self-documenting separation code, documentation outputs many benefits. Firstly, resulting scripts provide reproducible automatable workflow, unlike one-lines code run command line. Secondly, using IDE write provides syntax highlighting code linting features help write readable accurate code. Finally, separation code documentation output allows work easily even directly put production.course advocate scripting-first approach data science, though notebooks command line work definitely place.Notebooks great teaching rapid development tools strong limitations put production. Conversely, coding directly command line can leave trace workflow lead analysis replicated future.","code":""},{"path":"workflows-naming.html","id":"summary","chapter":"3 Naming Files","heading":"3.3.5 Summary","text":"Finally, let’s wrap things summarising learned naming files.dot want pick file names machine readable, human friendly play nicely default orderings provided us.Name files :Machine Readable,Human Readable,Order Friendly.dot, want pick file types widely accessible, easily read humans allow entire analysis reproduced.Use document types :Widely accessible,Easy read reproduce,Appropriate task hand.want name files pick file types best match team working task hand.","code":""},{"path":"workflows-code.html","id":"workflows-code","chapter":"4 Code","heading":"4 Code","text":"Effective Data Science still work--progress. chapter readable currently undergoing final polishing.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-code.html","id":"introduction-1","chapter":"4 Code","heading":"4.1 Introduction","text":"already described might organise effective data science project directory file level. chapter delve one step deeper consider can structure work within files. particular, ’ll focus code files .’ll start comparing two main approaches structuring code, namely functional programming object oriented programming.’ll see order code within scripts conventions name functions objects work code.Rounding chapter, ’ll summarise main points R style guide following course highlight useful packages writing effective code.","code":""},{"path":"workflows-code.html","id":"functional-programming","chapter":"4 Code","heading":"4.2 Functional Programming","text":"functional programming style two major properties:Object immutability,Object immutability,Complex programs written using function composition.Complex programs written using function composition.Firstly, original data objects aren’t modified altered code. met idea making new, cleaner versions raw data leave original messy data intact. Object immutability exact idea code context rather data context.Secondly, functional programming, complex problems solved decomposing series smaller problems. separate, self-contained function written solve sub-problem. individual function , , simple easy understand. makes small functions easy test easy reuse many places. Code complexity built composing functions various ways.can difficult get way thinking, people mathematical training often find quite natural. mathematicians many years experience working function compositions abstract, mathematical sense.\\[y = g(x) = f_3 \\circ f_2 \\circ f_1(x).\\]","code":""},{"path":"workflows-code.html","id":"the-pipe-operator","chapter":"4 Code","heading":"4.2.1 The Pipe Operator","text":"One issue functional programming lots nested functions means also lots nested brackets. start get tricky keep track upwards 3 functions composed. reading difficulty exacerbated functions additional arguments top original inputs.pipe operator %>% magrittr package helps issue. works exactly like function composition: takes whatever left (whether object output function) passes following function call first argument function.pipe operator often referred “syntactic sugar”. doesn’t add anything code rather makes code much palatable.R versions 4.1 greater, ’s built-version pipe operator, written using vertical bar symbol followed greater sign. type vertical bar, can usually find found backslash keyboard.(Just cause confusion, vertical bar symbol also called pipe symbol general programming contexts. )base R pipe usually behaves way pipe magrittr, cases differ. reasons back-compatibility consistency ’ll stick magrittr pipe course.","code":"\nlog(exp(cos(sin(pi))))\n#> [1] 1\nlibrary(magrittr)\npi %>% \n  sin() %>% \n  cos() %>% \n  exp() %>% \n  log()\n#> [1] 1\niris %>% \n  head(n = 3)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\nlibrary(magrittr)\npi |> \n  sin() |> \n  cos() |> \n  exp() |> \n  log()\n#> [1] 1\niris |> \n  head(n = 3)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa"},{"path":"workflows-code.html","id":"when-not-to-pipe","chapter":"4 Code","heading":"4.2.2 When not to pipe","text":"Pipes designed put focus actions performing rather object preforming operations . means two cases almost certainly use pipe.first need manipulate one object time. Using secondary objects reference points (leaving unchanged) course perfectly fine, pipes used applying sequence steps create new, modified version one primary object.Secondly, just can chain together many actions single pipeline, doesn’t mean necessarily . long sequences piped operations easier read nested functions, however still burden reader cognitive load short term memory. kind create meaningful, intermediate objects informative names. ’ll help reader easily understand logic within code.","code":""},{"path":"workflows-code.html","id":"object-oriented-programming","chapter":"4 Code","heading":"4.3 Object Oriented Programming","text":"main alternative functional programming object oriented programming.Solve problems using lots simple objectsR 3 OOP systems: S3, S4 R6.Objects belong class, methods fields.Example: agent based simulation beehive.","code":""},{"path":"workflows-code.html","id":"oop-philosophy","chapter":"4 Code","heading":"4.3.1 OOP Philosophy","text":"functional programming, solve complicated problems using lots simple functions. object oriented programming solve complicated problems using lots simple objects. programming approaches best depend particular type problem trying solve.Functional programming excellent types data science work. Object oriented comes problem many small components interacting one another. makes great things like designing agent-based simulations, ’ll come back moment.R three different systems object oriented programming (called S3, S4, R6), things can get bit complicated. won’t go detail , ’ll give overview main ideas.approach programming might useful future, example want extend base R functions work new types input, user-friendly displays. case (Advanced R)[https://adv-r.hadley.nz/] Hadley Wickham excellent reference text.OOP, object belongs class set methods associated . class defines object methods describe object can . top , object class-specific attributes data fields. fields shared objects class values take give information specific object.","code":""},{"path":"workflows-code.html","id":"oop-example","chapter":"4 Code","heading":"4.3.2 OOP Example","text":"sounding abstract. Let’s consider writing object oriented code simulate beehive. object bee, bee instance one three bee classes: might queen, worker drone example. Different bee classes different methods associated , describe bee can , example bees 6 methods let move , , left, right, forward backward within hive. additional “reproduce” method might defined queen bees pollinate method might defined workers. instance bee fields, give data specific bee. bees x, y z coordinate fields giving location within hive. queen class might additional field number offspring workers might additional field much pollen carrying.simulation progresses, methods applied object altering fields potentially creating destroying objects. different preservation mindset functional programming, hopefully can see natural approach many types problem.","code":""},{"path":"workflows-code.html","id":"structuring-r-script-headers","chapter":"4 Code","heading":"4.4 Structuring R Script Headers","text":"TL;DR\n- Start script comment 1-2 sentences explaining > .\n- setwd() rm(ls()) devil’s work.\n- “Session” > “Restart R” Keyboard shortcut: crtl/cmd + shift > + 0\n- Polite gather library() source() calls.\n- Rude mess people’s set using > install.packages().\n- Portable scripts use paths relative root directory project.First things first, let’s discuss top R scripts.almost always good idea start file commented sentences describing purpose script , work large team, perhaps contact questions script. (comments coming soon, don’t worry!)also good practise move library() source() calls top script. indicate packages helper function dependencies script; ’s useful know need installed trying run code.segues nicely next point, never hard code package installations. extremely bad practise rude script might alter another person’s R installation. don’t know already, precisely difference install.packages() library() call: install.packages() download code package users computer, library() takes downloaded code makes available current R session. avoid messing anyone’s R installation, always type install.package() commands directly console place corresponding library() calls within scripts.Next, likely , someone close , commit felony starting every script setting working directory clearing R’s global environment. bad practice, ’s indicative workflow ’s project based ’s problematic least two reasons. Firstly, path set likely work anyone else’s computer. Secondly, clearing environment like may look like gets back fresh, new R session previously loaded packages still loaded lurking background.Instead, achieve original aim starting new R session, go menu select “Session” drop select “Restart R”. Alternatively, can use keyboard shortcuts . “crtl + shift + 0” Windows “cmd + shift + 0” mac. fact keyboard shortcut exists quite strongly hint , reproducible project oriented workflow, restarting R quite often average working day. scripting equivalent “clear output rerun ” notebook.Finally, let’s circle back point made earlier setting working directory. reason work likely giving file paths specific computer, operating system file organisation system. chances someone else practically zero.","code":""},{"path":"workflows-code.html","id":"portable-file-paths-with-here","chapter":"4 Code","heading":"4.5 Portable File paths with {here}","text":"fix problem person- computer-specific file paths can two options.first use relative file paths. assume R script run current location moving levels project directory point file need.good solves problem paths breaking move project different location laptop. However, fully solve portability problem might move file different location within project. also solve problem windows uses MacOS linux use forward slashes file paths widows uses backslashes.resolve final two issues recommend using () function {} package. package looks .Rproj .git file identify root directory project creates file paths relative root project, suitable operating system code run .really quite marvellous. information use package, explore chapter R - Forgot, R Data Science project oriented workflow blog post.","code":"\n# Bad - breaks if project moved\nsource(\"zaks-mbp/Desktop/exciting-new-project/src/helper_functions/rolling_mean.R\")\n\n# Better - breaks if Windows\nsource(\"../../src/helper_functions/rolling_mean.R\")\n\n# Best - but use here:here() to check root directory correctly identified\nsource(here::here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n# For more info on the here package:\nvignette(\"here\")"},{"path":"workflows-code.html","id":"code-body","chapter":"4 Code","heading":"4.6 Code Body","text":"Moving now, go head body code. well named organised code facilitate reading understanding. Comments sectioning rest work.section designed introduction tidyverse style guide replacement .\n### CommentsComments may either short -line comments end line full lines dedicated comments. create either type comment R, simply type hash followed one space. rest line evaluated function comment. multi-line comments needed simply start multiple lines hash space.purpose comments explain , . explaining comments perhaps need consider writing informative function names, something return general advice section.Comments can also used add structure code, buy using commented lines hyphens equal signs chunk files minor major sections.Markdown-like section titles can added section subsection headers. Many IDEs, RStudio, interpret table contents , can easily navigate code.","code":"\n# This is an example script showing good use of comments and sectioning \n\nlibrary(here)\nsource(here(\"src\",\"helper_functions\",\"rolling_mean.R\"))\n\n#===============================================================================  <- 80 characters max for readability\n# Major Section on Comments ----\n#===============================================================================\n\n#-------------------------------------------------------------------------------\n##  Minor Section on inline comments ---- \n#-------------------------------------------------------------------------------\nx <- 1:10 # this is an inline comment\n\n#-------------------------------------------------------------------------------\n##  Minor Section on full line comments ---- \n#-------------------------------------------------------------------------------\nrolling_mean(x)\n# This is an full line comment"},{"path":"workflows-code.html","id":"objects-are-nouns","chapter":"4 Code","heading":"4.6.1 Objects are Nouns","text":"Object names use lowercase letters, numbers, _.Object names use lowercase letters, numbers, _.Use underscores (_) separate words within name. (snake_case)Use underscores (_) separate words within name. (snake_case)Use nouns, preferring singular plural names.Use nouns, preferring singular plural names.creating naming objects strong guideline objects named using short meaningful nouns. Names include special characters use underscores separate words within object name.similar file naming guide, note hyphens can’t used object names conflicts subtraction operator.naming objects, far possible use singular nouns. main reason plurisation rules English complex eventually trip either user code.","code":"\n# Good\nday_one\nday_1\n\n# Bad\nfirst_day_of_the_month\nDayOne\ndayone\ndjm1"},{"path":"workflows-code.html","id":"functions-are-verbs","chapter":"4 Code","heading":"4.6.2 Functions are Verbs","text":"Function names use lowercase letters, numbers, _.Function names use lowercase letters, numbers, _.Use underscores (_) separate words within name. (snake_case)Use underscores (_) separate words within name. (snake_case)Suggest imperative mood, recipe.Suggest imperative mood, recipe.Break long functions multiple lines. 4 vs 2 spaces.Break long functions multiple lines. 4 vs 2 spaces.guidelines naming functions broadly similar, advice functions verbs rather nouns.Functions named imperative mood, like recipe. consistency; function names range moods tenses leads coding nightmares.object names aim give functions arguments short, evocative names. functions many arguments long name, might able fit function definition single line. case can place argument double indented line function body single indented line.","code":"\n# Good\nadd_row()\npermute()\n\n# Bad\nrow_adder()\npermutation()\n\nlong_function_name <- function(\n    a = \"a long argument\",\n    b = \"another argument\",\n    c = \"another long argument\") {\n  # As usual code is indented by two spaces.\n}"},{"path":"workflows-code.html","id":"casing-consistantly","chapter":"4 Code","heading":"4.6.3 Casing Consistantly","text":"mentioned already, many options separating words within names:CamelCasepascalCasesnakecaseunderscore_separated ❤️hyphen-separatedpoint.separated 💀people used working Python tempting use point separation function names, spirit methods object oriented programming. Indeed, base R functions even use convention.However, reason advise already used methods R’s inbuilt OOP functionality. use underscore separation work.","code":""},{"path":"workflows-code.html","id":"style-guide-summary","chapter":"4 Code","heading":"4.6.4 Style Guide Summary","text":"Use comments structure codeObjects = NounsFunctions = VerbsUse snake case consistant grammar","code":""},{"path":"workflows-code.html","id":"further-tips-for-friendly-coding","chapter":"4 Code","heading":"4.7 Further Tips for Friendly Coding","text":"addition naming conventions style guide gives lots guidance writing code way kind future readers code.’m going go repeat guidance , motivation can boiled following points.Write code easily understood humans.Write code easily understood humans.Use informative names, typing cheap.Use informative names, typing cheap.Divide work logical stages, human memory expensive.writing code, keep future reader mind. means using names informative reasonably short, also means adding white space, comments formatting aid comprehension. Adding sort structure code also helps reduce cognitive burden placing human reading code.Informative names important short names. particularly true using flow controls, things like loops loops. loops like encounter approaching deadline urgently fixing bug? Almost surely second one, context immediately clear.computer doesn’t care call variable single letter, random key smash (like aksnbioawb) informative name. computer also doesn’t care include white space code - script still run. However, things friendly practices can help debugging co-workers collaborating.","code":"\n# Bad\nfor(i in dmt){\n  print(i)\n}\n\n# Good\nfor(temperature in daily_max_temperature){\n  print(temperature)\n}"},{"path":"workflows-code.html","id":"reduce-reuse-recycle","chapter":"4 Code","heading":"4.8 Reduce, Reuse, Recycle","text":"final section, ’ll look can make workflow efficient reducing amount code write, well reusing recycling code ’ve already written.","code":""},{"path":"workflows-code.html","id":"dry-coding","chapter":"4 Code","heading":"4.8.1 DRY Coding","text":"idea making workflow efficient reducing, reusing recycling code summarised DRY acronym: don’t repeat .can boiled three main points:something twice single script, write function thing,want use function elsewhere within project, save separate scriptIf want use function across projects, add package.course, like scoping projects first place, requires level clairvoyance: able look future see whether ’ll use function another script project. difficult, bordering impossible. practice, done retrospectively - find second script project needs function pull separate file include package.rule thumb, consider whether make function widely available . takes much less effort work now, ’s fresh mind, refamiliarise code several years time.Let’s now look implement sub-bullet points:\n“write function, document ” “write function, test ”.","code":""},{"path":"workflows-code.html","id":"rememer-how-to-use-your-own-code","chapter":"4 Code","heading":"4.8.2 Rememer how to use your own code","text":"come use function written somebody else, likely refer documentation teach remind things like expected inputs exactly method implemented.writing functions create documentation fills need. Even function just personal use, time ’ll forget exactly works.write function, document .documentation contain?InputsOutputsExample use casesAuthor (obvious working team)documentation describe inputs outputs function, simple example uses. working large team, documentation also indicate wrote function ’s responsible maintaining time.","code":""},{"path":"workflows-code.html","id":"roxygen2-for-documentation","chapter":"4 Code","heading":"4.8.3 {roxygen2} for documentation","text":"way used package simplify file path problems, ’ll use roxygen2 package simplify testing workflow.{roxygen2} package gives us easily insert-able temple documenting functions. means don’t waste time energy typing remembering boilerplate code. also puts documentation format allows us get hints auto-completion functions, just like functions use packages written people.use Roxygen, need install - doesn’t need loaded library call top script. ’ve done , cursor inside function definition, can insert skeleton code document function one two ways: can either use Rstudio menu keyboard short cut operating system.install.packages(\"roxygen2\")cursor inside function: Code > Insert Roxygen SkeletonKeyboard shortcut: cmd + option + shift + r crtl + option + shift + rFill relevant fields","code":""},{"path":"workflows-code.html","id":"an-roxygen2-example","chapter":"4 Code","heading":"4.8.4 An {roxygen2} example","text":", ’ve got example Roxygen skeleton document function calculates geometric mean vector. , hash followed apostrophe special type comment. indicates function documentation rather just regular comment.’ll fill fields skeleton apart export, ’ll remove. put function R package, export field makes available users package. Since just standalone function won’t need export field, though keeping wouldn’t actually cause us problems either.filled skeleton documentation might look something like . described function , expected inputs user can expect output. ’ve also given simple examples function can used.Roxygen, see package documentation chapter R packages function documentation.","code":"\n#' Title\n#'\n#' @param x \n#' @param remove_NA \n#'\n#' @return\n#' @export\n#'\n#' @examples\ngeometric_mean <- function(x, remove_NA = FALSE){\n  # Function body goes here\n}\n#' Calculate the geometric mean of a numeric vector\n#'\n#' @param x numeric vector\n#' @param remove_NA logical scalar, indicating whether NA values should be stripped before computation proceeds. \n#'\n#' @return the geometric mean of the values in `x`, a numeric scalar value. \n#'\n#' @examples\n#' geometric_mean(x = 1:10)\n#' geometric_mean(x = c(1:10, NA), remove_NA = TRUE)\n#' \ngeometric_mean <- function(x, remove_NA = FALSE){\n  # Function body goes here\n}"},{"path":"workflows-code.html","id":"checking-your-code","chapter":"4 Code","heading":"4.8.5 Checking Your Code","text":"write function, test .Testing code two main purposes:warn prevent user misuse (e.g. strange inputs),catch edge cases.top explaining functions work, really check work. job unit testing.Whenever write function test works intended . Additionally, test function robust misused user. Depending context, might accidental malicious misuse. Finally, check function behaves properly strange, still valid, inputs. known edge cases.Testing can bit brutal process, ’ve just created beautiful function now ’re job best break !","code":""},{"path":"workflows-code.html","id":"an-informal-testing-workflow","chapter":"4 Code","heading":"4.8.6 An Informal Testing Workflow","text":"Write functionExperiment function console, try break itFix break repeat.Problems: Time consuming reproducible.informal approach testing code might first write function play around console check behaves well give obvious inputs, edge cases deliberately wrong inputs. time manage break function, edit fix problem start process .testing code, informally. ’s record tried break code already. problem approach return code add new feature, ’ll probably forgotten least one informal tests ran first time around. goes efforts towards reproducibility automation. also makes easy break code used work just fine.","code":""},{"path":"workflows-code.html","id":"a-formal-testing-workflow","chapter":"4 Code","heading":"4.8.7 A Formal Testing Workflow","text":"can formalise testing workflow writing tests R script saving future reference. Remember first lecture saved tests/ directory, structure mirror src/ directory project. tests one function live single file, named function.One way writing tests use lots statements. testthat can syntactic heavy lifting us. lots helpful functions test output function expect.example, error function returns logical NA rather double NA. Yes, R really different types NA different types missing data, usually just handles nicely background .subtle difference probably something spotted , caused trouble much line. rigorous approach one benefits using testthat functions.fix test change expected output NA_real_.’ll revisit testthat package live session week, learn use test functions within packages.","code":"\ngeometric_mean <- function(x , remove_NA = FALSE){prod(x)^(1/length(x))}\ntestthat::expect_equal(\n  object = geometric_mean(x = c(1, NA), remove_NA = FALSE),\n  expected = NA)\n\n# Error: geometric_mean(x = c(1, NA), remove_NA = FALSE) not equal to NA.\n# Types not compatible: double is not logical\ntestthat::expect_equal(\n  object = geometric_mean(x = c(1,NA), remove_NA = FALSE),\n  expected = NA_real_)"},{"path":"workflows-code.html","id":"summary-1","chapter":"4 Code","heading":"4.9 Summary","text":"Functional Object Oriented ProgrammingStructuring scriptsStyling codeReduce, reuse, recycleDocumenting testingLet’s wrap summarising learned chapter.started discussion differences functional object oriented programming. R capable , data science work tends functional flavour .’ve described structure scripts style code make human-friendly easy debug possible.Finally, discussed write DRY code well documented tested.","code":""},{"path":"workflows-checklist.html","id":"workflows-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"workflows-checklist.html","id":"videos-chapters","chapter":"Checklist","heading":"4.10 Videos / Chapters","text":"Organising work (30 min) [slides]Naming Files (20 min) [slides]Organising code (27 min) [slides]\n","code":""},{"path":"workflows-checklist.html","id":"reading","chapter":"Checklist","heading":"4.11 Reading","text":"Use workflows section reading list support guide exploration week’s materials. Note texts divided core reading, reference materials materials interest.","code":""},{"path":"workflows-checklist.html","id":"tasks","chapter":"Checklist","heading":"4.12 Tasks","text":"Core:Find 3 data science projects Github explore organise work. Write post EdStem forum links three, couple paragraphs describe content structure one project.Find 3 data science projects Github explore organise work. Write post EdStem forum links three, couple paragraphs describe content structure one project.Create project directory (directories) course assignments.Create project directory (directories) course assignments.Write two R functions. first calculate geometric mean numeric vector. second calculate rolling arithmetic mean numeric vector.Write two R functions. first calculate geometric mean numeric vector. second calculate rolling arithmetic mean numeric vector.Bonus:Re-factor old project match project organisation coding guides course. might small research project, class notes collection homework assignments. Use R-based project possible. python projects, either translate R apply PEP8 style guide. Take care select suitably sized project meaningful exercise take hours.Re-factor old project match project organisation coding guides course. might small research project, class notes collection homework assignments. Use R-based project possible. python projects, either translate R apply PEP8 style guide. Take care select suitably sized project meaningful exercise take hours.able , host re-factored project publicly share rest class EdStem Discussion forum.able , host re-factored project publicly share rest class EdStem Discussion forum.","code":""},{"path":"workflows-checklist.html","id":"live-session","chapter":"Checklist","heading":"4.13 Live Session","text":"live session begin discussion week’s tasks. create minimal R package organise test functions written.Please come live session prepared discuss following points:make assignment projects subdirectories stand alone projects? ?make assignment projects subdirectories stand alone projects? ?terms met readings? find meanings?terms met readings? find meanings?consider writing rolling mean function?consider writing rolling mean function?","code":""},{"path":"data-introduction.html","id":"data-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.Data can difficult acquire gnarly get .raw material work data scientist , unsurprisingly, data. part course focus different ways data can stored, distributed obtained.able obtain read dataset often surprisingly large hurdle getting new data science project ground. skill able source read data many locations usually sanitised statistics programme: ’re given ready--go, cleaned CSV file focus placed modelling. week aims remedy equipping skills acquire manage data.begin week explore different file types. dictates type information can store, can access information read R. turn attention case data given directly. learn obtain data raw webpage request data via service known API.","code":""},{"path":"data-tabular.html","id":"data-tabular","chapter":"5 Tabular Data","heading":"5 Tabular Data","text":"Effective Data Science still work--progress. chapter undergoing heavy restructuring may confusing incomplete.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-tabular.html","id":"loading-tabular-data","chapter":"5 Tabular Data","heading":"5.1 Loading Tabular Data","text":"Recall simpler, open source formats improve accessibility reproducibility. begin reading three open data formats tabular data.random-data.csvrandom-data.csvrandom-data.tsvrandom-data.tsvrandom-data.txtrandom-data.txtEach data sets contains 26 observations 4 variables:id, Roman letter identifier;gaussian, standard normal random variates;gamma, gamma(1,1) random variates;uniform, uniform(0,1) random variates.","code":""},{"path":"data-tabular.html","id":"base-r","chapter":"5 Tabular Data","heading":"5.1.1 Base R","text":"Output data.frame object. (List vectors nice methods)","code":"\nrandom_df <- read.csv(file = 'random-data.csv')\nprint(random_df)\n#>    id    gaussian      gamma    uniform\n#> 1   a -1.20706575 0.98899970 0.22484576\n#> 2   b  0.27742924 0.03813386 0.08498474\n#> 3   c  1.08444118 1.09462335 0.63729826\n#> 4   d -2.34569770 1.49301101 0.43101637\n#> 5   e  0.42912469 5.40361248 0.07271609\n#> 6   f  0.50605589 1.72386539 0.80240202\n#> 7   g -0.57473996 1.95357133 0.32527830\n#> 8   h -0.54663186 0.07807803 0.75728904\n#> 9   i -0.56445200 0.21198194 0.58427152\n#> 10  j -0.89003783 0.20803673 0.70883941\n#> 11  k -0.47719270 2.08607862 0.42697577\n#> 12  l -0.99838644 0.49463708 0.34357270\n#> 13  m -0.77625389 0.77171305 0.75911999\n#> 14  n  0.06445882 0.37216648 0.42403021\n#> 15  o  0.95949406 1.88207991 0.56088725\n#> 16  p -0.11028549 0.76622568 0.11613577\n#> 17  q -0.51100951 0.50488585 0.30302180\n#> 18  r -0.91119542 0.22979791 0.47880269\n#> 19  s -0.83717168 0.75637275 0.34483055\n#> 20  t  2.41583518 0.62435969 0.60071414\n#> 21  u  0.13408822 0.64638373 0.07608332\n#> 22  v -0.49068590 0.11247545 0.95599261\n#> 23  w -0.44054787 0.11924307 0.02220682\n#> 24  x  0.45958944 4.91805535 0.84171063\n#> 25  y -0.69372025 0.60282666 0.63244245\n#> 26  z -1.44820491 0.64446571 0.31009417"},{"path":"data-tabular.html","id":"readr","chapter":"5 Tabular Data","heading":"5.1.2 {readr}","text":"Output tibble object. (List vectors nicer methods)","code":"\nrandom_tbl <- readr::read_csv(file = 'random-data.csv')\n#> Rows: 26 Columns: 4\n#> ── Column specification ─────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): id\n#> dbl (3): gaussian, gamma, uniform\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nprint(random_tbl)\n#> # A tibble: 26 × 4\n#>   id    gaussian  gamma uniform\n#>   <chr>    <dbl>  <dbl>   <dbl>\n#> 1 a       -1.21  0.989   0.225 \n#> 2 b        0.277 0.0381  0.0850\n#> 3 c        1.08  1.09    0.637 \n#> 4 d       -2.35  1.49    0.431 \n#> 5 e        0.429 5.40    0.0727\n#> 6 f        0.506 1.72    0.802 \n#> # … with 20 more rows"},{"path":"data-tabular.html","id":"benefits-of-readrread_csv","chapter":"5 Tabular Data","heading":"5.1.2.1 Benefits of readr::read_csv()","text":"Increased speed (approx. 10x) progress bar.Increased speed (approx. 10x) progress bar.Strings coerced factors. stringsAsFactors = FALSEStrings coerced factors. stringsAsFactors = FALSENo row names nice column names.row names nice column names.Reproducibility bonus: depend operating system.Reproducibility bonus: depend operating system.","code":""},{"path":"data-tabular.html","id":"wtf-tibbles","chapter":"5 Tabular Data","heading":"5.1.3 WTF: Tibbles","text":"","code":""},{"path":"data-tabular.html","id":"printing","chapter":"5 Tabular Data","heading":"5.1.3.1 Printing","text":"Default first 10 rows many columns comfortably fit screen.Default first 10 rows many columns comfortably fit screen.Can adjust behaviour print call:Can adjust behaviour print call:Bonus: Colour formatting IDE column tells ’s type.","code":"\n# print first three rows and all columns\nprint(random_tbl, n = 3, width = Inf)\n#> # A tibble: 26 × 4\n#>   id    gaussian  gamma uniform\n#>   <chr>    <dbl>  <dbl>   <dbl>\n#> 1 a       -1.21  0.989   0.225 \n#> 2 b        0.277 0.0381  0.0850\n#> 3 c        1.08  1.09    0.637 \n#> # … with 23 more rows"},{"path":"data-tabular.html","id":"subsetting","chapter":"5 Tabular Data","heading":"5.1.3.2 Subsetting","text":"Subsetting tibbles always return another tibble.helps avoids edge cases associated working data frames.","code":"\n# Row Subsetting\nrandom_tbl[1, ] # returns tibble\nrandom_df[1, ]  # returns data.frame\n\n# Column Subsetting\nrandom_tbl[ , 1]      # returns tibble\nrandom_df[ , 1]       # returns vector\n\n# Combined Subsetting\nrandom_tbl[1, 1]      # returns 1x1 tibble\nrandom_df[1, 1]       # returns single value"},{"path":"data-tabular.html","id":"other-readr-functions","chapter":"5 Tabular Data","heading":"5.1.4 Other {readr} functions","text":"See readr documentation, lots useful additional arguments can help reading messy data.Functions reading writing types tabular data work analogously.","code":""},{"path":"data-tabular.html","id":"reading-tabular-data","chapter":"5 Tabular Data","heading":"5.1.4.1 Reading Tabular Data","text":"","code":"\nlibrary(readr)\nread_tsv(\"random-data.tsv\")\nread_delim(\"random-data.txt\", delim = \" \")"},{"path":"data-tabular.html","id":"writing-tabular-data","chapter":"5 Tabular Data","heading":"5.1.4.2 Writing Tabular Data","text":"","code":"\nwrite_csv(random_tbl, \"random-data-2.csv\")\nwrite_tsv(random_tbl, \"random-data-2.tsv\")\nwrite_delim(random_tbl, \"random-data-2.tsv\", delim = \" \")"},{"path":"data-tabular.html","id":"need-for-speed","chapter":"5 Tabular Data","heading":"5.1.5 Need for Speed","text":"times load lots large data sets, case 10x speed-might sufficient.data set still fits inside RAM, check data.table::fread() optimised speed. (Alternatives exist optimal memory usage data large working memory, covered .)Note: can much faster, resulting data.table object lacks consistancy properties tibble sure check edge cases, returned value might expect.","code":""},{"path":"data-tabular.html","id":"tidy-data","chapter":"5 Tabular Data","heading":"5.2 Tidy Data","text":"","code":""},{"path":"data-tabular.html","id":"wide-vs.-tall-data","chapter":"5 Tabular Data","heading":"5.2.1 Wide vs. Tall Data","text":"","code":""},{"path":"data-tabular.html","id":"wide-data","chapter":"5 Tabular Data","heading":"5.2.1.1 Wide Data","text":"First column unique entriesFirst column unique entriesEasier humans read compute onEasier humans read compute onHarder machines compute onHarder machines compute ","code":""},{"path":"data-tabular.html","id":"tall-data","chapter":"5 Tabular Data","heading":"5.2.1.2 Tall Data","text":"First column repeating entriesFirst column repeating entriesHarder humans read compute onHarder humans read compute onEasier machines compute onEasier machines compute ","code":""},{"path":"data-tabular.html","id":"examples","chapter":"5 Tabular Data","heading":"5.2.1.3 Examples","text":"Example 1 (Wide)Example 1 (Tall)[Source: Wikipedia - Wide narrow data]Example 2 (Wide)Example 2 (Tall)[Source: Statology - Long vs wide data]","code":""},{"path":"data-tabular.html","id":"pivoting-wider-and-longer","chapter":"5 Tabular Data","heading":"5.2.1.4 Pivoting Wider and Longer","text":"Error control input analysis format-dependent.Error control input analysis format-dependent.Switching long wide formats useful control errors.Switching long wide formats useful control errors.Easy tidyr package functionsEasy tidyr package functions","code":"\ntidyr::pivot_longer()\ntidyr::pivot_wider()"},{"path":"data-tabular.html","id":"tidy-what","chapter":"5 Tabular Data","heading":"5.2.2 Tidy What?","text":"[Image: R4DS - Chapter 12]Tidy Data opinionated way store tabular data.Image Source: Chapter 12 R Data Science.column corresponds exactly one measured variableEach row corresponds exactly one observational unitEach cell contains exactly one value.Benefits tidy dataConsistent data format: Reduces cognitive load allows specialised tools (functions) efficiently work tabular data.Consistent data format: Reduces cognitive load allows specialised tools (functions) efficiently work tabular data.Vectorisation: Keeping variables columns allows efficient data manipulation. (goes back data frames tibbles lists vectors)Vectorisation: Keeping variables columns allows efficient data manipulation. (goes back data frames tibbles lists vectors)","code":""},{"path":"data-tabular.html","id":"example---tidy-longer","chapter":"5 Tabular Data","heading":"5.2.3 Example - Tidy Longer","text":"Consider trying plot data time series. year variable trapped column names!tidy data, need pivot_longer(). turn column names new year variable retaining cell contents new variable called cases.Much better!","code":"\ncountries\n#> # A tibble: 3 × 3\n#>   country     `1999` `2000`\n#>   <chr>        <dbl>  <dbl>\n#> 1 Afghanistan    745   2666\n#> 2 Brazil       37737  80488\n#> 3 China       212258 213766\nlibrary(magrittr)\ncountries %>% \n  tidyr::pivot_longer(cols = c(`1999`,`2000`), names_to = \"year\", values_to = \"cases\")\n#> # A tibble: 6 × 3\n#>   country     year   cases\n#>   <chr>       <chr>  <dbl>\n#> 1 Afghanistan 1999     745\n#> 2 Afghanistan 2000    2666\n#> 3 Brazil      1999   37737\n#> 4 Brazil      2000   80488\n#> 5 China       1999  212258\n#> 6 China       2000  213766"},{"path":"data-tabular.html","id":"example---tidy-wider","chapter":"5 Tabular Data","heading":"5.2.4 Example - Tidy Wider","text":"times might widen data tidy .example tidy. ?observational unit team. However, variable stored separate column, cells containing values.tidy data first generate tibble. use tribble() function, allows us create tibble row-wise rather column-wise.can tidy creating new columns value current Variable column taking values current Value column.","code":"\ntournament <- tibble::tribble(\n~Team  , ~Variable , ~Value,\n\"A\"    , \"Points\"  , 88    ,\n\"A\"    , \"Assists\" , 12    ,\n\"A\"    , \"Rebounds\", 22    ,\n\"B\"    , \"Points\"  , 91    ,\n\"B\"    , \"Assists\" , 17    ,\n\"B\"    , \"Rebounds\", 28    ,\n\"C\"    , \"Points\"  , 99    ,\n\"C\"    , \"Assists\" , 24    ,\n\"C\"    , \"Rebounds\", 30    ,\n\"D\"    , \"Points\"  , 94    ,\n\"D\"    , \"Assists\" , 28    ,\n\"D\"    , \"Rebounds\", 31    )\ntournament %>% \n  tidyr::pivot_wider(\n    id_cols = \"Team\", \n    names_from = \"Variable\",\n    values_from = \"Value\")\n#> # A tibble: 4 × 4\n#>   Team  Points Assists Rebounds\n#>   <chr>  <dbl>   <dbl>    <dbl>\n#> 1 A         88      12       22\n#> 2 B         91      17       28\n#> 3 C         99      24       30\n#> 4 D         94      28       31"},{"path":"data-tabular.html","id":"other-helpful-functions","chapter":"5 Tabular Data","heading":"5.2.5 Other helpful functions","text":"pivot_*() family functions resolve issues rows (many observations per row rows per observation).similar helper functions solve column issues:Multiple variables per column: tidyr::separate(),Multiple variables per column: tidyr::separate(),Multiple columns per variable: tidyr::unite().Multiple columns per variable: tidyr::unite().","code":""},{"path":"data-tabular.html","id":"missing-data","chapter":"5 Tabular Data","heading":"5.2.6 Missing Data","text":"tidy data, every cell contains value. Including cells missing values.Missing values coded NA (generic) type-specific NA, NA_character_.Missing values coded NA (generic) type-specific NA, NA_character_.readr family read_*() function good defaults helpful na argument.readr family read_*() function good defaults helpful na argument.Explicitly code NA values collecting data, avoid ambiguity: ” “, -999 worst 0.Explicitly code NA values collecting data, avoid ambiguity: ” “, -999 worst 0.missing values EDA videos…missing values EDA videos…","code":""},{"path":"data-tabular.html","id":"wrapping-up-1","chapter":"5 Tabular Data","heading":"5.3 Wrapping Up","text":"Reading tabular data range methodsReading tabular data range methodsIntroduced tibble tidy data (+ tidy always best)Introduced tibble tidy data (+ tidy always best)Tools tidying messy tabular dataTools tidying messy tabular data","code":""},{"path":"data-webscraping.html","id":"data-webscraping","chapter":"6 Web Scraping","heading":"6 Web Scraping","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-webscraping.html","id":"scraping-webpage-data-using-rvest","chapter":"6 Web Scraping","heading":"6.1 Scraping webpage data using {rvest}","text":"can’t always rely tidy, tabular data land desk. Sometimes going go gather data .’m suggesting need manually, likely need get data internet ’s made publicly privately available .might information webpage gather , data shared collaborator using API.chapter cover basics scraping webpages, following vignette {rvest} package.","code":""},{"path":"data-webscraping.html","id":"what-is-a-webpage","chapter":"6 Web Scraping","heading":"6.2 What is a webpage?","text":"can even hope get data webpage, first need understand webpage .Webpages written similar way LaTeX: content styling webpages handled separately coded using plain text files.fact, websites go one step LaTeX. content styling websites written different files different languages. HTML (HyperText Markup Language) used write content CSS (Cascading Style Sheets) used control appearance content ’s displayed user.","code":""},{"path":"data-webscraping.html","id":"html","chapter":"6 Web Scraping","heading":"6.3 HTML","text":"basic HTML page styling applied might look something like :","code":"<html>\n<head>\n  <title>Page title<\/title>\n<\/head>\n<body>\n  <h1 id='first'>A level 1 heading<\/h1>\n  <p>Hello World!<\/p>\n  <p>Here is some plain text &amp; <b>some bold text.<\/b><\/p>\n  <img src='myimg.png' width='100' height='100'>\n<\/body>"},{"path":"data-webscraping.html","id":"html-elements","chapter":"6 Web Scraping","heading":"6.3.1 HTML elements","text":"Just like XML data files, HTML hierarchical structure. structure crafted using HTML elements. HTML element made start tag, optional attributes, end tag.can see first level header, <h1> opening tag, id='first' additional attribute <\/h1> closing tag. Everything opening closing tag contents element. also special elements consist single tag optional attributes. example <img> tag.Since < > used start end tags, can’t write directly HTML document. Instead, use escape characters. sounds fancy, ’s just alternative way write characters serve special function within language.can write greater &gt; less &lt;. might notice escapes use ampersand (&). means want literal ampersand webpage, escape using &amp;.wide range possible HTML tags escapes. ’ll cover common tags lecture don’t need worry escapes much rvest automatically handle .","code":""},{"path":"data-webscraping.html","id":"important-html-elements","chapter":"6 Web Scraping","heading":"6.3.2 Important HTML Elements","text":", excess 100 HTML elements. important ones know :<html> element, must enclose every HTML page. <html> element must two child elements within . <head> element contains metadata document, like page title shown browser tab CSS style sheet applied. <body> element contains content see browser.<html> element, must enclose every HTML page. <html> element must two child elements within . <head> element contains metadata document, like page title shown browser tab CSS style sheet applied. <body> element contains content see browser.Block elements used give structure page. elements like headings, sub-headings <h1> way <h6>. category also contains paragraph elements <p>, ordered lists <ol> unordered lists <ul>.Block elements used give structure page. elements like headings, sub-headings <h1> way <h6>. category also contains paragraph elements <p>, ordered lists <ol> unordered lists <ul>.Finally, inline tags like <b> bold, <> italics, <> hyperlinks used format text inside block elements.Finally, inline tags like <b> bold, <> italics, <> hyperlinks used format text inside block elements.come across tag ’ve never seen , can find just little bit googling. good resource MDN Web Docs produced Mozilla, company makes Firefox web browser. W3schools website another great resource web development coding resources generally.","code":""},{"path":"data-webscraping.html","id":"html-attributes","chapter":"6 Web Scraping","heading":"6.4 HTML Attributes","text":"’ve seen one example header additional attribute. generally, tags can named attributes. attributes contained within opening tag look something like:Two important attributes id class. attributes used conjunction CSS file control visual appearance page. often useful identify elements interested scraping data page.","code":"<tag attribute1='value1' attribute2='value2'>element contents<\/tag>"},{"path":"data-webscraping.html","id":"css-selectors","chapter":"6 Web Scraping","heading":"6.5 CSS Selectors","text":"Cascading Style Sheet used describe HTML content displayed. , CSS ’s system selecting elements webpage, called CSS selectors.CSS selectors define patterns locating HTML elements particular style applied . happy side-effect can sometimes useful scraping, provide concise way describing elements want extract.CSS Selectors can work level element type, class, tag can used nested (cascading) way.p selector select paragraph <p> elements.p selector select paragraph <p> elements..title selector select elements class “title”..title selector select elements class “title”.p.special selector select <p> elements class “special”.p.special selector select <p> elements class “special”.#title selector select element id attribute “title”.#title selector select element id attribute “title”.want select single element id attributes particularly useful must unique within html document. Unfortunately, helpful developer added id attribute element(s) want scrape!want learn CSS selectors recommend starting fun CSS dinner tutorial build base knowledge using W3schools resources reference explore webpages wild.","code":""},{"path":"data-webscraping.html","id":"which-attributes-and-selectors-do-you-need","chapter":"6 Web Scraping","heading":"6.6 Which Attributes and Selectors Do You Need?","text":"scrape data webpage, first identify tag attribute combinations interested gathering.find elements interest, three options. go hardest easiest also least robust.right click + “inspect page source” (F12)right click + “inspect”Rvest Selector Gadget (useful fallible)Inspecting source familiar websites can useful way get head around concepts. Beware though sophisticated webpages can quite intimidating. good place start simpler, static websites personal websites, rather dynamic webpages online retailers social media platforms.","code":""},{"path":"data-webscraping.html","id":"reading-html-with-rvest","chapter":"6 Web Scraping","heading":"6.7 Reading HTML with {rvest}","text":"rvest, reading html page can simple loading tabular data.class resulting object xml_document. type object low-level package xml2, allows read xml files R.can see object split several components: first metadata type document scraped, followed head body html document.several possible approaches extracting information document.","code":"\nhtml <- rvest::read_html(\"https://www.zakvarty.com/professional/teaching.html\")\nclass(html)\n#> [1] \"xml_document\" \"xml_node\"\nhtml\n#> {html_document}\n#> <html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#> [2] <body class=\"nav-fixed\">\\n\\n<div id=\"quarto-search-results\"><\/div>\\n   ..."},{"path":"data-webscraping.html","id":"extracting-html-elements","chapter":"6 Web Scraping","heading":"6.8 Extracting HTML elements","text":"rvest can extract single element html_element(), matching elements html_elements(). functions take document object one CSS selectors inputs.can also combine nest selectors. example might want extract links within paragraphs second level headers.","code":"\nlibrary(rvest)\n\nhtml %>% html_elements(\"h1\")\n#> {xml_nodeset (1)}\n#> [1] <h1>Teaching<\/h1>\nhtml %>% html_elements(\"h2\")\n#> {xml_nodeset (2)}\n#> [1] <h2 id=\"toc-title\">On this page<\/h2>\n#> [2] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History<\/h2>\nhtml %>% html_elements(\"p\")\n#> {xml_nodeset (2)}\n#> [1] <p>I am fortunate to have had the opportunity to teach in a variety of ...\n#> [2] <p>I am an associate fellow of the Higher Education Academy, which you ...\nhtml %>% html_elements(\"p a,h2\")\n#> {xml_nodeset (3)}\n#> [1] <h2 id=\"toc-title\">On this page<\/h2>\n#> [2] <a href=\"https://www.advance-he.ac.uk/fellowship/associate-fellowship\" ...\n#> [3] <h2 class=\"anchored\" data-anchor-id=\"course-history\">Course History<\/h2>"},{"path":"data-webscraping.html","id":"extracting-data-from-html-elements","chapter":"6 Web Scraping","heading":"6.9 Extracting Data From HTML Elements","text":"Now ’ve got elements care extracted complete document. get data need elements?’ll usually get data either contents HTML element else one ’s attributes. ’re really lucky, data need already formatted HTML table list.","code":""},{"path":"data-webscraping.html","id":"extracting-text","chapter":"6 Web Scraping","heading":"6.9.1 Extracting text","text":"functions rvest::html_text() rvest::html_text2() can used extract plain text contents HTML element.difference html_text() html_text2() handle whitespace. HTML whitespace line breaks little influence code interpreted computer (similar R different Python). html_text() extract text raw html, html_text2() best extract text way gives something similar ’d see browser.","code":"\nhtml %>% \n  html_elements(\"#teaching li\") %>% \n  html_text2()\n#> [1] \"one-to-one tuition for high school students;\"                                   \n#> [2] \"running workshops and computer labs for undergraduate and postgraduate modules;\"\n#> [3] \"delivering short courses on scientific communication and LaTeX;\"                \n#> [4] \"supervising an undergraduate research project;\"                                 \n#> [5] \"developing and lecturing postgraduate modules in statistics and data science.\""},{"path":"data-webscraping.html","id":"extracting-attributes","chapter":"6 Web Scraping","heading":"6.9.2 Extracting Attributes","text":"Attributes also used record information might like collect. example, destination links stored href attribute source images stored src attribute.example , consider trying extract twitter link icon page footer. quite tricky locate html source, used Selector Gadget help find correct combination elements.extract href attribute scraped element, use rvest::html_attr() function.Note: rvest::html_attr() always return character string (list character strings). extracting attribute describes quantity, width image, ’ll need convert string required data type. example, width measures pixels might use .integer().","code":"\nhtml %>% html_element(\".compact:nth-child(1) .nav-link\")\n#> {html_node}\n#> <a class=\"nav-link\" href=\"https://www.twitter.com/zakvarty\">\n#> [1] <i class=\"bi bi-twitter\" role=\"img\">\\n<\/i>\nhtml %>% \n  html_elements(\".compact:nth-child(1) .nav-link\") %>% \n  html_attr(\"href\")\n#> [1] \"https://www.twitter.com/zakvarty\""},{"path":"data-webscraping.html","id":"extracting-tables","chapter":"6 Web Scraping","heading":"6.9.3 Extracting tables","text":"HTML tables composed similar, nested manner LaTeX tables.four main elements know make HTML table:<table>,<tr> (table row),<th> (table heading),<td> (table data).’s simple example data, formatted HTML table:Since tables common way store data, rvest includes useful function html_table() converts directly HTML table tibble.Applying real scraped data can easily extract table taught courses.","code":"\nhtml_2 <- minimal_html(\"\n  <table>\n    <tr>\n      <th>Name<\/th>\n      <th>Number<\/th>\n    <\/tr>\n    <tr>\n      <td>A<\/td>\n      <td>1<\/td>\n    <\/tr>\n    <tr>\n      <td>B<\/td>\n      <td>2<\/td>\n    <\/tr>\n    <tr>\n      <td>C<\/td>\n      <td>3<\/td>\n    <\/tr>\n  <\/table>\n  \")\nhtml_2 %>% \n  html_element(\"table\") %>% \n  html_table()\n#> # A tibble: 3 × 2\n#>   Name  Number\n#>   <chr>  <int>\n#> 1 A          1\n#> 2 B          2\n#> 3 C          3\nhtml %>% \n  html_element(\"table\") %>% \n  html_table()\n#> # A tibble: 25 × 3\n#>   Year      Course                            Role                       \n#>   <chr>     <chr>                             <chr>                      \n#> 1 \"2021-22\" Supervised Learning               Lecturer                   \n#> 2 \"\"        Ethics in Data Science I          Lecturer                   \n#> 3 \"\"        Ethics in Data Science II         Lecturer                   \n#> 4 \"—\"       —                                 —                          \n#> 5 \"2020-21\" MATH562/582: Extreme Value Theory Lecturer                   \n#> 6 \"\"        MATH331: Bayesian Inference       Graduate teaching assistant\n#> # … with 19 more rows"},{"path":"data-webscraping.html","id":"tip-for-building-tibbles","chapter":"6 Web Scraping","heading":"6.10 Tip for Building Tibbles","text":"scraping data webpage, end-goal typically going constructing data.frame tibble.following description tidy data, ’ll want row correspond repeated unit HTML page. case, shouldUse html_elements() select elements contain observation unit;Use html_element() extract variables observations.Taking approach guarantees ’ll get number values variable, html_element() always returns number outputs inputs. vital missing data - every observation unit value every variable interest.example, consider extract text starwars dataset.unordered list list item corresponds one observational unit (one character starwars universe). name character given bold, character species specified italics weight character denoted .weight class. However, characters subset variables defined: example Yoda species entry.try extract element directly, vectors variable values different lengths. don’t know missing values , can’t line back make tibble.instead start extracting list item elements using html_elements(). done , can use html_element() extract variable characters. pad NAs, can collate tibble.","code":"\nstarwars_html <- minimal_html(\"\n  <ul>\n    <li><b>C-3PO<\/b> is a <i>droid<\/i> that weighs <span class='weight'>167 kg<\/span><\/li>\n    <li><b>R2-D2<\/b> is a <i>droid<\/i> that weighs <span class='weight'>96 kg<\/span><\/li>\n    <li><b>Yoda<\/b> weighs <span class='weight'>66 kg<\/span><\/li>\n    <li><b>R4-P17<\/b> is a <i>droid<\/i><\/li>\n  <\/ul>\n  \")\nstarwars_html %>% html_elements(\"b\") %>% html_text2()\n#> [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_html %>% html_elements(\"i\") %>% html_text2()\n#> [1] \"droid\" \"droid\" \"droid\"\nstarwars_html %>% html_elements(\".weight\") %>% html_text2()\n#> [1] \"167 kg\" \"96 kg\"  \"66 kg\"\nstarwars_characters <- starwars_html %>% html_elements(\"li\")\n\nstarwars_characters %>% html_element(\"b\") %>% html_text2()\n#> [1] \"C-3PO\"  \"R2-D2\"  \"Yoda\"   \"R4-P17\"\nstarwars_characters %>% html_element(\"i\") %>% html_text2()\n#> [1] \"droid\" \"droid\" NA      \"droid\"\nstarwars_characters %>% html_element(\".weight\") %>% html_text2()\n#> [1] \"167 kg\" \"96 kg\"  \"66 kg\"  NA\ntibble::tibble(\n  name = starwars_characters %>% html_element(\"b\") %>% html_text2(),\n  species = starwars_characters %>% html_element(\"i\") %>% html_text2(),\n  weight = starwars_characters %>% html_element(\".weight\") %>% html_text2()\n)\n#> # A tibble: 4 × 3\n#>   name   species weight\n#>   <chr>  <chr>   <chr> \n#> 1 C-3PO  droid   167 kg\n#> 2 R2-D2  droid   96 kg \n#> 3 Yoda   <NA>    66 kg \n#> 4 R4-P17 droid   <NA>"},{"path":"data-apis.html","id":"data-apis","chapter":"7 APIs","heading":"7 APIs","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-apis.html","id":"aquiring-data-via-an-api","chapter":"7 APIs","heading":"7.1 Aquiring Data Via an API","text":"’ve already established can’t always rely tidy, tabular data land desk.Sometimes going go gather data . already seen scrape information directly HTML source webpage. surely easer way. Thankfully, often !chapter cover basics obtaining data via API. material draws together Introduction APIs book Brian Cooksey DIY web data section STAT545 University British Columbia.","code":""},{"path":"data-apis.html","id":"why-do-i-need-to-know-about-apis","chapter":"7 APIs","heading":"7.2 Why do I need to know about APIs?","text":"API, application programming interface, set rules allows different software applications communicate .data scientist, often need access data stored remote servers cloud-based services. APIs provide convenient way data scientists programmatically retrieve data, without manually download data sets process locally computer.multiple benefits including automation standardisation data sharing.Automation: much faster machine process data request human. machine handling data requests also scales much better either number complexity data requests grows. Additionally, lower risk introducing human error. example, human might accidentally share wrong data, can serious legal repercussions.Automation: much faster machine process data request human. machine handling data requests also scales much better either number complexity data requests grows. Additionally, lower risk introducing human error. example, human might accidentally share wrong data, can serious legal repercussions.Standardisation: machine process data requests requires format requests associated responses standardised. allows data sharing retrieval become reproducible programmatic aspect work.Standardisation: machine process data requests requires format requests associated responses standardised. allows data sharing retrieval become reproducible programmatic aspect work.","code":""},{"path":"data-apis.html","id":"what-is-an-api","chapter":"7 APIs","heading":"7.3 What is an API?","text":", APIs great, exactly ?human--human communication, set rules governing acceptable behaviour known etiquette. Depending live, social etiquette can rather strict. rules computer--computer communication take whole new level, machines can room left interpretation.set rules governing interactions computers programmes known protocol.APIs provide standard protocol different programs interact one another. makes easier developers build complex systems leveraging functionality existing services platforms. benefits working standardised modular way apply equally well sharing data writing code organising files.two sides communication machines communicate known server client.Servers can seem intimidating, unlike laptop mobile phone don’t input output devices; keyboard, monitor, mouse. Despite , servers just regular computers designed store data run programmes. Servers don’t input output devices intended used remotely, via another computer. need screen mouse user miles away. Nothing scary going !People often find clients much less intimidating - simply computer application might contact sever.","code":""},{"path":"data-apis.html","id":"http","chapter":"7 APIs","heading":"7.4 HTTP","text":"leads us one step rabbit-hole. API protocol defines rules applications communicate one another. communication happen?HTTP (Hypertext Transfer Protocol) dominant mode communication World Wide Web. can see secure version HTTP, HTTPS, start web addresses top browser. example:HTTP foundation data communication web used transfer files (text, images, videos) web servers clients.understand HTTP communications, find helpful imagine client server customer waiter restaurant. client makes request server, tries comply giving response. server might respond confirm request completed successfully. Alternatively, server might respond error message, (hopefully) informative request completed.request-response model basis HTTP, communication system used majority APIs.","code":"https://www.zakvarty.com/blog"},{"path":"data-apis.html","id":"http-requests","chapter":"7 APIs","heading":"7.5 HTTP Requests","text":"HTTP request consists :Uniform Resource Locator (URL) [unique identifier thing]Method [tells server type action requested client]Headers [meta-information request, e.g. device type]Body [Data client wants send server]","code":""},{"path":"data-apis.html","id":"url","chapter":"7 APIs","heading":"7.5.1 URL","text":"URL HTTP request specifies request going made, example http://example.com.","code":""},{"path":"data-apis.html","id":"method","chapter":"7 APIs","heading":"7.5.2 Method","text":"action client wants take indicated set well-defined methods HTTP verbs. common HTTP verbs GET, POST, PUT, PATCH, DELETE.GET verb used retrieve resource server, web page image. POST verb used send data server, submitting form uploading file. PUT verb used replace resource server new one, PATCH verb used update resource server without replacing entirely. Finally, DELETE verb used delete resource server.addition common HTTP verbs, also several less frequently used verbs. used specialized purposes, requesting headers resource, testing connectivity client server.","code":""},{"path":"data-apis.html","id":"header","chapter":"7 APIs","heading":"7.5.3 Header","text":"request headers contain meta-information request. information device type included within request.","code":""},{"path":"data-apis.html","id":"body","chapter":"7 APIs","heading":"7.5.4 Body","text":"Finally, body request contains data client providing server.","code":""},{"path":"data-apis.html","id":"http-responses","chapter":"7 APIs","heading":"7.6 HTTP Responses","text":"server receives request attempt fulfil send response back client.response similar structure request apart :responses URL,responses method,responses status code.","code":""},{"path":"data-apis.html","id":"status-codes","chapter":"7 APIs","heading":"7.6.1 Status Codes","text":"status code 3 digit number, specific meaning. common error codes might (already ) come across :200: Success,404: Page found (400s errors),503: Page .data science context, successful response return requested data within data field. likely given JSON XML format.","code":""},{"path":"data-apis.html","id":"authentication","chapter":"7 APIs","heading":"7.7 Authentication","text":"Now know applications communicate, might ask can control access API types request can make. can done server setting appropriate permissions client. server verify client really claims ?Authentication way ensure authorized clients able access API. typically done server requiring client provide secret information uniquely identifies , whenever make requests API. information allows API server validate authenticity user authorises request.","code":""},{"path":"data-apis.html","id":"basic-authentication","chapter":"7 APIs","heading":"7.7.1 Basic Authentication","text":"various ways implement API authentication.Basic authentication involves legitimate client username password. encrypted version included Authorization header HTTP request. hear matches server’s records request processed. , special status code (401) returned client.Basic authentication dangerous put restrictions client can authorised. Additional, individualised restrictions can added using alternative authentication scheme.","code":""},{"path":"data-apis.html","id":"api-key-authentication","chapter":"7 APIs","heading":"7.7.2 API Key Authentication","text":"API key long, random string letters numbers assigned authorised user. API key distinct user’s password keys typically issued service provides API. Using keys rather basic authentication allows API provider track limit usage API.example, provider may issue unique API key developer organization wants use API. provider can limit access certain data. also limit number requests key can make given time period prevent access certain administrative functions, like changing passwords deleting accounts.Unlike Basic Authentication, standard way client sharing key server. Depending API might Authorization field header, end URL (http://example.com?api_key=my_secret_key), within body data.","code":""},{"path":"data-apis.html","id":"api-wrappers","chapter":"7 APIs","heading":"7.8 API wrappers","text":"’ve learned lot internet works. Fortunately, lot time won’t worry new information debugging purposes.best case scenario, kind developer written “wrapper” function API. wrappers functions R construct HTTP request . particularly lucky, API wrapper also format response , converting XML JSON back R object ready immediate use.","code":""},{"path":"data-apis.html","id":"geonames-wrapper","chapter":"7 APIs","heading":"7.9 {geonames} wrapper","text":"rOpenSci curated list many wrappers accessing scientific data using R. focus GeoNames API, gives open access geographical database. access data, use wrapper functions provided {geonames} package.aim illustrate important steps getting started new API.","code":""},{"path":"data-apis.html","id":"set-up","chapter":"7 APIs","heading":"7.9.1 Set up","text":"can get data GeoNames API, first need little bit set .Install load {geonames} CRANCreate user account GeoNames APIActivate account (see activation email)Enable free web services GeoNames account logging link.Enable free web services GeoNames account logging link.Tell R credentials GeoNames.Tell R credentials GeoNames.use following code tell R credentials, absolutely .save username environment variable, also puts API credentials directly script. share script others (internally, externally publicly) sharing credentials . good!","code":"\n#install.packages(\"geonames\")\nlibrary(geonames)\noptions(geonamesUsername=\"example_username\")"},{"path":"data-apis.html","id":"keep-it-secret-keep-it-safe","chapter":"7 APIs","heading":"7.9.2 Keep it Secret, Keep it Safe","text":"solution problem add credentials environment variables .Rprofile rather script. .Rprofile R script run start every session. can created edited directly, can also created edited within R.make/open .Rprofile use edit_r_profile() function usethis package.Within file, add options(geonamesUsername=\"example_username\") new line, remembering replace example_username GeoNames username.final step check file ends blank line, save restart R. set start using {geonames}.set procedure indicative API wrappers, course details vary API. good documentation important!","code":"\nlibrary(usethis)\nusethis::edit_r_profile()"},{"path":"data-apis.html","id":"using-geonames","chapter":"7 APIs","heading":"7.9.3 Using {geonames}","text":"GeoNames whole host different geo-datasets can explore.\nfirst example, let’s get geo-tagged wikipedia articles within 1km Imperial College London.Looking structure imperial_neighbours can see data frame one row per geo-tagged wikipedia article.confirm correct location can inspect title first five neighbours.Nothing surprising , mainly departments college Exhibition Road, runs along one side campus. sorts check important - initially forgot minus longitude getting results East London!","code":"\nimperial_coords <- list(lat = 51.49876, lon = -0.1749)\nsearch_radius_km <- 1\n\nimperial_neighbours <- geonames::GNfindNearbyWikipedia(\n  lat = imperial_coords$lat,\n  lng = imperial_coords$lon, \n  radius = search_radius_km,\n  lang = \"en\",                # english language articles\n  maxRows = 500              # maximum number of results to return \n)\nstr(imperial_neighbours)\n#> 'data.frame':    204 obs. of  13 variables:\n#>  $ summary     : chr  \"The Department of Mechanical Engineering is responsible for teaching and research in mechanical engineering at \"| __truncated__ \"Imperial College Business School is a global business school located in London. The business school was opened \"| __truncated__ \"Exhibition Road is a street in South Kensington, London which is home to several major museums and academic est\"| __truncated__ \"Imperial College School of Medicine (ICSM) is the medical school of Imperial College London in England, and one\"| __truncated__ ...\n#>  $ elevation   : chr  \"20\" \"18\" \"19\" \"24\" ...\n#>  $ feature     : chr  \"edu\" \"edu\" \"landmark\" \"edu\" ...\n#>  $ lng         : chr  \"-0.1746\" \"-0.1748\" \"-0.17425\" \"-0.1757\" ...\n#>  $ distance    : chr  \"0.0335\" \"0.0494\" \"0.0508\" \"0.0558\" ...\n#>  $ rank        : chr  \"81\" \"91\" \"90\" \"96\" ...\n#>  $ lang        : chr  \"en\" \"en\" \"en\" \"en\" ...\n#>  $ title       : chr  \"Department of Mechanical Engineering, Imperial College London\" \"Imperial College Business School\" \"Exhibition Road\" \"Imperial College School of Medicine\" ...\n#>  $ lat         : chr  \"51.498524\" \"51.4992\" \"51.4989722222222\" \"51.4987\" ...\n#>  $ wikipediaUrl: chr  \"en.wikipedia.org/wiki/Department_of_Mechanical_Engineering%2C_Imperial_College_London\" \"en.wikipedia.org/wiki/Imperial_College_Business_School\" \"en.wikipedia.org/wiki/Exhibition_Road\" \"en.wikipedia.org/wiki/Imperial_College_School_of_Medicine\" ...\n#>  $ countryCode : chr  NA \"AE\" NA \"GB\" ...\n#>  $ thumbnailImg: chr  NA NA NA NA ...\n#>  $ geoNameId   : chr  NA NA NA NA ...\nimperial_neighbours$title[1:5]\n#> [1] \"Department of Mechanical Engineering, Imperial College London\"             \n#> [2] \"Imperial College Business School\"                                          \n#> [3] \"Exhibition Road\"                                                           \n#> [4] \"Imperial College School of Medicine\"                                       \n#> [5] \"Department of Civil and Environmental Engineering, Imperial College London\""},{"path":"data-apis.html","id":"what-if-there-is-no-wrapper","chapter":"7 APIs","heading":"7.10 What if there is no wrapper?","text":"wrapper function, can still access APIs fairly easilty using httr package.look example using OMDb, open source version IMDb, get information movie Mean Girls.use OMDB API need request free API key, follow verification link add API key .Rprofile.can restart R safely access API key within R session.Using documentation API, requests URLs following form, terms angular brackets replaced .little bit effort, can write function composes type request URL us. using glue package help us join strings together.Running example get:can use httr package construct request store response get.Thankfully success! get 401 error code , check clicked activation link API key.full structure response quite complicated, can easily extract requested data using content()","code":"\n# Add this to .Rprofile, pasting in your own API key\noptions(OMDB_API_Key = \"PASTE YOUR KEY HERE\")\n# Load your API key into the current R session,\nombd_api_key <- getOption(\"OMDB_API_Key\")http://www.omdbapi.com/?t=<TITLE>&y=<YEAR>&plot=<LENGTH>&r=<FORMAT>&apikey=<API_KEY>\n\n#' Compose search requests for the OMBD API\n#'\n#' @param title String defining title to search for. Words are separated by \"+\".\n#' @param year String defining release year to search for\n#' @param plot String defining whether \"short\" or \"full\" plot is returned\n#' @param format String defining return format. One of \"json\" or \"xml\"\n#' @param api_key String defining your OMDb API key.\n#'\n#' @return String giving a OMBD search request URL\n#'\n#' @examples \n#' omdb_url(\"mean+girls\", \"2004\", \"short\", \"json\", getOption(OMBD_API_Key))\n#' \nomdb_url <- function(title, year, plot, format, api_key) {\n  glue::glue(\"http://www.omdbapi.com/?t={title}&y={year}&plot={plot}&r={format}&apikey={api_key}\")\n}\nmean_girls_request <- omdb_url(\n  title = \"mean+girls\",\n  year =  \"2004\",\n  plot = \"short\",\n  format =  \"json\",\n  api_key =  getOption(\"OMDB_API_Key\"))\nresponse <- httr::GET(url = mean_girls_request)\nhttr::status_code(response)\n#> [1] 200\nhttr::content(response)\n#> $Title\n#> [1] \"Mean Girls\"\n#> \n#> $Year\n#> [1] \"2004\"\n#> \n#> $Rated\n#> [1] \"PG-13\"\n#> \n#> $Released\n#> [1] \"30 Apr 2004\"\n#> \n#> $Runtime\n#> [1] \"97 min\"\n#> \n#> $Genre\n#> [1] \"Comedy\"\n#> \n#> $Director\n#> [1] \"Mark Waters\"\n#> \n#> $Writer\n#> [1] \"Rosalind Wiseman, Tina Fey\"\n#> \n#> $Actors\n#> [1] \"Lindsay Lohan, Jonathan Bennett, Rachel McAdams\"\n#> \n#> $Plot\n#> [1] \"Cady Heron is a hit with The Plastics, the A-list girl clique at her new school, until she makes the mistake of falling for Aaron Samuels, the ex-boyfriend of alpha Plastic Regina George.\"\n#> \n#> $Language\n#> [1] \"English, German, Vietnamese, Swahili\"\n#> \n#> $Country\n#> [1] \"United States, Canada\"\n#> \n#> $Awards\n#> [1] \"7 wins & 25 nominations\"\n#> \n#> $Poster\n#> [1] \"https://m.media-amazon.com/images/M/MV5BMjE1MDQ4MjI1OV5BMl5BanBnXkFtZTcwNzcwODAzMw@@._V1_SX300.jpg\"\n#> \n#> $Ratings\n#> $Ratings[[1]]\n#> $Ratings[[1]]$Source\n#> [1] \"Internet Movie Database\"\n#> \n#> $Ratings[[1]]$Value\n#> [1] \"7.1/10\"\n#> \n#> \n#> $Ratings[[2]]\n#> $Ratings[[2]]$Source\n#> [1] \"Rotten Tomatoes\"\n#> \n#> $Ratings[[2]]$Value\n#> [1] \"84%\"\n#> \n#> \n#> $Ratings[[3]]\n#> $Ratings[[3]]$Source\n#> [1] \"Metacritic\"\n#> \n#> $Ratings[[3]]$Value\n#> [1] \"66/100\"\n#> \n#> \n#> \n#> $Metascore\n#> [1] \"66\"\n#> \n#> $imdbRating\n#> [1] \"7.1\"\n#> \n#> $imdbVotes\n#> [1] \"385,107\"\n#> \n#> $imdbID\n#> [1] \"tt0377092\"\n#> \n#> $Type\n#> [1] \"movie\"\n#> \n#> $DVD\n#> [1] \"21 Sep 2004\"\n#> \n#> $BoxOffice\n#> [1] \"$86,058,055\"\n#> \n#> $Production\n#> [1] \"N/A\"\n#> \n#> $Website\n#> [1] \"N/A\"\n#> \n#> $Response\n#> [1] \"True\""},{"path":"data-apis.html","id":"wrapping-up-2","chapter":"7 APIs","heading":"7.11 Wrapping up","text":"learned bit internet works, benefits using API share data request data Open APIs.obtaining data internet ’s vital keep credentials safe, don’t work needed.Keep API keys code. Store .Rprofile (make sure version control!)Keep API keys code. Store .Rprofile (make sure version control!)Scraping always last resort. API already?Scraping always last resort. API already?Writing code access API can painful necessary.Writing code access API can painful necessary.Don’t repeat people, suitable wrapper exists use .Don’t repeat people, suitable wrapper exists use .","code":""},{"path":"data-checklist.html","id":"data-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"data-checklist.html","id":"videos-chapters-1","chapter":"Checklist","heading":"Videos / Chapters","text":"Tabular Data (27 min) [slides] [https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/02-01-tabular-data--csvs/02-01-tabular-data.pdf]Tabular Data (27 min) [slides] [https://github.com/zakvarty/effective-data-science-slides-2022/raw/main/02-01-tabular-data--csvs/02-01-tabular-data.pdf]Web Scraping (22 min) [slides] [slides]Web Scraping (22 min) [slides] [slides]APIs (25 min) [slides] [slides]APIs (25 min) [slides] [slides]","code":""},{"path":"data-checklist.html","id":"reading-1","chapter":"Checklist","heading":"Reading","text":"Use Acquiring Sharing Data section reading list support guide exploration week’s topics. Note texts divided core reading, reference materials materials interest.","code":""},{"path":"data-checklist.html","id":"tasks-1","chapter":"Checklist","heading":"Tasks","text":"Core:Live SessionIn live session …","code":""},{"path":"edav-introduction.html","id":"edav-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-wrangling.html","id":"edav-wrangling","chapter":"9 Data Wrangling","heading":"9 Data Wrangling","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-wrangling.html","id":"what-is-data-wrangling","chapter":"9 Data Wrangling","heading":"9.1 What is Data Wrangling?","text":"Okay, ’ve got data. ’s great start!might handed collaborator, requested via API scraped raw html webpage. worst case scenario, ’re actual scientist (just data one) spent last several months life painstakingly measuring flower petals car parts. Now really want something useful data.’ve seen already can load data R pivot wider longer formats, probably isn’t enough satisfy curiosity. want able view data, manipulate subset , create new variables existing ones cross-reference dataset others. things possible R known various collective names including data manipulation, data munging data wrangling.’ve decided use term data wrangling . ’s data manipulation sounds boring heck data munging unpleasant say makes imagine squelching sort information swamp.follows, ’ll give fly-tour tools data wrangling R, showing examples along way. ’ll focus common useful operations link extensive guides wrangling data R, can refer back need .","code":""},{"path":"edav-wrangling.html","id":"example-data-sets","chapter":"9 Data Wrangling","heading":"9.2 Example Data Sets","text":"demonstrate standard skills use two datasets. mtcars data comes built R installation. second data set look penguins data palmerpenguins.","code":"\nlibrary(palmerpenguins)\npengins <- palmerpenguins::penguins\ncars <- datasets::mtcars"},{"path":"edav-wrangling.html","id":"viewing-your-data","chapter":"9 Data Wrangling","heading":"9.3 Viewing Your Data","text":"","code":""},{"path":"edav-wrangling.html","id":"view","chapter":"9 Data Wrangling","heading":"9.3.1 View()","text":"View() function can used create spreadsheet-like view data. RStudio open new tab.View() work “matrix-like” R object, tibble, data frame, vector matrix. Note capital letter - function called View(), view().Screenshot RStduio files pane, containg spreadsheet view palmer penguins data set.","code":"\nView(penguins)"},{"path":"edav-wrangling.html","id":"head","chapter":"9 Data Wrangling","heading":"9.3.2 head()","text":"large data sets, might want (able ) view . can use head() view first rows. integer argument n specifies number rows like return.","code":"\nhead(x = pengins, n = 3)\n#> # A tibble: 3 × 8\n#>   species island    bill_length_mm bill_depth_mm flippe…¹ body_…² sex    year\n#>   <fct>   <fct>              <dbl>         <dbl>    <int>   <int> <fct> <int>\n#> 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007\n#> 2 Adelie  Torgersen           39.5          17.4      186    3800 fema…  2007\n#> 3 Adelie  Torgersen           40.3          18        195    3250 fema…  2007\n#> # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g"},{"path":"edav-wrangling.html","id":"str","chapter":"9 Data Wrangling","heading":"9.3.3 str()","text":"alternative way view large data set, one complicated format examine structure str(). useful way inspect structure list-like objects, particularly ’ve got nested structure.","code":"\nstr(penguins)\n#> tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n#>  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#>  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n#>  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n#>  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n#>  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n#>  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n#>  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ..."},{"path":"edav-wrangling.html","id":"names","chapter":"9 Data Wrangling","heading":"9.3.4 names()","text":"Finally, just want access variable names can names() function base R.Similarly, can explicitly access row column names data frame tibble using colnames() rownames().cars data, car model stored row names. doesn’t really jive idea tidy data - ’ll see fix shortly.","code":"\nnames(penguins)\n#> [1] \"species\"           \"island\"            \"bill_length_mm\"   \n#> [4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n#> [7] \"sex\"               \"year\"\ncolnames(cars)\n#>  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n#> [11] \"carb\"\nrownames(cars)\n#>  [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n#>  [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n#>  [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n#> [10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n#> [13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n#> [16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n#> [19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n#> [22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n#> [25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n#> [28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n#> [31] \"Maserati Bora\"       \"Volvo 142E\""},{"path":"edav-wrangling.html","id":"renaming-variables","chapter":"9 Data Wrangling","heading":"9.4 Renaming Variables","text":"","code":""},{"path":"edav-wrangling.html","id":"colnames","chapter":"9 Data Wrangling","heading":"9.4.1 colnames()","text":"function colnames() can used set, well retrieve, column names.","code":"\ncars_renamed <- cars \ncolnames(cars_renamed)[1] <- \"miles_per_gallon\"\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cyl\"              \"disp\"            \n#>  [4] \"hp\"               \"drat\"             \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\""},{"path":"edav-wrangling.html","id":"dplyrrename","chapter":"9 Data Wrangling","heading":"9.4.2 dplyr::rename()","text":"can also use functions dplyr rename columns. Let’s alter second column name.done part pipe, making many alterations.using dplyr function remember format new_name = old_name. matches format used create data frame tibble, opposite order python function name often catches people .section creating new variables, see alternative way copying column deleting original.","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\ncars_renamed <- rename(.data = cars_renamed, cylinders = cyl)\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cylinders\"        \"disp\"            \n#>  [4] \"hp\"               \"drat\"             \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\"\ncars_renamed <- cars_renamed %>% \n  rename(displacement = disp) %>% \n  rename(horse_power = hp) %>% \n  rename(rear_axel_ratio = drat)\n\ncolnames(cars_renamed)\n#>  [1] \"miles_per_gallon\" \"cylinders\"        \"displacement\"    \n#>  [4] \"horse_power\"      \"rear_axel_ratio\"  \"wt\"              \n#>  [7] \"qsec\"             \"vs\"               \"am\"              \n#> [10] \"gear\"             \"carb\""},{"path":"edav-wrangling.html","id":"subsetting-1","chapter":"9 Data Wrangling","heading":"9.5 Subsetting","text":"","code":""},{"path":"edav-wrangling.html","id":"base-r-1","chapter":"9 Data Wrangling","heading":"9.5.1 Base R","text":"base R can extract rows, columns combinations thereof using index notation.Using negative indexing can remove rows columnsYou can also select rows columns names. can done using bracket syntax ([ ]) dollar syntax ($).Since penguins tibble, return different types object. Subsetting tibble bracket syntax return tibble, extracting column using dollar syntax returns vector values.","code":"\n# First row\npenguins[1, ]\n#> # A tibble: 1 × 8\n#>   species island    bill_length_mm bill_depth_mm flippe…¹ body_…² sex    year\n#>   <fct>   <fct>              <dbl>         <dbl>    <int>   <int> <fct> <int>\n#> 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007\n#> # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n# First Column \npenguins[ , 1]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # … with 338 more rows\n\n# Rows 2-3 of columns 1, 2 and 4\npenguins[2:3, c(1, 2, 4)]\n#> # A tibble: 2 × 3\n#>   species island    bill_depth_mm\n#>   <fct>   <fct>             <dbl>\n#> 1 Adelie  Torgersen          17.4\n#> 2 Adelie  Torgersen          18\n# Drop all but first row\npenguins[-(2:344), ]\n#> # A tibble: 1 × 8\n#>   species island    bill_length_mm bill_depth_mm flippe…¹ body_…² sex    year\n#>   <fct>   <fct>              <dbl>         <dbl>    <int>   <int> <fct> <int>\n#> 1 Adelie  Torgersen           39.1          18.7      181    3750 male   2007\n#> # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n# Drop all but first column \npenguins[ , -(2:8)]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # … with 338 more rows\npengins[ , \"species\"]\n#> # A tibble: 344 × 1\n#>   species\n#>   <fct>  \n#> 1 Adelie \n#> 2 Adelie \n#> 3 Adelie \n#> 4 Adelie \n#> 5 Adelie \n#> 6 Adelie \n#> # … with 338 more rows\npenguins$species\n#>   [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>   [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#>  [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n#> [148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n#> [155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n#> [274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n#> [281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n#> [344] Chinstrap\n#> Levels: Adelie Chinstrap Gentoo"},{"path":"edav-wrangling.html","id":"filter-and-select","chapter":"9 Data Wrangling","heading":"9.5.2 filter() and select()","text":"dplyr two functions subsetting, filter() subsets rows select() subsets column.functions list like retain. Filter select calls can piped together subset based row column values.Subsetting rows can inverted negating filter() statementand dropping columns can done selecting columns except one(s) want drop.","code":"\npenguins %>% \n  select(species, island, body_mass_g)\n#> # A tibble: 344 × 3\n#>   species island    body_mass_g\n#>   <fct>   <fct>           <int>\n#> 1 Adelie  Torgersen        3750\n#> 2 Adelie  Torgersen        3800\n#> 3 Adelie  Torgersen        3250\n#> 4 Adelie  Torgersen          NA\n#> 5 Adelie  Torgersen        3450\n#> 6 Adelie  Torgersen        3650\n#> # … with 338 more rows\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(body_mass_g > 6000)\n#> # A tibble: 2 × 3\n#>   species island body_mass_g\n#>   <fct>   <fct>        <int>\n#> 1 Gentoo  Biscoe        6300\n#> 2 Gentoo  Biscoe        6050\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(!(body_mass_g > 6000))\n#> # A tibble: 340 × 3\n#>   species island    body_mass_g\n#>   <fct>   <fct>           <int>\n#> 1 Adelie  Torgersen        3750\n#> 2 Adelie  Torgersen        3800\n#> 3 Adelie  Torgersen        3250\n#> 4 Adelie  Torgersen        3450\n#> 5 Adelie  Torgersen        3650\n#> 6 Adelie  Torgersen        3625\n#> # … with 334 more rows\npenguins %>% \n  select(species, island, body_mass_g) %>% \n  filter(!(body_mass_g > 6000)) %>% \n  select(!c(species, island))\n#> # A tibble: 340 × 1\n#>   body_mass_g\n#>         <int>\n#> 1        3750\n#> 2        3800\n#> 3        3250\n#> 4        3450\n#> 5        3650\n#> 6        3625\n#> # … with 334 more rows"},{"path":"edav-wrangling.html","id":"creating-new-variables","chapter":"9 Data Wrangling","heading":"9.6 Creating New Variables","text":"","code":""},{"path":"edav-wrangling.html","id":"base-r-2","chapter":"9 Data Wrangling","heading":"9.6.1 Base R","text":"can create new variables base R assigning vector correct length new column name.drop original column data frame, gives us alternative way renaming columns.One thing aware operation preserve column ordering.Generally speaking, code relies columns specific order fragile - breaks easily. possible, try write code another way ’s robust column reordering. ’ve done removing wt column looking column index part code, rather assuming always fourth column.","code":"\ncars_renamed$weight <- cars_renamed$wt\ncars_renamed <- cars_renamed[ ,-which(names(cars_renamed) == \"wt\")]\nhead(cars_renamed, n = 5)\n#>                   miles_per_gallon cylinders displacement horse_power\n#> Mazda RX4                     21.0         6          160         110\n#> Mazda RX4 Wag                 21.0         6          160         110\n#> Datsun 710                    22.8         4          108          93\n#> Hornet 4 Drive                21.4         6          258         110\n#> Hornet Sportabout             18.7         8          360         175\n#>                   rear_axel_ratio  qsec vs am gear carb weight\n#> Mazda RX4                    3.90 16.46  0  1    4    4  2.620\n#> Mazda RX4 Wag                3.90 17.02  0  1    4    4  2.875\n#> Datsun 710                   3.85 18.61  1  1    4    1  2.320\n#> Hornet 4 Drive               3.08 19.44  1  0    3    1  3.215\n#> Hornet Sportabout            3.15 17.02  0  0    3    2  3.440"},{"path":"edav-wrangling.html","id":"dplyrmutate","chapter":"9 Data Wrangling","heading":"9.6.2 dplyr::mutate()","text":"function dplyr create new columns mutate(). Let’s create another column car’s weight kilogrammes rather tonnes.can also create new columns functions multiple columns.","code":"\ncars_renamed <- cars_renamed %>% \n  mutate(weight_kg = weight * 1000)\n\ncars_renamed %>% \n  select(miles_per_gallon, cylinders, displacement, weight, weight_kg) %>% \n  head(n = 5)\n#>                   miles_per_gallon cylinders displacement weight weight_kg\n#> Mazda RX4                     21.0         6          160  2.620      2620\n#> Mazda RX4 Wag                 21.0         6          160  2.875      2875\n#> Datsun 710                    22.8         4          108  2.320      2320\n#> Hornet 4 Drive                21.4         6          258  3.215      3215\n#> Hornet Sportabout             18.7         8          360  3.440      3440\ncars_renamed <- cars_renamed %>% \n  mutate(cylinder_adjusted_mpg = miles_per_gallon / cylinders)"},{"path":"edav-wrangling.html","id":"rownames_to_column","chapter":"9 Data Wrangling","heading":"9.6.3 rownames_to_column()","text":"One useful example adding additional row data frame convert row names column data fame.’s neat function called rownames_to_column() tibble add first column remove row names one step.","code":"\ncars %>% \n  mutate(model = rownames(cars_renamed)) %>% \n  select(mpg, cyl, model) %>% \n  head(n = 5)\n#>                    mpg cyl             model\n#> Mazda RX4         21.0   6         Mazda RX4\n#> Mazda RX4 Wag     21.0   6     Mazda RX4 Wag\n#> Datsun 710        22.8   4        Datsun 710\n#> Hornet 4 Drive    21.4   6    Hornet 4 Drive\n#> Hornet Sportabout 18.7   8 Hornet Sportabout\ncars %>% \n  tibble::rownames_to_column(var = \"model\") %>% \n  head(n = 5)\n#>               model  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"},{"path":"edav-wrangling.html","id":"rowids_to_column","chapter":"9 Data Wrangling","heading":"9.6.4 rowids_to_column()","text":"Another function tibble adds row id observation new column. often useful ordering combining tables.","code":"\ncars %>% \n  tibble::rowid_to_column(var = \"row_id\") %>% \n  head(n = 5)\n#>   row_id  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> 1      1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> 2      2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> 3      3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> 4      4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> 5      5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2"},{"path":"edav-wrangling.html","id":"summaries","chapter":"9 Data Wrangling","heading":"9.7 Summaries","text":"summarise() function allows collapse data frame single row, using summary statistic choosing.can calculate average bill length penguins single summarise() function call.Since missing values, might instead want calculate mean recorded values.can also use summarise() gather multiple summaries single data frame., isn’t overly exciting. might, rightly, wonder ’d want use summarise() calls just use simpler base R calls directly.One benefit summarise calls ensure consistent output. However, main advantage comes want apply summaries distinct subgroups data.","code":"\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm))\n#> # A tibble: 1 × 1\n#>   average_bill_length_mm\n#>                    <dbl>\n#> 1                     NA\nsummarise(penguins, average_bill_length_mm = mean(bill_length_mm, na.rm = TRUE))\n#> # A tibble: 1 × 1\n#>   average_bill_length_mm\n#>                    <dbl>\n#> 1                   43.9\nbill_length_mm_summary <- penguins %>% \n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n\nbill_length_mm_summary\n#> # A tibble: 1 × 8\n#>    mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1  43.9   44.4  59.6  32.1  39.2  44.4  39.2  59.6"},{"path":"edav-wrangling.html","id":"grouped-operations","chapter":"9 Data Wrangling","heading":"9.8 Grouped Operations","text":"real benefit summarise() comes combination group_by(). allows calculate summary statistics level factor one additional line code. ’re re-calculating set summary statistics just found penguins, individual species.can group multiple factors calculate summaries distinct combination levels within data set. group combinations species island belong.","code":"\npenguins %>% \n  group_by(species) %>%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n#> # A tibble: 3 × 9\n#>   species    mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie     38.8   38.8  46    32.1  36.8  38.8  36.8  46  \n#> 2 Chinstrap  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 3 Gentoo     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\npenguin_summary_stats <- penguins %>% \n  group_by(species, island) %>%\n  summarise(\n    mean = mean(bill_length_mm, na.rm = TRUE),\n    median = median(bill_length_mm, na.rm = TRUE),\n    min = max(bill_length_mm, na.rm = TRUE),\n    q_0 = min(bill_length_mm, na.rm = TRUE),\n    q_1 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_2 = median(bill_length_mm, na.rm = TRUE),\n    q_3 = quantile(bill_length_mm, prob = 0.25, na.rm = TRUE),\n    q_4 = max(bill_length_mm, na.rm = TRUE))\n#> `summarise()` has grouped output by 'species'. You can override using the\n#> `.groups` argument.\n\npenguin_summary_stats\n#> # A tibble: 5 × 10\n#> # Groups:   species [3]\n#>   species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n#> 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n#> 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n#> 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6"},{"path":"edav-wrangling.html","id":"ungrouping","chapter":"9 Data Wrangling","heading":"9.8.1 Ungrouping","text":"default, call summarise() undo one level grouping. means previous result still grouped species.(can see tibble output , also examining structure returned data frame. tells us S3 object class “grouped_df”, inherits properties “tbl_df”, “tbl”, “data.frame” objects.)Since grouped two variables, R expects us use two summaries returning data frame (tibble) grouped. One way satisfy use apply second summary species level grouping.However, won’t always want apply another summary. case, can undo grouping using ungroup(). Remembering ungroup common gotcha cause confusion working multiple-group summaries.’s alternative method achieve thing single step using dplyr versions 1.0.0 . set .groups parameter summarise() function call, determines grouping returned data frame..groups parameter can take 4 possible values:“drop_last”: dropping last level grouping (option v1.0.0);“drop_last”: dropping last level grouping (option v1.0.0);“drop”: levels grouping dropped;“drop”: levels grouping dropped;“keep”: grouping structure .data;“keep”: grouping structure .data;“rowwise”: row group.“rowwise”: row group.default, “drop_last” used results 1 row “keep” used otherwise.","code":"\nclass(penguin_summary_stats)\n#> [1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\npenguin_summary_stats %>% \n  summarise_all(mean, na.rm = TRUE)\n#> # A tibble: 3 × 10\n#>   species   island  mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>      <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie        NA  38.8   38.7  45.2  33.4  37.0  38.7  37.0  45.2\n#> 2 Chinstrap     NA  48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 3 Gentoo        NA  47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6\nungroup(penguin_summary_stats)\n#> # A tibble: 5 × 10\n#>   species   island     mean median   min   q_0   q_1   q_2   q_3   q_4\n#>   <fct>     <fct>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 Adelie    Biscoe     39.0   38.7  45.6  34.5  37.7  38.7  37.7  45.6\n#> 2 Adelie    Dream      38.5   38.6  44.1  32.1  36.8  38.6  36.8  44.1\n#> 3 Adelie    Torgersen  39.0   38.9  46    33.5  36.7  38.9  36.7  46  \n#> 4 Chinstrap Dream      48.8   49.6  58    40.9  46.3  49.6  46.3  58  \n#> 5 Gentoo    Biscoe     47.5   47.3  59.6  40.9  45.3  47.3  45.3  59.6"},{"path":"edav-wrangling.html","id":"reordering-factors","chapter":"9 Data Wrangling","heading":"9.9 Reordering Factors","text":"R stored factors integer values, maps set labels. factor levels appear data assigned coded integer value mapping factor levels integers depend order labels appear data.can annoying, particularly factor levels relate properties aren’t numerical inherent ordering . example , t-shirt size twelve people.Irritatingly, sizes aren’t order extra large isn’t included ’s included particular sample. leads awkward looking summary tables plots.can fix creating new variable factors explicitly coded correct order. also need specify drop empty groups part group_by().","code":"\ntshirts <- tibble::tibble(\n  id = 1:12, \n  size = as.factor(c(\"L\", NA, \"M\", \"S\", \"XS\", \"M\", \"XXL\", \"L\", \"XS\", \"M\", \"L\", \"S\"))\n)\n\nlevels(tshirts$size)\n#> [1] \"L\"   \"M\"   \"S\"   \"XS\"  \"XXL\"\ntshirts %>% group_by(size) %>% summarise(count = n())\n#> # A tibble: 6 × 2\n#>   size  count\n#>   <fct> <int>\n#> 1 L         3\n#> 2 M         3\n#> 3 S         2\n#> 4 XS        2\n#> 5 XXL       1\n#> 6 <NA>      1\ntidy_tshirt_levels <- c(\"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\", NA)\n\ntshirts %>% \n  mutate(size_tidy = factor(size, levels = tidy_tshirt_levels)) %>% \n  group_by(size_tidy, .drop = FALSE ) %>% \n  summarise(count = n())\n#> # A tibble: 7 × 2\n#>   size_tidy count\n#>   <fct>     <int>\n#> 1 XS            2\n#> 2 S             2\n#> 3 M             3\n#> 4 L             3\n#> 5 XL            0\n#> 6 XXL           1\n#> # … with 1 more row"},{"path":"edav-wrangling.html","id":"be-aware-factors","chapter":"9 Data Wrangling","heading":"9.10 Be Aware: Factors","text":"seen little already, categorical variables can cause issues wrangling presenting data R. problems solvable using base R techniques forcats package provides tools common problems. includes functions changing order factor levels values associated.examples functions package include:fct_reorder(): Reordering factor another variable.fct_infreq(): Reordering factor frequency values.fct_relevel(): Changing order factor hand.fct_lump(): Collapsing least/frequent values factor “”.Examples can found forcats vignette factors chapter R data science.","code":""},{"path":"edav-wrangling.html","id":"be-aware-strings","chapter":"9 Data Wrangling","heading":"9.11 Be Aware: Strings","text":"Working analysing text data skill unto . However, useful able basic manipulation character strings programatically.R developed statistical programming language, well suited computational modelling aspects working text data base R string manipulation functions can bit unwieldy times.stringr package aims combat providing useful helper functions range text management problems. Even analysing text data can useful, example remove prefixes lot column names.Suppose wanted keep text following underscore column names. using regular expression extract lower-case upper-case letters follow underscore.Alternatively, can avoid using regular expressions. can split column name underscore keep second part string., unless plan work extensively text data, recommend look string manipulations need . strings section R Data Science useful starting point.","code":"#> Warning: `as_data_frame()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n#> Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#> `.name_repair` is omitted as of tibble 2.0.0.\n#> ℹ Using compatibility `.name_repair`.\n#> ℹ The deprecated feature was likely used in the tibble package.\n#>   Please report the issue at < ]8;;https://github.com/tidyverse/tibble/issueshttps://github.com/tidyverse/tibble/issues ]8;;>.\nhead(poorly_named_df)\n#> # A tibble: 6 × 11\n#>   observ…¹   V1_A   V2_B    V3_C    V4_D    V5_E   V6_F   V7_G   V8_H    V9_I\n#>      <int>  <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n#> 1        1  0.449 -0.851 -0.835  -0.272   1.32    1.02   1.30  0.0498  2.15  \n#> 2        2 -0.528 -0.414 -0.0407  0.439   0.765  -1.39   1.11  1.54    0.684 \n#> 3        3  1.41  -2.40  -1.39   -0.0251  1.60   -0.722 -0.140 3.04    2.01  \n#> 4        4 -0.296 -0.679 -1.81    0.918   0.748  -1.01   2.75  2.82   -1.33  \n#> 5        5 -0.191 -1.00  -0.880  -0.385   0.257   0.939  0.196 0.877   0.0919\n#> 6        6  2.29   0.851 -1.42   -0.0618 -0.0646  0.117  0.124 1.47    0.416 \n#> # … with 1 more variable: V10_J <dbl>, and abbreviated variable name\n#> #   ¹​observation_id\nstringr::str_extract(names(poorly_named_df), pattern = \"(?<=_)([a-zA-Z]+)\")\n#>  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\"\n# split column names at underscores and inspect structure of resuting object\nsplit_strings <- stringr::str_split(names(poorly_named_df), pattern = \"_\")\nstr(split_strings)\n#> List of 11\n#>  $ : chr [1:2] \"observation\" \"id\"\n#>  $ : chr [1:2] \"V1\" \"A\"\n#>  $ : chr [1:2] \"V2\" \"B\"\n#>  $ : chr [1:2] \"V3\" \"C\"\n#>  $ : chr [1:2] \"V4\" \"D\"\n#>  $ : chr [1:2] \"V5\" \"E\"\n#>  $ : chr [1:2] \"V6\" \"F\"\n#>  $ : chr [1:2] \"V7\" \"G\"\n#>  $ : chr [1:2] \"V8\" \"H\"\n#>  $ : chr [1:2] \"V9\" \"I\"\n#>  $ : chr [1:2] \"V10\" \"J\"\n\n# keep only the second element of each character vector in the list\npurrr::map_chr(split_strings, function(x){x[2]})\n#>  [1] \"id\" \"A\"  \"B\"  \"C\"  \"D\"  \"E\"  \"F\"  \"G\"  \"H\"  \"I\"  \"J\""},{"path":"edav-wrangling.html","id":"be-aware-date-times","chapter":"9 Data Wrangling","heading":"9.12 Be Aware: Date-Times","text":"Remember fuss made storing dates ISO standard format? dates times complicated enough work adding extra ambiguity.\\[ \\text{YYYY} - \\text{MM} - \\text{DD}\\]\nDates, times time intervals reconcile two factors: physical orbit Earth around Sun social geopolitical mechanisms determine measure record passing time. makes history date time records fascinating can make working type data complicated.Moving larger smaller time spans: leap years alter number days year, months variable length (February’s length changing year year). data measured place uses daylight saving, one day year 23 hours long another 25 hours long. make things worse, dates hour clocks change uniform across countries, might distinct time zones change time.Even level minutes seconds aren’t safe - since Earth’s orbit gradually slowing leap second added approximately every 21 months. things better looking longer time scales across cultures, might account different calendars: months added removed altered time, calendar systems still take different approaches measuring time using different units origin points.issues careful working date time data. Functions help can found lubridate package, examples dates times chapter R data science.","code":""},{"path":"edav-wrangling.html","id":"be-aware-relational-data","chapter":"9 Data Wrangling","heading":"9.13 Be Aware: Relational Data","text":"data need stored across two data frames need able cross-reference match values observational unit. sort data know relational data, used extensively data science.variables use match observational units across data frames known keys. primary key belongs first table foreign key belongs secondary table. various ways join data frames, depending want retain.","code":""},{"path":"edav-wrangling.html","id":"join-types","chapter":"9 Data Wrangling","heading":"9.13.0.1 Join types","text":"might want keep observational units key variables values data frames, known inner join.Inner join diagram. Source: R Data ScienceYou might instead want keep units primary table pad NAs corresponding foreign key second table. results (outer) left-join.Diagram left, right outer joins. Source: R Data ScienceConversely, might keep units second table pad NAs corresponding foreign key primary table. imaginatively named (outer) right-join.(outer) full join, observational units either table retained missing values padded NAs.Things get complicated keys don’t uniquely identify observational units either one tables. ’d recommend start exploring ideas relational data chapter R Data Science.","code":""},{"path":"edav-wrangling.html","id":"why-and-where-to-learn-more","chapter":"9 Data Wrangling","heading":"9.13.0.2 Why and where to learn more","text":"Working relational data essential getting data science running wilds reality. businesses companies don’t store data huge single csv file. one isn’t efficient, cells empty. Secondly, ’s secure approach, since can’t grant partial access data. ’s information usually stored many data frames (generically known tables) within one databases.data silos created, maintained, accessed destroyed using relational data base management system. management systems use code manage access stored data, just like seen dplyr commands . might well heard SQL programming language (many variants), popular language data base management inspiration dplyr package verbs.’d like learn many excellent introductory SQL books courses, ’d recommend picking one focuses data analysis data science unless really want dig efficient storage querying databases.","code":""},{"path":"edav-wrangling.html","id":"wrapping-up-3","chapter":"9 Data Wrangling","heading":"9.14 Wrapping up","text":":Learned wrangle tabular data R dplyrLearned wrangle tabular data R dplyrMet idea relational data dplyr’s relationship SQLMet idea relational data dplyr’s relationship SQLBecome aware tricky data types packages can help.Become aware tricky data types packages can help.","code":""},{"path":"edav-analysis.html","id":"edav-analysis","chapter":"10 Exploratory Data Analysis","heading":"10 Exploratory Data Analysis","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-visualisation.html","id":"edav-visualisation","chapter":"11 Data Visualisation","heading":"11 Data Visualisation","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-checklist.html","id":"edav-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"edav-checklist.html","id":"videos-chapters-2","chapter":"Checklist","heading":"11.1 Videos / Chapters","text":"[ ][ ][ ][ ][ ][ ]","code":""},{"path":"edav-checklist.html","id":"reading-2","chapter":"Checklist","heading":"11.2 Reading","text":"","code":""},{"path":"edav-checklist.html","id":"activities","chapter":"Checklist","heading":"11.3 Activities","text":"","code":""},{"path":"edav-checklist.html","id":"live-session-2","chapter":"Checklist","heading":"11.4 Live Session","text":"","code":""},{"path":"production-introduction.html","id":"production-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-reproducibility.html","id":"production-reproducibility","chapter":"12 Reproducibility","heading":"12 Reproducibility","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-reproducibility.html","id":"reproducibility-and-replicability","chapter":"12 Reproducibility","heading":"12.1 Reproducibility and Replicability —————————–","text":"Write chapter following structure articlehttps://www.kdnuggets.com/2019/11/reproducibility-replicability-data-science.htmlhttps://www.kdnuggets.com/2019/11/reproducibility-replicability-data-science.htmlThe Ethical Algorithm M Kearns Roth (Chapter 4) https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591&context=L&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Local%20Search%20Engine&tab=Everything&query=,contains,kearns%20and%20roth&mode=BasicThe Ethical Algorithm M Kearns Roth (Chapter 4) https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=alma991000531083101591&context=L&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Local%20Search%20Engine&tab=Everything&query=,contains,kearns%20and%20roth&mode=BasicThe ASA Statement \\(p\\)-values: Context, Process Purpose https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1080_00031305_2016_1154108&context=PC&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Primo%20Central&tab=Everything&query=,contains,ASA%20p-value&offset=0The ASA Statement \\(p\\)-values: Context, Process Purpose https://library-search.imperial.ac.uk/discovery/fulldisplay?docid=cdi_informaworld_taylorfrancis_310_1080_00031305_2016_1154108&context=PC&vid=44IMP_INST:ICL_VU1&lang=en&search_scope=MyInst_and_CI&adaptor=Primo%20Central&tab=Everything&query=,contains,ASA%20p-value&offset=0The garden forking paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013) http://stat.columbia.edu/~gelman/research/unpublished/forking.pdfThe garden forking paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013) http://stat.columbia.edu/~gelman/research/unpublished/forking.pdfhttps://docker-curriculum.com/ Prakhar Srivastavhttps://docker-curriculum.com/ Prakhar Srivastav","code":""},{"path":"production-explainability.html","id":"production-explainability","chapter":"13 Explainability","heading":"13 Explainability","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-explainability.html","id":"explainability--","chapter":"13 Explainability","heading":"13.1 Explainability ————————————————-","text":"LIME paper ArXiV: https://arxiv.org/abs/1602.04938. Ribeiro et al (2016) “Trust ?”: Explaining Predictions Classifier.LIME paper ArXiV: https://arxiv.org/abs/1602.04938. Ribeiro et al (2016) “Trust ?”: Explaining Predictions Classifier.LIME pacakge documentation CRAN https://cran.r-project.org/web/packages/lime/index.htmlLIME pacakge documentation CRAN https://cran.r-project.org/web/packages/lime/index.htmlUnderstanding LIME tutorial - T Pedersen M BenestyUnderstanding LIME tutorial - T Pedersen M Benestyhttps://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.htmlReference: Interpretable Machine Learning: Guide Making Black Box Models Explainable Christoph Molnar","code":""},{"path":"production-scalability.html","id":"production-scalability","chapter":"14 Scalability","heading":"14 Scalability","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-scalability.html","id":"scalability","chapter":"14 Scalability","heading":"14.1 Scalability ————————————————–","text":"Structure talk around Alex’s parallelisation slidesCode profilingCode profilingvectorisevectoriseparalleliseparalleliseUse better language: C++, PythonUse better language: C++, PythonDocumentation forDocumentation forapplyapplypurr::mappurr::mapfurr::pmapfurr::pmapAdvanced R (Second Edition) Hadley Wickham. Chapters 23 24, measuring improving performanceAdvanced R (Second Edition) Hadley Wickham. Chapters 23 24, measuring improving performanceAdvanced R (Second Edition) Hadley Wickham. Chapter 25, measuring improving performanceAdvanced R (Second Edition) Hadley Wickham. Chapter 25, measuring improving performance","code":""},{"path":"production-checklist.html","id":"production-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"production-checklist.html","id":"videos-chapters-3","chapter":"Checklist","heading":"14.2 Videos / Chapters","text":"[ ][ ][ ][ ][ ][ ]","code":""},{"path":"production-checklist.html","id":"reading-3","chapter":"Checklist","heading":"14.3 Reading","text":"","code":""},{"path":"production-checklist.html","id":"activities-1","chapter":"Checklist","heading":"14.4 Activities","text":"","code":""},{"path":"production-checklist.html","id":"live-session-3","chapter":"Checklist","heading":"14.5 Live Session","text":"","code":""},{"path":"ethics-introduction.html","id":"ethics-introduction","chapter":"Introduction","heading":"Introduction","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-privacy.html","id":"ethics-privacy","chapter":"16 Privacy","heading":"16 Privacy","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-fairness.html","id":"ethics-fairness","chapter":"17 Fairness","heading":"17 Fairness","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-conduct.html","id":"ethics-conduct","chapter":"18 Codes of Conduct","heading":"18 Codes of Conduct","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-checklist.html","id":"ethics-checklist","chapter":"Checklist","heading":"Checklist","text":"Effective Data Science still work--progress. chapter currently dumping ground ideas, don’t recommend reading .like contribute development EDS, may https://github.com/zakvarty/data_science_notes.","code":""},{"path":"ethics-checklist.html","id":"videos-chapters-4","chapter":"Checklist","heading":"18.1 Videos / Chapters","text":"[ ][ ][ ][ ][ ][ ]","code":""},{"path":"ethics-checklist.html","id":"reading-4","chapter":"Checklist","heading":"18.2 Reading","text":"","code":""},{"path":"ethics-checklist.html","id":"activities-2","chapter":"Checklist","heading":"18.3 Activities","text":"","code":""},{"path":"ethics-checklist.html","id":"live-session-4","chapter":"Checklist","heading":"18.4 Live Session","text":"","code":""},{"path":"reading-list.html","id":"reading-list","chapter":"19 Reading List","heading":"19 Reading List","text":"Effective Data Science still work--progress. chapter largely complete just needs final proof reading.like contribute development EDS, may https://github.com/zakvarty/data_science_notes.reading list organised topic, according week course. split several categories.Core Materials: form core part course activities.Core Materials: form core part course activities.Reference Materials: used extensively course, seen helpful guides, rather required reading cover cover.Reference Materials: used extensively course, seen helpful guides, rather required reading cover cover.Materials Interest: form core part course, give deeper understanding interesting perspective weekly topic. might fun stuff .Materials Interest: form core part course, give deeper understanding interesting perspective weekly topic. might fun stuff .","code":""},{"path":"reading-list.html","id":"workflows-reading","chapter":"19 Reading List","heading":"19.1 Effective Data Science Workflows","text":"","code":""},{"path":"reading-list.html","id":"core-materials","chapter":"19 Reading List","heading":"Core Materials","text":"Tidyverse R Style Guide Hadley Wickham.Wilson, et al (2017). Good Enough Practices Scientific Computing. PLOS Computational Biology.","code":""},{"path":"reading-list.html","id":"reference-materials","chapter":"19 Reading List","heading":"Reference Materials","text":"R Data Science Chapters 2, 6 8 Hadley Wickham Garrett Grolemund. Chapters covering R workflow basics, scripting project based workflow.R Data Science Chapters 2, 6 8 Hadley Wickham Garrett Grolemund. Chapters covering R workflow basics, scripting project based workflow.Documentation {} packageDocumentation {} packageR Packages Book (Second Edition) Hadley Wickham Jenny Bryan.R Packages Book (Second Edition) Hadley Wickham Jenny Bryan.","code":""},{"path":"reading-list.html","id":"materials-of-interest","chapter":"19 Reading List","heading":"Materials of Interest","text":"STAT545, Part 1 Jennifer Bryan STAT 545 TAsWhat forgot teach R, Chapters 2-4 Jennifer Bryan Jim Hester.Broman et al (2017). Recommendations Funding Agencies Supporting Reproducible Research. American Statistical Association.Advanced R Hadley Wickham Section introductions functional object oriented approaches programming.Advanced R Hadley Wickham Section introductions functional object oriented approaches programming.Atlassian Article Agile Project ManagementAtlassian Article Agile Project ManagementThe Pragmatic Programmer, 20th Anniversary Edition Edition David Thomas Andrew Hunt. section DRY coding others freely available.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week. Chapter 4 Efficient Workflows links nicely last week’s topics.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week. Chapter 4 Efficient Workflows links nicely last week’s topics.Towards Principled Bayesian Workflow Michael Betancourt.Towards Principled Bayesian Workflow Michael Betancourt.Happy Git GitHub useR Jennifer Bryan","code":""},{"path":"reading-list.html","id":"data-reading","chapter":"19 Reading List","heading":"19.2 Aquiring and Sharing Data","text":"","code":""},{"path":"reading-list.html","id":"core-materials-1","chapter":"19 Reading List","heading":"Core Materials","text":"R Data Science Chapters 9 - 12 Hadley Wickham. chapters introduce tibbles data structure, import data R wrangle data tidy format.R Data Science Chapters 9 - 12 Hadley Wickham. chapters introduce tibbles data structure, import data R wrangle data tidy format.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week.Efficient R programming Colin Gillespie Robin Lovelace. Chapter 5 considers Efficient Input/Output relevant week.Wickham (2014). Tidy Data. Journal Statistical Software. paper brought tidy data mainstream.Wickham (2014). Tidy Data. Journal Statistical Software. paper brought tidy data mainstream.","code":""},{"path":"reading-list.html","id":"reference-materials-1","chapter":"19 Reading List","heading":"Reference Materials","text":"{readr} documentationThe {readr} documentationThe {data.table} documentation vignetteThe {data.table} documentation vignetteThe {rvest} documentationThe {rvest} documentationThe {tidyr} documentationThe {tidyr} documentationMDN Web Docs HTML CSSMDN Web Docs HTML CSS","code":""},{"path":"reading-list.html","id":"materials-of-interest-1","chapter":"19 Reading List","heading":"Materials of Interest","text":"Introduction APIs Brian CookseyR Data Science (Second Edition) Chapters within Import section.covers importing data spreadsheets, databases, using Apache Arrow importing hierarchical data well web scraping.","code":""},{"path":"reading-list.html","id":"data-exploration-and-visualisation","chapter":"19 Reading List","heading":"19.3 Data Exploration and Visualisation","text":"","code":""},{"path":"reading-list.html","id":"core-materials-2","chapter":"19 Reading List","heading":"Core Materials","text":"Exploratory Data Analysis R Roger Peng.Chapters 3 4 core reading, respectively introducing data frame manipulation {dplyr} example workflow exploratory data analysis. chapters may useful references.Flexible Imputation Missing Data Stef van Buuren. Sections 1.1-1.4 give thorough introduction missing data problems.","code":""},{"path":"reading-list.html","id":"referene-materials","chapter":"19 Reading List","heading":"Referene Materials","text":"ggplot2 Tutorial Beautiful Plotting R https://www.cedricscherer.com/2019/08/05/-ggplot2-tutorial--beautiful-plotting--r/) Cédric Scherer.ggplot2 Tutorial Beautiful Plotting R https://www.cedricscherer.com/2019/08/05/-ggplot2-tutorial--beautiful-plotting--r/) Cédric Scherer.{dplyr} documentationThe {dplyr} documentationRStudio Data Transformation Cheat SheetRStudio Data Transformation Cheat SheetR Data Science (First Edition) Chapters Data Transformations, Exploratory Data Analysis Relational Data.R Data Science (First Edition) Chapters Data Transformations, Exploratory Data Analysis Relational Data.Equivalent sections R Data Science Second EditionEquivalent sections R Data Science Second Edition","code":""},{"path":"reading-list.html","id":"materials-of-interest-2","chapter":"19 Reading List","heading":"Materials of Interest","text":"Wickham, H. (2010). Layered Grammar Graphics. Journal Computational Graphical Statistics.Wickham, H. (2010). Layered Grammar Graphics. Journal Computational Graphical Statistics.Better Data Visualisations Jonathan SchwabishBetter Data Visualisations Jonathan SchwabishData Visualization: Practical Introduction Kieran Healy","code":""},{"path":"reading-list.html","id":"preparing-for-production","chapter":"19 Reading List","heading":"19.4 Preparing for Production","text":"","code":""},{"path":"reading-list.html","id":"core-materials-3","chapter":"19 Reading List","heading":"Core Materials","text":"Ethical Algorithm M Kearns Roth (Chapter 4)Ethical Algorithm M Kearns Roth (Chapter 4)Ribeiro et al (2016). “Trust ?”: Explaining Predictions Classifier.Ribeiro et al (2016). “Trust ?”: Explaining Predictions Classifier.","code":""},{"path":"reading-list.html","id":"reference-materials-2","chapter":"19 Reading List","heading":"Reference Materials","text":"Docker Curriculum Prakhar Srivastav.Docker Curriculum Prakhar Srivastav.LIME package documentation CRAN.LIME package documentation CRAN.Interpretable Machine Learning: Guide Making Black Box Models Explainable Christoph Molnar.Interpretable Machine Learning: Guide Making Black Box Models Explainable Christoph Molnar.Documentation apply(), map() pmap()Documentation apply(), map() pmap()Advanced R (Second Edition) Hadley Wickham. Chapter 23 measuring performance Chapter 24 improving performance.Advanced R (Second Edition) Hadley Wickham. Chapter 23 measuring performance Chapter 24 improving performance.","code":""},{"path":"reading-list.html","id":"materials-of-interest-3","chapter":"19 Reading List","heading":"Materials of Interest","text":"ASA Statement \\(p\\)-values: Context, Process PurposeThe ASA Statement \\(p\\)-values: Context, Process PurposeThe Garden Forking Paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013)Garden Forking Paths: multiple comparisons can problem,\neven “Fishing expedition” “p-hacking” research\nhypothesis posited ahead time. Gelman E loken (2013)Understanding LIME tutorial T Pedersen M Benesty.Understanding LIME tutorial T Pedersen M Benesty.Advanced R (Second Edition) Hadley Wickham. Chapter 25 writing R code C++.Advanced R (Second Edition) Hadley Wickham. Chapter 25 writing R code C++.","code":""},{"path":"reading-list.html","id":"data-science-ethics","chapter":"19 Reading List","heading":"19.5 Data Science Ethics","text":"","code":""},{"path":"reading-list.html","id":"core-materials-4","chapter":"19 Reading List","heading":"Core Materials","text":"Ethical Algorithm M Kearns Roth. Chapters 1 2 Algorithmic Privacy Algortihmic Fairness.Ethical Algorithm M Kearns Roth. Chapters 1 2 Algorithmic Privacy Algortihmic Fairness.Gender Shades: Intersectional Accuracy Disparities Commercial Gender Classification Joy Buolamwini Timnit Gebru (2018). Proceedings 1st Conference Fairness, Accountability Transparency.Gender Shades: Intersectional Accuracy Disparities Commercial Gender Classification Joy Buolamwini Timnit Gebru (2018). Proceedings 1st Conference Fairness, Accountability Transparency.Robust De-anonymization Large Sparse Datasets Arvind Narayanan Vitaly Shmatikov (2008). IEEE Symposium Security Privacy.Robust De-anonymization Large Sparse Datasets Arvind Narayanan Vitaly Shmatikov (2008). IEEE Symposium Security Privacy.","code":""},{"path":"reading-list.html","id":"reference-materials-3","chapter":"19 Reading List","heading":"Reference Materials","text":"Fairness machine learning\nLimitations Opportunities Solon Barocas, Moritz Hardt Arvind Narayanan.Fairness machine learning\nLimitations Opportunities Solon Barocas, Moritz Hardt Arvind Narayanan.Professional Guidleines Data Ethics :\nAmerican Mathematical Society\nEuropean Union\nUK Government\nRoyal Statistical Society\nDutch Government\nProfessional Guidleines Data Ethics :American Mathematical SocietyThe European UnionUK GovernmentRoyal Statistical SocietyDutch Government","code":""},{"path":"reading-list.html","id":"materials-of-interest-4","chapter":"19 Reading List","heading":"Materials of Interest","text":"Algorithmic Fairness (2020). Pre-print review paper Dana Pessach Erez Shmueli.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
